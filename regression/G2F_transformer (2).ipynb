{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "9ykJu9Ibg2Tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tab_transformer_pytorch"
      ],
      "metadata": {
        "id": "DCTVV9sGqxMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7cdb25-9a5d-4d1c-de6e-9cf93c9783c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tab_transformer_pytorch\n",
            "  Downloading tab_transformer_pytorch-0.4.2-py3-none-any.whl.metadata (914 bytes)\n",
            "Requirement already satisfied: einops>=0.8 in /usr/local/lib/python3.11/dist-packages (from tab_transformer_pytorch) (0.8.1)\n",
            "Collecting hyper-connections>=0.1.15 (from tab_transformer_pytorch)\n",
            "  Downloading hyper_connections-0.1.15-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: torch>=2.3 in /usr/local/lib/python3.11/dist-packages (from tab_transformer_pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.3->tab_transformer_pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->tab_transformer_pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3->tab_transformer_pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3->tab_transformer_pytorch) (3.0.2)\n",
            "Downloading tab_transformer_pytorch-0.4.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading hyper_connections-0.1.15-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyper-connections, tab_transformer_pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed hyper-connections-0.1.15 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tab_transformer_pytorch-0.4.2\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install hyper-connections"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptuLWgdhq54M",
        "outputId": "fef5edfb-dffe-4bbd-a35b-46189716f5c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hyper-connections in /usr/local/lib/python3.11/dist-packages (0.1.15)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from hyper-connections) (0.8.1)\n",
            "Requirement already satisfied: torch>=2.3 in /usr/local/lib/python3.11/dist-packages (from hyper-connections) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3->hyper-connections) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3->hyper-connections) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.3->hyper-connections) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1HhGt2sbqzbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHvUR1ULmtkE"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tab_transformer_pytorch import TabTransformer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Loading with proper handling of mixed types\n",
        "template = pd.read_csv('1_Submission_Template_2024 (2).csv', low_memory=False)"
      ],
      "metadata": {
        "id": "zPq1iAvuomRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comp_testing = pd.read_csv('testing_data (1).csv', low_memory=False)"
      ],
      "metadata": {
        "id": "nmCRPx85rH5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_all= pd.read_csv('training_data (1).csv', low_memory=False)"
      ],
      "metadata": {
        "id": "Mp2bC5AurnNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subset 2000 rows in training_with_common\n",
        "small_training = training_all.head(2000)"
      ],
      "metadata": {
        "id": "VuUVbvdLsEWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Preprocessing\n",
        "target_col = \"Yield_Mg_ha\"\n",
        "df = training_all.dropna(subset=[target_col])\n",
        "\n",
        "# Separate features by type\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "numerical_cols.remove(target_col)\n",
        "\n",
        "# Print column info for debugging\n",
        "print(\"Number of categorical columns:\", len(categorical_cols))\n",
        "print(\"Number of numerical columns:\", len(numerical_cols))\n",
        "\n",
        "# Remove any columns with all NaN values\n",
        "numerical_cols = [col for col in numerical_cols if not df[col].isna().all()]\n",
        "print(\"Number of numerical columns after removing all-NaN columns:\", len(numerical_cols))\n",
        "\n",
        "# Handle remaining NaN values in numerical columns\n",
        "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
        "\n",
        "# Initialize encoders\n",
        "label_encoders = {}\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Process categorical columns with LabelEncoder\n",
        "categorical_data = np.zeros((len(df), len(categorical_cols)), dtype=np.int64)\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    categorical_data[:, i] = label_encoders[col].fit_transform(df[col].astype(str).values)\n",
        "\n",
        "# Process numerical columns with StandardScaler\n",
        "numerical_data = scaler.fit_transform(df[numerical_cols].values).astype(np.float32)\n",
        "\n",
        "# Define num_continuous before using it\n",
        "num_continuous = len(numerical_cols)\n",
        "print(\"Number of continuous features:\", num_continuous)\n"
      ],
      "metadata": {
        "id": "ttrkjFgRnlAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0398b7-d214-4cda-a49f-fbdbb9a1d324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of categorical columns: 3\n",
            "Number of numerical columns: 658\n",
            "Number of numerical columns after removing all-NaN columns: 658\n",
            "Number of continuous features: 658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Preprocessing\n",
        "target_col = \"Yield_Mg_ha\"\n",
        "df = training_all.dropna(subset=[target_col])\n",
        "\n",
        "# Separate features by type\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
        "numerical_cols.remove(target_col)\n",
        "\n",
        "# Print column info for debugging\n",
        "print(\"Number of categorical columns:\", len(categorical_cols))\n",
        "print(\"Number of numerical columns:\", len(numerical_cols))\n",
        "\n",
        "# Remove any columns with all NaN values\n",
        "numerical_cols = [col for col in numerical_cols if not df[col].isna().all()]\n",
        "print(\"Number of numerical columns after removing all-NaN columns:\", len(numerical_cols))\n",
        "\n",
        "# Handle remaining NaN values in numerical columns\n",
        "df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
        "\n",
        "# Initialize encoders\n",
        "label_encoders = {}\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Process categorical columns with LabelEncoder\n",
        "categorical_data = np.zeros((len(df), len(categorical_cols)), dtype=np.int64)\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    label_encoders[col] = LabelEncoder()\n",
        "    categorical_data[:, i] = label_encoders[col].fit_transform(df[col].astype(str).values)\n",
        "\n",
        "# Process numerical columns with StandardScaler\n",
        "numerical_data = scaler.fit_transform(df[numerical_cols].values).astype(np.float32)\n",
        "\n",
        "# Define num_continuous before using it\n",
        "num_continuous = len(numerical_cols)\n",
        "print(\"Number of continuous features:\", num_continuous)\n"
      ],
      "metadata": {
        "id": "yWjDuxi7npgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4610914-9b81-4535-cf14-0148a09b39b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of categorical columns: 3\n",
            "Number of numerical columns: 658\n",
            "Number of numerical columns after removing all-NaN columns: 658\n",
            "Number of continuous features: 658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare target with scaling\n",
        "target_scaler = StandardScaler()\n",
        "y = target_scaler.fit_transform(df[target_col].values.reshape(-1, 1)).astype(np.float32)\n",
        "\n",
        "# Print data statistics for debugging\n",
        "print(\"\\nNumerical data stats:\")\n",
        "print(\"Mean:\", np.mean(numerical_data))\n",
        "print(\"Std:\", np.std(numerical_data))\n",
        "print(\"Any NaN:\", np.isnan(numerical_data).any())\n",
        "print(\"Shape:\", numerical_data.shape)\n",
        "\n",
        "print(\"\\nCategorical data stats:\")\n",
        "print(\"Min:\", np.min(categorical_data))\n",
        "print(\"Max:\", np.max(categorical_data))\n",
        "print(\"Any NaN:\", np.isnan(categorical_data).any())\n",
        "print(\"Shape:\", categorical_data.shape)\n",
        "\n",
        "print(\"\\nTarget stats:\")\n",
        "print(\"Mean:\", np.mean(y))\n",
        "print(\"Std:\", np.std(y))\n",
        "print(\"Any NaN:\", np.isnan(y).any())\n",
        "print(\"Shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "6_nXalSDn9MV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2e1f87-5355-4f08-edb2-5e921498fc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Numerical data stats:\n",
            "Mean: 1.0168962e-09\n",
            "Std: 1.0000013\n",
            "Any NaN: False\n",
            "Shape: (164921, 658)\n",
            "\n",
            "Categorical data stats:\n",
            "Min: 0\n",
            "Max: 5026\n",
            "Any NaN: False\n",
            "Shape: (164921, 3)\n",
            "\n",
            "Target stats:\n",
            "Mean: 5.1812212e-09\n",
            "Std: 1.0\n",
            "Any NaN: False\n",
            "Shape: (164921, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "r6QQeyqShBwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Train-Test Split\n",
        "cat_train, cat_test, num_train, num_test, y_train, y_test = train_test_split(\n",
        "    categorical_data, numerical_data, y,\n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Calculate continuous mean and std\n",
        "cont_mean_std = torch.tensor(\n",
        "    np.stack([\n",
        "        np.zeros(num_continuous),  # mean is 0 after StandardScaler\n",
        "        np.ones(num_continuous)    # std is 1 after StandardScaler\n",
        "    ], axis=1),\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "# 5. Model Initialization with smaller architecture\n",
        "categories = tuple([len(label_encoders[col].classes_) for col in categorical_cols])\n",
        "\n",
        "model = TabTransformer(\n",
        "    categories=categories,\n",
        "    num_continuous=num_continuous,\n",
        "    dim=16,\n",
        "    dim_out=1,\n",
        "    depth=3,\n",
        "    heads=4,\n",
        "    attn_dropout=0.1,\n",
        "    ff_dropout=0.1,\n",
        "    mlp_hidden_mults=(2, 2),\n",
        "    mlp_act=nn.ReLU(),\n",
        "    continuous_mean_std=cont_mean_std\n",
        ")\n",
        "\n",
        "# Rest of the code remains the same...\n",
        "\n"
      ],
      "metadata": {
        "id": "GasdbFTGoD-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Training Loop with gradient clipping\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # reduced learning rate\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Convert data to tensors\n",
        "cat_train_tensor = torch.tensor(cat_train, dtype=torch.long)\n",
        "num_train_tensor = torch.tensor(num_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "# Training loop with added checks\n",
        "epochs = 10\n",
        "batch_size = 64  # increased batch size\n",
        "n_samples = len(cat_train)\n",
        "max_grad_norm = 1.0  # for gradient clipping\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        batch_cat = cat_train_tensor[i:i+batch_size]\n",
        "        batch_num = num_train_tensor[i:i+batch_size]\n",
        "        batch_y = y_train_tensor[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_cat, batch_num)\n",
        "\n",
        "        # Check for NaN in outputs\n",
        "        if torch.isnan(outputs).any():\n",
        "            print(f\"NaN detected in outputs at batch starting with index {i}\")\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        # Check for NaN in loss\n",
        "        if torch.isnan(loss):\n",
        "            print(f\"NaN detected in loss at batch starting with index {i}\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        valid_batches += 1\n",
        "\n",
        "        # Print batch loss for monitoring\n",
        "        if i % (5 * batch_size) == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{i//batch_size}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    if valid_batches > 0:\n",
        "        avg_loss = total_loss / valid_batches\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')\n",
        "    else:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}]: No valid batches!')\n",
        "\n",
        "# 7. Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    cat_test_tensor = torch.tensor(cat_test, dtype=torch.long)\n",
        "    num_test_tensor = torch.tensor(num_test, dtype=torch.float32)\n",
        "    y_pred = model(cat_test_tensor, num_test_tensor).numpy()\n",
        "\n",
        "    # Inverse transform predictions and actual values\n",
        "    y_pred_original = target_scaler.inverse_transform(y_pred)\n",
        "    y_test_original = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
        "    print(f\"Test RMSE: {rmse:.4f}\")\n",
        "    # Calculate R-squared\n",
        "    from sklearn.metrics import r2_score\n",
        "    r2 = r2_score(y_test_original, y_pred_original)\n",
        "    print(f\"Test R-squared: {r2:.4f}\")\n",
        "\n",
        "    # Calculate Correlation\n",
        "    correlation = np.corrcoef(y_test_original.ravel(), y_pred_original.ravel())[0, 1]\n",
        "    print(f\"Test Correlation: {correlation:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwGvOKPVs6wI",
        "outputId": "14a591ee-356d-4f8a-d7a8-cc7aa42751ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Batch [0], Loss: 0.4559\n",
            "Epoch [1/10], Batch [5], Loss: 0.4824\n",
            "Epoch [1/10], Batch [10], Loss: 0.4185\n",
            "Epoch [1/10], Batch [15], Loss: 0.4884\n",
            "Epoch [1/10], Batch [20], Loss: 0.4272\n",
            "Epoch [1/10], Batch [25], Loss: 0.5522\n",
            "Epoch [1/10], Batch [30], Loss: 0.4616\n",
            "Epoch [1/10], Batch [35], Loss: 0.6066\n",
            "Epoch [1/10], Batch [40], Loss: 0.2928\n",
            "Epoch [1/10], Batch [45], Loss: 0.4129\n",
            "Epoch [1/10], Batch [50], Loss: 0.5504\n",
            "Epoch [1/10], Batch [55], Loss: 0.5235\n",
            "Epoch [1/10], Batch [60], Loss: 0.4905\n",
            "Epoch [1/10], Batch [65], Loss: 0.4739\n",
            "Epoch [1/10], Batch [70], Loss: 0.5149\n",
            "Epoch [1/10], Batch [75], Loss: 0.5726\n",
            "Epoch [1/10], Batch [80], Loss: 0.4694\n",
            "Epoch [1/10], Batch [85], Loss: 0.5072\n",
            "Epoch [1/10], Batch [90], Loss: 0.5936\n",
            "Epoch [1/10], Batch [95], Loss: 0.6558\n",
            "Epoch [1/10], Batch [100], Loss: 0.2611\n",
            "Epoch [1/10], Batch [105], Loss: 0.3685\n",
            "Epoch [1/10], Batch [110], Loss: 0.4372\n",
            "Epoch [1/10], Batch [115], Loss: 0.6182\n",
            "Epoch [1/10], Batch [120], Loss: 0.4433\n",
            "Epoch [1/10], Batch [125], Loss: 0.4915\n",
            "Epoch [1/10], Batch [130], Loss: 0.3590\n",
            "Epoch [1/10], Batch [135], Loss: 0.4391\n",
            "Epoch [1/10], Batch [140], Loss: 0.4397\n",
            "Epoch [1/10], Batch [145], Loss: 0.4603\n",
            "Epoch [1/10], Batch [150], Loss: 0.4392\n",
            "Epoch [1/10], Batch [155], Loss: 0.5982\n",
            "Epoch [1/10], Batch [160], Loss: 0.5250\n",
            "Epoch [1/10], Batch [165], Loss: 0.3418\n",
            "Epoch [1/10], Batch [170], Loss: 0.2560\n",
            "Epoch [1/10], Batch [175], Loss: 0.4655\n",
            "Epoch [1/10], Batch [180], Loss: 0.5696\n",
            "Epoch [1/10], Batch [185], Loss: 0.5048\n",
            "Epoch [1/10], Batch [190], Loss: 0.3171\n",
            "Epoch [1/10], Batch [195], Loss: 0.5867\n",
            "Epoch [1/10], Batch [200], Loss: 0.6528\n",
            "Epoch [1/10], Batch [205], Loss: 0.5695\n",
            "Epoch [1/10], Batch [210], Loss: 0.2783\n",
            "Epoch [1/10], Batch [215], Loss: 0.5364\n",
            "Epoch [1/10], Batch [220], Loss: 0.5627\n",
            "Epoch [1/10], Batch [225], Loss: 0.6269\n",
            "Epoch [1/10], Batch [230], Loss: 0.4630\n",
            "Epoch [1/10], Batch [235], Loss: 0.3966\n",
            "Epoch [1/10], Batch [240], Loss: 0.4066\n",
            "Epoch [1/10], Batch [245], Loss: 0.3006\n",
            "Epoch [1/10], Batch [250], Loss: 0.3490\n",
            "Epoch [1/10], Batch [255], Loss: 0.3531\n",
            "Epoch [1/10], Batch [260], Loss: 0.3610\n",
            "Epoch [1/10], Batch [265], Loss: 0.3512\n",
            "Epoch [1/10], Batch [270], Loss: 0.4285\n",
            "Epoch [1/10], Batch [275], Loss: 0.4734\n",
            "Epoch [1/10], Batch [280], Loss: 0.3957\n",
            "Epoch [1/10], Batch [285], Loss: 0.5909\n",
            "Epoch [1/10], Batch [290], Loss: 0.4219\n",
            "Epoch [1/10], Batch [295], Loss: 0.3579\n",
            "Epoch [1/10], Batch [300], Loss: 0.3215\n",
            "Epoch [1/10], Batch [305], Loss: 0.7213\n",
            "Epoch [1/10], Batch [310], Loss: 0.2910\n",
            "Epoch [1/10], Batch [315], Loss: 0.3692\n",
            "Epoch [1/10], Batch [320], Loss: 0.3583\n",
            "Epoch [1/10], Batch [325], Loss: 0.2091\n",
            "Epoch [1/10], Batch [330], Loss: 0.4075\n",
            "Epoch [1/10], Batch [335], Loss: 0.4192\n",
            "Epoch [1/10], Batch [340], Loss: 0.4837\n",
            "Epoch [1/10], Batch [345], Loss: 0.3964\n",
            "Epoch [1/10], Batch [350], Loss: 0.4544\n",
            "Epoch [1/10], Batch [355], Loss: 0.4173\n",
            "Epoch [1/10], Batch [360], Loss: 0.3621\n",
            "Epoch [1/10], Batch [365], Loss: 0.3494\n",
            "Epoch [1/10], Batch [370], Loss: 0.4335\n",
            "Epoch [1/10], Batch [375], Loss: 0.3061\n",
            "Epoch [1/10], Batch [380], Loss: 0.3254\n",
            "Epoch [1/10], Batch [385], Loss: 0.4165\n",
            "Epoch [1/10], Batch [390], Loss: 0.4828\n",
            "Epoch [1/10], Batch [395], Loss: 0.3623\n",
            "Epoch [1/10], Batch [400], Loss: 0.2934\n",
            "Epoch [1/10], Batch [405], Loss: 0.2558\n",
            "Epoch [1/10], Batch [410], Loss: 0.3369\n",
            "Epoch [1/10], Batch [415], Loss: 0.4864\n",
            "Epoch [1/10], Batch [420], Loss: 0.3735\n",
            "Epoch [1/10], Batch [425], Loss: 0.5454\n",
            "Epoch [1/10], Batch [430], Loss: 0.3269\n",
            "Epoch [1/10], Batch [435], Loss: 0.4902\n",
            "Epoch [1/10], Batch [440], Loss: 0.7722\n",
            "Epoch [1/10], Batch [445], Loss: 0.6664\n",
            "Epoch [1/10], Batch [450], Loss: 0.5074\n",
            "Epoch [1/10], Batch [455], Loss: 0.3766\n",
            "Epoch [1/10], Batch [460], Loss: 0.3356\n",
            "Epoch [1/10], Batch [465], Loss: 0.3326\n",
            "Epoch [1/10], Batch [470], Loss: 0.3976\n",
            "Epoch [1/10], Batch [475], Loss: 0.5647\n",
            "Epoch [1/10], Batch [480], Loss: 0.3905\n",
            "Epoch [1/10], Batch [485], Loss: 0.6152\n",
            "Epoch [1/10], Batch [490], Loss: 0.4424\n",
            "Epoch [1/10], Batch [495], Loss: 0.3399\n",
            "Epoch [1/10], Batch [500], Loss: 0.5972\n",
            "Epoch [1/10], Batch [505], Loss: 0.3682\n",
            "Epoch [1/10], Batch [510], Loss: 0.2736\n",
            "Epoch [1/10], Batch [515], Loss: 0.5545\n",
            "Epoch [1/10], Batch [520], Loss: 0.4384\n",
            "Epoch [1/10], Batch [525], Loss: 0.3604\n",
            "Epoch [1/10], Batch [530], Loss: 0.3289\n",
            "Epoch [1/10], Batch [535], Loss: 0.3632\n",
            "Epoch [1/10], Batch [540], Loss: 0.3690\n",
            "Epoch [1/10], Batch [545], Loss: 0.4630\n",
            "Epoch [1/10], Batch [550], Loss: 0.3427\n",
            "Epoch [1/10], Batch [555], Loss: 0.4000\n",
            "Epoch [1/10], Batch [560], Loss: 0.4761\n",
            "Epoch [1/10], Batch [565], Loss: 0.3980\n",
            "Epoch [1/10], Batch [570], Loss: 0.3992\n",
            "Epoch [1/10], Batch [575], Loss: 0.6240\n",
            "Epoch [1/10], Batch [580], Loss: 0.5530\n",
            "Epoch [1/10], Batch [585], Loss: 0.4280\n",
            "Epoch [1/10], Batch [590], Loss: 0.4681\n",
            "Epoch [1/10], Batch [595], Loss: 0.3288\n",
            "Epoch [1/10], Batch [600], Loss: 0.5708\n",
            "Epoch [1/10], Batch [605], Loss: 0.5694\n",
            "Epoch [1/10], Batch [610], Loss: 0.5023\n",
            "Epoch [1/10], Batch [615], Loss: 0.4442\n",
            "Epoch [1/10], Batch [620], Loss: 0.4019\n",
            "Epoch [1/10], Batch [625], Loss: 0.5642\n",
            "Epoch [1/10], Batch [630], Loss: 0.5475\n",
            "Epoch [1/10], Batch [635], Loss: 0.5480\n",
            "Epoch [1/10], Batch [640], Loss: 0.3750\n",
            "Epoch [1/10], Batch [645], Loss: 0.5802\n",
            "Epoch [1/10], Batch [650], Loss: 0.7290\n",
            "Epoch [1/10], Batch [655], Loss: 0.5333\n",
            "Epoch [1/10], Batch [660], Loss: 0.6077\n",
            "Epoch [1/10], Batch [665], Loss: 0.5872\n",
            "Epoch [1/10], Batch [670], Loss: 0.3816\n",
            "Epoch [1/10], Batch [675], Loss: 0.5145\n",
            "Epoch [1/10], Batch [680], Loss: 0.5365\n",
            "Epoch [1/10], Batch [685], Loss: 0.3125\n",
            "Epoch [1/10], Batch [690], Loss: 0.4159\n",
            "Epoch [1/10], Batch [695], Loss: 0.5719\n",
            "Epoch [1/10], Batch [700], Loss: 0.4051\n",
            "Epoch [1/10], Batch [705], Loss: 0.3781\n",
            "Epoch [1/10], Batch [710], Loss: 0.4516\n",
            "Epoch [1/10], Batch [715], Loss: 0.4212\n",
            "Epoch [1/10], Batch [720], Loss: 0.5135\n",
            "Epoch [1/10], Batch [725], Loss: 0.6401\n",
            "Epoch [1/10], Batch [730], Loss: 0.3656\n",
            "Epoch [1/10], Batch [735], Loss: 0.4705\n",
            "Epoch [1/10], Batch [740], Loss: 0.4809\n",
            "Epoch [1/10], Batch [745], Loss: 0.4477\n",
            "Epoch [1/10], Batch [750], Loss: 0.6140\n",
            "Epoch [1/10], Batch [755], Loss: 0.3412\n",
            "Epoch [1/10], Batch [760], Loss: 0.3692\n",
            "Epoch [1/10], Batch [765], Loss: 0.3867\n",
            "Epoch [1/10], Batch [770], Loss: 0.4301\n",
            "Epoch [1/10], Batch [775], Loss: 0.3826\n",
            "Epoch [1/10], Batch [780], Loss: 0.3345\n",
            "Epoch [1/10], Batch [785], Loss: 0.4276\n",
            "Epoch [1/10], Batch [790], Loss: 0.3750\n",
            "Epoch [1/10], Batch [795], Loss: 0.4457\n",
            "Epoch [1/10], Batch [800], Loss: 0.2808\n",
            "Epoch [1/10], Batch [805], Loss: 0.7465\n",
            "Epoch [1/10], Batch [810], Loss: 0.5461\n",
            "Epoch [1/10], Batch [815], Loss: 0.3680\n",
            "Epoch [1/10], Batch [820], Loss: 0.3344\n",
            "Epoch [1/10], Batch [825], Loss: 0.4935\n",
            "Epoch [1/10], Batch [830], Loss: 0.5294\n",
            "Epoch [1/10], Batch [835], Loss: 0.4793\n",
            "Epoch [1/10], Batch [840], Loss: 0.3836\n",
            "Epoch [1/10], Batch [845], Loss: 0.3578\n",
            "Epoch [1/10], Batch [850], Loss: 0.4438\n",
            "Epoch [1/10], Batch [855], Loss: 0.5703\n",
            "Epoch [1/10], Batch [860], Loss: 0.6221\n",
            "Epoch [1/10], Batch [865], Loss: 0.3738\n",
            "Epoch [1/10], Batch [870], Loss: 0.3485\n",
            "Epoch [1/10], Batch [875], Loss: 0.4046\n",
            "Epoch [1/10], Batch [880], Loss: 0.4431\n",
            "Epoch [1/10], Batch [885], Loss: 0.4834\n",
            "Epoch [1/10], Batch [890], Loss: 0.5189\n",
            "Epoch [1/10], Batch [895], Loss: 0.4043\n",
            "Epoch [1/10], Batch [900], Loss: 0.4086\n",
            "Epoch [1/10], Batch [905], Loss: 0.4501\n",
            "Epoch [1/10], Batch [910], Loss: 0.4419\n",
            "Epoch [1/10], Batch [915], Loss: 0.4970\n",
            "Epoch [1/10], Batch [920], Loss: 0.4518\n",
            "Epoch [1/10], Batch [925], Loss: 0.4727\n",
            "Epoch [1/10], Batch [930], Loss: 0.2732\n",
            "Epoch [1/10], Batch [935], Loss: 0.6479\n",
            "Epoch [1/10], Batch [940], Loss: 0.3837\n",
            "Epoch [1/10], Batch [945], Loss: 0.3099\n",
            "Epoch [1/10], Batch [950], Loss: 0.3798\n",
            "Epoch [1/10], Batch [955], Loss: 0.3917\n",
            "Epoch [1/10], Batch [960], Loss: 0.4134\n",
            "Epoch [1/10], Batch [965], Loss: 0.5247\n",
            "Epoch [1/10], Batch [970], Loss: 0.2396\n",
            "Epoch [1/10], Batch [975], Loss: 0.3524\n",
            "Epoch [1/10], Batch [980], Loss: 0.3251\n",
            "Epoch [1/10], Batch [985], Loss: 0.5022\n",
            "Epoch [1/10], Batch [990], Loss: 0.3847\n",
            "Epoch [1/10], Batch [995], Loss: 0.4056\n",
            "Epoch [1/10], Batch [1000], Loss: 0.3409\n",
            "Epoch [1/10], Batch [1005], Loss: 0.4664\n",
            "Epoch [1/10], Batch [1010], Loss: 0.5817\n",
            "Epoch [1/10], Batch [1015], Loss: 0.4198\n",
            "Epoch [1/10], Batch [1020], Loss: 0.3646\n",
            "Epoch [1/10], Batch [1025], Loss: 0.5831\n",
            "Epoch [1/10], Batch [1030], Loss: 0.4851\n",
            "Epoch [1/10], Batch [1035], Loss: 0.2768\n",
            "Epoch [1/10], Batch [1040], Loss: 0.4718\n",
            "Epoch [1/10], Batch [1045], Loss: 0.3572\n",
            "Epoch [1/10], Batch [1050], Loss: 0.4996\n",
            "Epoch [1/10], Batch [1055], Loss: 0.4761\n",
            "Epoch [1/10], Batch [1060], Loss: 0.3434\n",
            "Epoch [1/10], Batch [1065], Loss: 0.2740\n",
            "Epoch [1/10], Batch [1070], Loss: 0.3773\n",
            "Epoch [1/10], Batch [1075], Loss: 0.3859\n",
            "Epoch [1/10], Batch [1080], Loss: 0.5017\n",
            "Epoch [1/10], Batch [1085], Loss: 0.4224\n",
            "Epoch [1/10], Batch [1090], Loss: 0.7148\n",
            "Epoch [1/10], Batch [1095], Loss: 0.5377\n",
            "Epoch [1/10], Batch [1100], Loss: 0.5181\n",
            "Epoch [1/10], Batch [1105], Loss: 0.3801\n",
            "Epoch [1/10], Batch [1110], Loss: 0.3993\n",
            "Epoch [1/10], Batch [1115], Loss: 0.3590\n",
            "Epoch [1/10], Batch [1120], Loss: 0.3776\n",
            "Epoch [1/10], Batch [1125], Loss: 0.5118\n",
            "Epoch [1/10], Batch [1130], Loss: 0.3219\n",
            "Epoch [1/10], Batch [1135], Loss: 0.5164\n",
            "Epoch [1/10], Batch [1140], Loss: 0.3596\n",
            "Epoch [1/10], Batch [1145], Loss: 0.4354\n",
            "Epoch [1/10], Batch [1150], Loss: 0.5503\n",
            "Epoch [1/10], Batch [1155], Loss: 0.2929\n",
            "Epoch [1/10], Batch [1160], Loss: 0.3317\n",
            "Epoch [1/10], Batch [1165], Loss: 0.3726\n",
            "Epoch [1/10], Batch [1170], Loss: 0.2679\n",
            "Epoch [1/10], Batch [1175], Loss: 0.2450\n",
            "Epoch [1/10], Batch [1180], Loss: 0.3610\n",
            "Epoch [1/10], Batch [1185], Loss: 0.4700\n",
            "Epoch [1/10], Batch [1190], Loss: 0.5227\n",
            "Epoch [1/10], Batch [1195], Loss: 0.4940\n",
            "Epoch [1/10], Batch [1200], Loss: 0.3593\n",
            "Epoch [1/10], Batch [1205], Loss: 0.4978\n",
            "Epoch [1/10], Batch [1210], Loss: 0.4047\n",
            "Epoch [1/10], Batch [1215], Loss: 0.4550\n",
            "Epoch [1/10], Batch [1220], Loss: 0.3878\n",
            "Epoch [1/10], Batch [1225], Loss: 0.4161\n",
            "Epoch [1/10], Batch [1230], Loss: 0.4934\n",
            "Epoch [1/10], Batch [1235], Loss: 0.3361\n",
            "Epoch [1/10], Batch [1240], Loss: 0.5189\n",
            "Epoch [1/10], Batch [1245], Loss: 0.3513\n",
            "Epoch [1/10], Batch [1250], Loss: 0.4160\n",
            "Epoch [1/10], Batch [1255], Loss: 0.5363\n",
            "Epoch [1/10], Batch [1260], Loss: 0.4388\n",
            "Epoch [1/10], Batch [1265], Loss: 0.3996\n",
            "Epoch [1/10], Batch [1270], Loss: 0.3404\n",
            "Epoch [1/10], Batch [1275], Loss: 0.3613\n",
            "Epoch [1/10], Batch [1280], Loss: 0.4189\n",
            "Epoch [1/10], Batch [1285], Loss: 0.2678\n",
            "Epoch [1/10], Batch [1290], Loss: 0.3837\n",
            "Epoch [1/10], Batch [1295], Loss: 0.3961\n",
            "Epoch [1/10], Batch [1300], Loss: 0.4256\n",
            "Epoch [1/10], Batch [1305], Loss: 0.5157\n",
            "Epoch [1/10], Batch [1310], Loss: 0.3770\n",
            "Epoch [1/10], Batch [1315], Loss: 0.5141\n",
            "Epoch [1/10], Batch [1320], Loss: 0.3796\n",
            "Epoch [1/10], Batch [1325], Loss: 0.3813\n",
            "Epoch [1/10], Batch [1330], Loss: 0.4066\n",
            "Epoch [1/10], Batch [1335], Loss: 0.3600\n",
            "Epoch [1/10], Batch [1340], Loss: 0.4565\n",
            "Epoch [1/10], Batch [1345], Loss: 0.3904\n",
            "Epoch [1/10], Batch [1350], Loss: 0.4750\n",
            "Epoch [1/10], Batch [1355], Loss: 0.6691\n",
            "Epoch [1/10], Batch [1360], Loss: 0.4020\n",
            "Epoch [1/10], Batch [1365], Loss: 0.4774\n",
            "Epoch [1/10], Batch [1370], Loss: 0.4774\n",
            "Epoch [1/10], Batch [1375], Loss: 0.4247\n",
            "Epoch [1/10], Batch [1380], Loss: 0.5533\n",
            "Epoch [1/10], Batch [1385], Loss: 0.6942\n",
            "Epoch [1/10], Batch [1390], Loss: 0.6273\n",
            "Epoch [1/10], Batch [1395], Loss: 0.4624\n",
            "Epoch [1/10], Batch [1400], Loss: 0.4791\n",
            "Epoch [1/10], Batch [1405], Loss: 0.3595\n",
            "Epoch [1/10], Batch [1410], Loss: 0.3220\n",
            "Epoch [1/10], Batch [1415], Loss: 0.4418\n",
            "Epoch [1/10], Batch [1420], Loss: 0.3242\n",
            "Epoch [1/10], Batch [1425], Loss: 0.3508\n",
            "Epoch [1/10], Batch [1430], Loss: 0.7598\n",
            "Epoch [1/10], Batch [1435], Loss: 0.6342\n",
            "Epoch [1/10], Batch [1440], Loss: 0.5344\n",
            "Epoch [1/10], Batch [1445], Loss: 0.5889\n",
            "Epoch [1/10], Batch [1450], Loss: 0.4199\n",
            "Epoch [1/10], Batch [1455], Loss: 0.3564\n",
            "Epoch [1/10], Batch [1460], Loss: 0.2868\n",
            "Epoch [1/10], Batch [1465], Loss: 0.5200\n",
            "Epoch [1/10], Batch [1470], Loss: 0.3717\n",
            "Epoch [1/10], Batch [1475], Loss: 0.2399\n",
            "Epoch [1/10], Batch [1480], Loss: 0.4607\n",
            "Epoch [1/10], Batch [1485], Loss: 0.5517\n",
            "Epoch [1/10], Batch [1490], Loss: 0.4947\n",
            "Epoch [1/10], Batch [1495], Loss: 0.5136\n",
            "Epoch [1/10], Batch [1500], Loss: 0.2703\n",
            "Epoch [1/10], Batch [1505], Loss: 0.4313\n",
            "Epoch [1/10], Batch [1510], Loss: 0.3751\n",
            "Epoch [1/10], Batch [1515], Loss: 0.3477\n",
            "Epoch [1/10], Batch [1520], Loss: 0.3449\n",
            "Epoch [1/10], Batch [1525], Loss: 0.4214\n",
            "Epoch [1/10], Batch [1530], Loss: 0.4216\n",
            "Epoch [1/10], Batch [1535], Loss: 0.5565\n",
            "Epoch [1/10], Batch [1540], Loss: 0.4629\n",
            "Epoch [1/10], Batch [1545], Loss: 0.4820\n",
            "Epoch [1/10], Batch [1550], Loss: 0.4433\n",
            "Epoch [1/10], Batch [1555], Loss: 0.3009\n",
            "Epoch [1/10], Batch [1560], Loss: 0.4318\n",
            "Epoch [1/10], Batch [1565], Loss: 0.4315\n",
            "Epoch [1/10], Batch [1570], Loss: 0.6068\n",
            "Epoch [1/10], Batch [1575], Loss: 0.5284\n",
            "Epoch [1/10], Batch [1580], Loss: 0.3831\n",
            "Epoch [1/10], Batch [1585], Loss: 0.3639\n",
            "Epoch [1/10], Batch [1590], Loss: 0.5628\n",
            "Epoch [1/10], Batch [1595], Loss: 0.3654\n",
            "Epoch [1/10], Batch [1600], Loss: 0.4223\n",
            "Epoch [1/10], Batch [1605], Loss: 0.5250\n",
            "Epoch [1/10], Batch [1610], Loss: 0.3546\n",
            "Epoch [1/10], Batch [1615], Loss: 0.4797\n",
            "Epoch [1/10], Batch [1620], Loss: 0.4361\n",
            "Epoch [1/10], Batch [1625], Loss: 0.4287\n",
            "Epoch [1/10], Batch [1630], Loss: 0.3516\n",
            "Epoch [1/10], Batch [1635], Loss: 0.3420\n",
            "Epoch [1/10], Batch [1640], Loss: 0.5022\n",
            "Epoch [1/10], Batch [1645], Loss: 0.4541\n",
            "Epoch [1/10], Batch [1650], Loss: 0.4230\n",
            "Epoch [1/10], Batch [1655], Loss: 0.4976\n",
            "Epoch [1/10], Batch [1660], Loss: 0.3504\n",
            "Epoch [1/10], Batch [1665], Loss: 0.3094\n",
            "Epoch [1/10], Batch [1670], Loss: 0.3386\n",
            "Epoch [1/10], Batch [1675], Loss: 0.5764\n",
            "Epoch [1/10], Batch [1680], Loss: 0.4607\n",
            "Epoch [1/10], Batch [1685], Loss: 0.5170\n",
            "Epoch [1/10], Batch [1690], Loss: 0.4235\n",
            "Epoch [1/10], Batch [1695], Loss: 0.3223\n",
            "Epoch [1/10], Batch [1700], Loss: 0.4176\n",
            "Epoch [1/10], Batch [1705], Loss: 0.3308\n",
            "Epoch [1/10], Batch [1710], Loss: 0.6346\n",
            "Epoch [1/10], Batch [1715], Loss: 0.4043\n",
            "Epoch [1/10], Batch [1720], Loss: 0.4747\n",
            "Epoch [1/10], Batch [1725], Loss: 0.5079\n",
            "Epoch [1/10], Batch [1730], Loss: 0.3891\n",
            "Epoch [1/10], Batch [1735], Loss: 0.3716\n",
            "Epoch [1/10], Batch [1740], Loss: 0.3664\n",
            "Epoch [1/10], Batch [1745], Loss: 0.3458\n",
            "Epoch [1/10], Batch [1750], Loss: 0.5547\n",
            "Epoch [1/10], Batch [1755], Loss: 0.3352\n",
            "Epoch [1/10], Batch [1760], Loss: 0.2328\n",
            "Epoch [1/10], Batch [1765], Loss: 0.4274\n",
            "Epoch [1/10], Batch [1770], Loss: 0.3471\n",
            "Epoch [1/10], Batch [1775], Loss: 0.3690\n",
            "Epoch [1/10], Batch [1780], Loss: 0.4650\n",
            "Epoch [1/10], Batch [1785], Loss: 0.4433\n",
            "Epoch [1/10], Batch [1790], Loss: 0.5333\n",
            "Epoch [1/10], Batch [1795], Loss: 0.3816\n",
            "Epoch [1/10], Batch [1800], Loss: 0.3696\n",
            "Epoch [1/10], Batch [1805], Loss: 0.4823\n",
            "Epoch [1/10], Batch [1810], Loss: 0.3209\n",
            "Epoch [1/10], Batch [1815], Loss: 0.3826\n",
            "Epoch [1/10], Batch [1820], Loss: 0.4293\n",
            "Epoch [1/10], Batch [1825], Loss: 0.4689\n",
            "Epoch [1/10], Batch [1830], Loss: 0.4055\n",
            "Epoch [1/10], Batch [1835], Loss: 0.3735\n",
            "Epoch [1/10], Batch [1840], Loss: 0.3495\n",
            "Epoch [1/10], Batch [1845], Loss: 0.3997\n",
            "Epoch [1/10], Batch [1850], Loss: 0.2784\n",
            "Epoch [1/10], Batch [1855], Loss: 0.2595\n",
            "Epoch [1/10], Batch [1860], Loss: 0.3673\n",
            "Epoch [1/10], Batch [1865], Loss: 0.4190\n",
            "Epoch [1/10], Batch [1870], Loss: 0.3721\n",
            "Epoch [1/10], Batch [1875], Loss: 0.4016\n",
            "Epoch [1/10], Batch [1880], Loss: 0.5650\n",
            "Epoch [1/10], Batch [1885], Loss: 0.4982\n",
            "Epoch [1/10], Batch [1890], Loss: 0.4994\n",
            "Epoch [1/10], Batch [1895], Loss: 0.4845\n",
            "Epoch [1/10], Batch [1900], Loss: 0.6134\n",
            "Epoch [1/10], Batch [1905], Loss: 0.2675\n",
            "Epoch [1/10], Batch [1910], Loss: 0.4325\n",
            "Epoch [1/10], Batch [1915], Loss: 0.3183\n",
            "Epoch [1/10], Batch [1920], Loss: 0.4423\n",
            "Epoch [1/10], Batch [1925], Loss: 0.4192\n",
            "Epoch [1/10], Batch [1930], Loss: 0.3450\n",
            "Epoch [1/10], Batch [1935], Loss: 0.3051\n",
            "Epoch [1/10], Batch [1940], Loss: 0.3754\n",
            "Epoch [1/10], Batch [1945], Loss: 0.3551\n",
            "Epoch [1/10], Batch [1950], Loss: 0.5303\n",
            "Epoch [1/10], Batch [1955], Loss: 0.4112\n",
            "Epoch [1/10], Batch [1960], Loss: 0.5267\n",
            "Epoch [1/10], Batch [1965], Loss: 0.6040\n",
            "Epoch [1/10], Batch [1970], Loss: 0.6812\n",
            "Epoch [1/10], Batch [1975], Loss: 0.7692\n",
            "Epoch [1/10], Batch [1980], Loss: 0.4209\n",
            "Epoch [1/10], Batch [1985], Loss: 0.4843\n",
            "Epoch [1/10], Batch [1990], Loss: 0.4059\n",
            "Epoch [1/10], Batch [1995], Loss: 0.3263\n",
            "Epoch [1/10], Batch [2000], Loss: 0.3236\n",
            "Epoch [1/10], Batch [2005], Loss: 0.5901\n",
            "Epoch [1/10], Batch [2010], Loss: 0.4798\n",
            "Epoch [1/10], Batch [2015], Loss: 0.5239\n",
            "Epoch [1/10], Batch [2020], Loss: 0.3674\n",
            "Epoch [1/10], Batch [2025], Loss: 0.3253\n",
            "Epoch [1/10], Batch [2030], Loss: 0.4046\n",
            "Epoch [1/10], Batch [2035], Loss: 0.3283\n",
            "Epoch [1/10], Batch [2040], Loss: 0.3791\n",
            "Epoch [1/10], Batch [2045], Loss: 0.3595\n",
            "Epoch [1/10], Batch [2050], Loss: 0.4856\n",
            "Epoch [1/10], Batch [2055], Loss: 0.3951\n",
            "Epoch [1/10], Batch [2060], Loss: 0.4879\n",
            "Epoch [1/10], Average Loss: 0.4437\n",
            "Epoch [2/10], Batch [0], Loss: 0.4796\n",
            "Epoch [2/10], Batch [5], Loss: 0.4523\n",
            "Epoch [2/10], Batch [10], Loss: 0.3615\n",
            "Epoch [2/10], Batch [15], Loss: 0.4651\n",
            "Epoch [2/10], Batch [20], Loss: 0.3987\n",
            "Epoch [2/10], Batch [25], Loss: 0.5512\n",
            "Epoch [2/10], Batch [30], Loss: 0.4364\n",
            "Epoch [2/10], Batch [35], Loss: 0.5423\n",
            "Epoch [2/10], Batch [40], Loss: 0.2942\n",
            "Epoch [2/10], Batch [45], Loss: 0.3658\n",
            "Epoch [2/10], Batch [50], Loss: 0.5656\n",
            "Epoch [2/10], Batch [55], Loss: 0.4438\n",
            "Epoch [2/10], Batch [60], Loss: 0.4638\n",
            "Epoch [2/10], Batch [65], Loss: 0.4309\n",
            "Epoch [2/10], Batch [70], Loss: 0.5067\n",
            "Epoch [2/10], Batch [75], Loss: 0.5309\n",
            "Epoch [2/10], Batch [80], Loss: 0.4004\n",
            "Epoch [2/10], Batch [85], Loss: 0.4910\n",
            "Epoch [2/10], Batch [90], Loss: 0.5832\n",
            "Epoch [2/10], Batch [95], Loss: 0.6165\n",
            "Epoch [2/10], Batch [100], Loss: 0.2496\n",
            "Epoch [2/10], Batch [105], Loss: 0.3530\n",
            "Epoch [2/10], Batch [110], Loss: 0.3791\n",
            "Epoch [2/10], Batch [115], Loss: 0.5545\n",
            "Epoch [2/10], Batch [120], Loss: 0.3885\n",
            "Epoch [2/10], Batch [125], Loss: 0.5036\n",
            "Epoch [2/10], Batch [130], Loss: 0.3734\n",
            "Epoch [2/10], Batch [135], Loss: 0.4675\n",
            "Epoch [2/10], Batch [140], Loss: 0.3776\n",
            "Epoch [2/10], Batch [145], Loss: 0.4260\n",
            "Epoch [2/10], Batch [150], Loss: 0.4387\n",
            "Epoch [2/10], Batch [155], Loss: 0.5098\n",
            "Epoch [2/10], Batch [160], Loss: 0.4806\n",
            "Epoch [2/10], Batch [165], Loss: 0.3247\n",
            "Epoch [2/10], Batch [170], Loss: 0.2208\n",
            "Epoch [2/10], Batch [175], Loss: 0.4507\n",
            "Epoch [2/10], Batch [180], Loss: 0.5133\n",
            "Epoch [2/10], Batch [185], Loss: 0.4397\n",
            "Epoch [2/10], Batch [190], Loss: 0.2956\n",
            "Epoch [2/10], Batch [195], Loss: 0.5547\n",
            "Epoch [2/10], Batch [200], Loss: 0.5772\n",
            "Epoch [2/10], Batch [205], Loss: 0.4739\n",
            "Epoch [2/10], Batch [210], Loss: 0.2718\n",
            "Epoch [2/10], Batch [215], Loss: 0.5323\n",
            "Epoch [2/10], Batch [220], Loss: 0.5248\n",
            "Epoch [2/10], Batch [225], Loss: 0.6396\n",
            "Epoch [2/10], Batch [230], Loss: 0.4589\n",
            "Epoch [2/10], Batch [235], Loss: 0.3590\n",
            "Epoch [2/10], Batch [240], Loss: 0.3744\n",
            "Epoch [2/10], Batch [245], Loss: 0.2628\n",
            "Epoch [2/10], Batch [250], Loss: 0.3487\n",
            "Epoch [2/10], Batch [255], Loss: 0.3301\n",
            "Epoch [2/10], Batch [260], Loss: 0.3637\n",
            "Epoch [2/10], Batch [265], Loss: 0.3271\n",
            "Epoch [2/10], Batch [270], Loss: 0.4163\n",
            "Epoch [2/10], Batch [275], Loss: 0.4494\n",
            "Epoch [2/10], Batch [280], Loss: 0.3802\n",
            "Epoch [2/10], Batch [285], Loss: 0.5416\n",
            "Epoch [2/10], Batch [290], Loss: 0.3995\n",
            "Epoch [2/10], Batch [295], Loss: 0.3426\n",
            "Epoch [2/10], Batch [300], Loss: 0.2999\n",
            "Epoch [2/10], Batch [305], Loss: 0.6659\n",
            "Epoch [2/10], Batch [310], Loss: 0.2734\n",
            "Epoch [2/10], Batch [315], Loss: 0.3404\n",
            "Epoch [2/10], Batch [320], Loss: 0.3371\n",
            "Epoch [2/10], Batch [325], Loss: 0.2121\n",
            "Epoch [2/10], Batch [330], Loss: 0.3674\n",
            "Epoch [2/10], Batch [335], Loss: 0.3965\n",
            "Epoch [2/10], Batch [340], Loss: 0.4695\n",
            "Epoch [2/10], Batch [345], Loss: 0.3927\n",
            "Epoch [2/10], Batch [350], Loss: 0.4263\n",
            "Epoch [2/10], Batch [355], Loss: 0.3774\n",
            "Epoch [2/10], Batch [360], Loss: 0.3500\n",
            "Epoch [2/10], Batch [365], Loss: 0.2828\n",
            "Epoch [2/10], Batch [370], Loss: 0.4296\n",
            "Epoch [2/10], Batch [375], Loss: 0.3002\n",
            "Epoch [2/10], Batch [380], Loss: 0.3169\n",
            "Epoch [2/10], Batch [385], Loss: 0.4080\n",
            "Epoch [2/10], Batch [390], Loss: 0.5051\n",
            "Epoch [2/10], Batch [395], Loss: 0.3492\n",
            "Epoch [2/10], Batch [400], Loss: 0.2654\n",
            "Epoch [2/10], Batch [405], Loss: 0.2351\n",
            "Epoch [2/10], Batch [410], Loss: 0.3311\n",
            "Epoch [2/10], Batch [415], Loss: 0.4667\n",
            "Epoch [2/10], Batch [420], Loss: 0.3473\n",
            "Epoch [2/10], Batch [425], Loss: 0.5409\n",
            "Epoch [2/10], Batch [430], Loss: 0.2990\n",
            "Epoch [2/10], Batch [435], Loss: 0.4438\n",
            "Epoch [2/10], Batch [440], Loss: 0.6896\n",
            "Epoch [2/10], Batch [445], Loss: 0.6897\n",
            "Epoch [2/10], Batch [450], Loss: 0.4825\n",
            "Epoch [2/10], Batch [455], Loss: 0.3613\n",
            "Epoch [2/10], Batch [460], Loss: 0.3333\n",
            "Epoch [2/10], Batch [465], Loss: 0.3146\n",
            "Epoch [2/10], Batch [470], Loss: 0.3761\n",
            "Epoch [2/10], Batch [475], Loss: 0.5459\n",
            "Epoch [2/10], Batch [480], Loss: 0.3802\n",
            "Epoch [2/10], Batch [485], Loss: 0.5878\n",
            "Epoch [2/10], Batch [490], Loss: 0.3950\n",
            "Epoch [2/10], Batch [495], Loss: 0.2990\n",
            "Epoch [2/10], Batch [500], Loss: 0.5710\n",
            "Epoch [2/10], Batch [505], Loss: 0.3443\n",
            "Epoch [2/10], Batch [510], Loss: 0.2718\n",
            "Epoch [2/10], Batch [515], Loss: 0.4915\n",
            "Epoch [2/10], Batch [520], Loss: 0.3751\n",
            "Epoch [2/10], Batch [525], Loss: 0.3902\n",
            "Epoch [2/10], Batch [530], Loss: 0.3216\n",
            "Epoch [2/10], Batch [535], Loss: 0.3457\n",
            "Epoch [2/10], Batch [540], Loss: 0.3559\n",
            "Epoch [2/10], Batch [545], Loss: 0.4361\n",
            "Epoch [2/10], Batch [550], Loss: 0.3252\n",
            "Epoch [2/10], Batch [555], Loss: 0.3863\n",
            "Epoch [2/10], Batch [560], Loss: 0.4069\n",
            "Epoch [2/10], Batch [565], Loss: 0.3998\n",
            "Epoch [2/10], Batch [570], Loss: 0.3871\n",
            "Epoch [2/10], Batch [575], Loss: 0.5427\n",
            "Epoch [2/10], Batch [580], Loss: 0.5387\n",
            "Epoch [2/10], Batch [585], Loss: 0.4310\n",
            "Epoch [2/10], Batch [590], Loss: 0.4413\n",
            "Epoch [2/10], Batch [595], Loss: 0.3102\n",
            "Epoch [2/10], Batch [600], Loss: 0.5380\n",
            "Epoch [2/10], Batch [605], Loss: 0.5306\n",
            "Epoch [2/10], Batch [610], Loss: 0.4662\n",
            "Epoch [2/10], Batch [615], Loss: 0.4471\n",
            "Epoch [2/10], Batch [620], Loss: 0.3509\n",
            "Epoch [2/10], Batch [625], Loss: 0.5358\n",
            "Epoch [2/10], Batch [630], Loss: 0.5358\n",
            "Epoch [2/10], Batch [635], Loss: 0.5264\n",
            "Epoch [2/10], Batch [640], Loss: 0.3464\n",
            "Epoch [2/10], Batch [645], Loss: 0.5575\n",
            "Epoch [2/10], Batch [650], Loss: 0.7149\n",
            "Epoch [2/10], Batch [655], Loss: 0.4919\n",
            "Epoch [2/10], Batch [660], Loss: 0.5616\n",
            "Epoch [2/10], Batch [665], Loss: 0.5399\n",
            "Epoch [2/10], Batch [670], Loss: 0.3438\n",
            "Epoch [2/10], Batch [675], Loss: 0.5133\n",
            "Epoch [2/10], Batch [680], Loss: 0.4957\n",
            "Epoch [2/10], Batch [685], Loss: 0.2733\n",
            "Epoch [2/10], Batch [690], Loss: 0.4091\n",
            "Epoch [2/10], Batch [695], Loss: 0.5061\n",
            "Epoch [2/10], Batch [700], Loss: 0.3971\n",
            "Epoch [2/10], Batch [705], Loss: 0.3132\n",
            "Epoch [2/10], Batch [710], Loss: 0.4101\n",
            "Epoch [2/10], Batch [715], Loss: 0.4100\n",
            "Epoch [2/10], Batch [720], Loss: 0.4961\n",
            "Epoch [2/10], Batch [725], Loss: 0.6215\n",
            "Epoch [2/10], Batch [730], Loss: 0.3492\n",
            "Epoch [2/10], Batch [735], Loss: 0.4535\n",
            "Epoch [2/10], Batch [740], Loss: 0.4525\n",
            "Epoch [2/10], Batch [745], Loss: 0.4443\n",
            "Epoch [2/10], Batch [750], Loss: 0.5691\n",
            "Epoch [2/10], Batch [755], Loss: 0.2976\n",
            "Epoch [2/10], Batch [760], Loss: 0.3451\n",
            "Epoch [2/10], Batch [765], Loss: 0.3761\n",
            "Epoch [2/10], Batch [770], Loss: 0.4121\n",
            "Epoch [2/10], Batch [775], Loss: 0.3114\n",
            "Epoch [2/10], Batch [780], Loss: 0.3377\n",
            "Epoch [2/10], Batch [785], Loss: 0.3781\n",
            "Epoch [2/10], Batch [790], Loss: 0.3420\n",
            "Epoch [2/10], Batch [795], Loss: 0.4296\n",
            "Epoch [2/10], Batch [800], Loss: 0.2760\n",
            "Epoch [2/10], Batch [805], Loss: 0.6916\n",
            "Epoch [2/10], Batch [810], Loss: 0.5300\n",
            "Epoch [2/10], Batch [815], Loss: 0.3365\n",
            "Epoch [2/10], Batch [820], Loss: 0.3056\n",
            "Epoch [2/10], Batch [825], Loss: 0.4667\n",
            "Epoch [2/10], Batch [830], Loss: 0.5174\n",
            "Epoch [2/10], Batch [835], Loss: 0.4299\n",
            "Epoch [2/10], Batch [840], Loss: 0.3444\n",
            "Epoch [2/10], Batch [845], Loss: 0.3405\n",
            "Epoch [2/10], Batch [850], Loss: 0.4026\n",
            "Epoch [2/10], Batch [855], Loss: 0.5627\n",
            "Epoch [2/10], Batch [860], Loss: 0.5787\n",
            "Epoch [2/10], Batch [865], Loss: 0.3907\n",
            "Epoch [2/10], Batch [870], Loss: 0.3334\n",
            "Epoch [2/10], Batch [875], Loss: 0.3709\n",
            "Epoch [2/10], Batch [880], Loss: 0.3920\n",
            "Epoch [2/10], Batch [885], Loss: 0.4586\n",
            "Epoch [2/10], Batch [890], Loss: 0.4942\n",
            "Epoch [2/10], Batch [895], Loss: 0.3672\n",
            "Epoch [2/10], Batch [900], Loss: 0.4163\n",
            "Epoch [2/10], Batch [905], Loss: 0.4271\n",
            "Epoch [2/10], Batch [910], Loss: 0.4201\n",
            "Epoch [2/10], Batch [915], Loss: 0.4687\n",
            "Epoch [2/10], Batch [920], Loss: 0.4489\n",
            "Epoch [2/10], Batch [925], Loss: 0.4372\n",
            "Epoch [2/10], Batch [930], Loss: 0.2501\n",
            "Epoch [2/10], Batch [935], Loss: 0.6145\n",
            "Epoch [2/10], Batch [940], Loss: 0.3671\n",
            "Epoch [2/10], Batch [945], Loss: 0.2884\n",
            "Epoch [2/10], Batch [950], Loss: 0.3735\n",
            "Epoch [2/10], Batch [955], Loss: 0.3460\n",
            "Epoch [2/10], Batch [960], Loss: 0.3694\n",
            "Epoch [2/10], Batch [965], Loss: 0.4664\n",
            "Epoch [2/10], Batch [970], Loss: 0.2441\n",
            "Epoch [2/10], Batch [975], Loss: 0.3461\n",
            "Epoch [2/10], Batch [980], Loss: 0.3351\n",
            "Epoch [2/10], Batch [985], Loss: 0.4716\n",
            "Epoch [2/10], Batch [990], Loss: 0.3806\n",
            "Epoch [2/10], Batch [995], Loss: 0.3658\n",
            "Epoch [2/10], Batch [1000], Loss: 0.3349\n",
            "Epoch [2/10], Batch [1005], Loss: 0.4525\n",
            "Epoch [2/10], Batch [1010], Loss: 0.5597\n",
            "Epoch [2/10], Batch [1015], Loss: 0.3951\n",
            "Epoch [2/10], Batch [1020], Loss: 0.3301\n",
            "Epoch [2/10], Batch [1025], Loss: 0.5642\n",
            "Epoch [2/10], Batch [1030], Loss: 0.4410\n",
            "Epoch [2/10], Batch [1035], Loss: 0.2933\n",
            "Epoch [2/10], Batch [1040], Loss: 0.4451\n",
            "Epoch [2/10], Batch [1045], Loss: 0.3465\n",
            "Epoch [2/10], Batch [1050], Loss: 0.4331\n",
            "Epoch [2/10], Batch [1055], Loss: 0.4686\n",
            "Epoch [2/10], Batch [1060], Loss: 0.3350\n",
            "Epoch [2/10], Batch [1065], Loss: 0.2601\n",
            "Epoch [2/10], Batch [1070], Loss: 0.3656\n",
            "Epoch [2/10], Batch [1075], Loss: 0.3735\n",
            "Epoch [2/10], Batch [1080], Loss: 0.4651\n",
            "Epoch [2/10], Batch [1085], Loss: 0.4087\n",
            "Epoch [2/10], Batch [1090], Loss: 0.6714\n",
            "Epoch [2/10], Batch [1095], Loss: 0.5304\n",
            "Epoch [2/10], Batch [1100], Loss: 0.4921\n",
            "Epoch [2/10], Batch [1105], Loss: 0.3774\n",
            "Epoch [2/10], Batch [1110], Loss: 0.3857\n",
            "Epoch [2/10], Batch [1115], Loss: 0.3327\n",
            "Epoch [2/10], Batch [1120], Loss: 0.3586\n",
            "Epoch [2/10], Batch [1125], Loss: 0.5187\n",
            "Epoch [2/10], Batch [1130], Loss: 0.3083\n",
            "Epoch [2/10], Batch [1135], Loss: 0.4929\n",
            "Epoch [2/10], Batch [1140], Loss: 0.3326\n",
            "Epoch [2/10], Batch [1145], Loss: 0.4093\n",
            "Epoch [2/10], Batch [1150], Loss: 0.5233\n",
            "Epoch [2/10], Batch [1155], Loss: 0.2868\n",
            "Epoch [2/10], Batch [1160], Loss: 0.3251\n",
            "Epoch [2/10], Batch [1165], Loss: 0.3424\n",
            "Epoch [2/10], Batch [1170], Loss: 0.2627\n",
            "Epoch [2/10], Batch [1175], Loss: 0.2291\n",
            "Epoch [2/10], Batch [1180], Loss: 0.3724\n",
            "Epoch [2/10], Batch [1185], Loss: 0.4141\n",
            "Epoch [2/10], Batch [1190], Loss: 0.5580\n",
            "Epoch [2/10], Batch [1195], Loss: 0.4685\n",
            "Epoch [2/10], Batch [1200], Loss: 0.3466\n",
            "Epoch [2/10], Batch [1205], Loss: 0.4649\n",
            "Epoch [2/10], Batch [1210], Loss: 0.3955\n",
            "Epoch [2/10], Batch [1215], Loss: 0.4502\n",
            "Epoch [2/10], Batch [1220], Loss: 0.3955\n",
            "Epoch [2/10], Batch [1225], Loss: 0.4174\n",
            "Epoch [2/10], Batch [1230], Loss: 0.4802\n",
            "Epoch [2/10], Batch [1235], Loss: 0.3282\n",
            "Epoch [2/10], Batch [1240], Loss: 0.5082\n",
            "Epoch [2/10], Batch [1245], Loss: 0.3426\n",
            "Epoch [2/10], Batch [1250], Loss: 0.4189\n",
            "Epoch [2/10], Batch [1255], Loss: 0.5122\n",
            "Epoch [2/10], Batch [1260], Loss: 0.3959\n",
            "Epoch [2/10], Batch [1265], Loss: 0.3720\n",
            "Epoch [2/10], Batch [1270], Loss: 0.3159\n",
            "Epoch [2/10], Batch [1275], Loss: 0.3360\n",
            "Epoch [2/10], Batch [1280], Loss: 0.3934\n",
            "Epoch [2/10], Batch [1285], Loss: 0.2442\n",
            "Epoch [2/10], Batch [1290], Loss: 0.3654\n",
            "Epoch [2/10], Batch [1295], Loss: 0.3858\n",
            "Epoch [2/10], Batch [1300], Loss: 0.4087\n",
            "Epoch [2/10], Batch [1305], Loss: 0.4564\n",
            "Epoch [2/10], Batch [1310], Loss: 0.3352\n",
            "Epoch [2/10], Batch [1315], Loss: 0.5006\n",
            "Epoch [2/10], Batch [1320], Loss: 0.3588\n",
            "Epoch [2/10], Batch [1325], Loss: 0.3651\n",
            "Epoch [2/10], Batch [1330], Loss: 0.4054\n",
            "Epoch [2/10], Batch [1335], Loss: 0.3557\n",
            "Epoch [2/10], Batch [1340], Loss: 0.4385\n",
            "Epoch [2/10], Batch [1345], Loss: 0.3864\n",
            "Epoch [2/10], Batch [1350], Loss: 0.4612\n",
            "Epoch [2/10], Batch [1355], Loss: 0.6587\n",
            "Epoch [2/10], Batch [1360], Loss: 0.3862\n",
            "Epoch [2/10], Batch [1365], Loss: 0.4575\n",
            "Epoch [2/10], Batch [1370], Loss: 0.4532\n",
            "Epoch [2/10], Batch [1375], Loss: 0.4008\n",
            "Epoch [2/10], Batch [1380], Loss: 0.5325\n",
            "Epoch [2/10], Batch [1385], Loss: 0.6571\n",
            "Epoch [2/10], Batch [1390], Loss: 0.6326\n",
            "Epoch [2/10], Batch [1395], Loss: 0.4484\n",
            "Epoch [2/10], Batch [1400], Loss: 0.4651\n",
            "Epoch [2/10], Batch [1405], Loss: 0.3478\n",
            "Epoch [2/10], Batch [1410], Loss: 0.3036\n",
            "Epoch [2/10], Batch [1415], Loss: 0.4165\n",
            "Epoch [2/10], Batch [1420], Loss: 0.2864\n",
            "Epoch [2/10], Batch [1425], Loss: 0.3406\n",
            "Epoch [2/10], Batch [1430], Loss: 0.7116\n",
            "Epoch [2/10], Batch [1435], Loss: 0.6149\n",
            "Epoch [2/10], Batch [1440], Loss: 0.4993\n",
            "Epoch [2/10], Batch [1445], Loss: 0.5564\n",
            "Epoch [2/10], Batch [1450], Loss: 0.4194\n",
            "Epoch [2/10], Batch [1455], Loss: 0.3396\n",
            "Epoch [2/10], Batch [1460], Loss: 0.2744\n",
            "Epoch [2/10], Batch [1465], Loss: 0.4595\n",
            "Epoch [2/10], Batch [1470], Loss: 0.3640\n",
            "Epoch [2/10], Batch [1475], Loss: 0.2501\n",
            "Epoch [2/10], Batch [1480], Loss: 0.4164\n",
            "Epoch [2/10], Batch [1485], Loss: 0.5384\n",
            "Epoch [2/10], Batch [1490], Loss: 0.4786\n",
            "Epoch [2/10], Batch [1495], Loss: 0.4912\n",
            "Epoch [2/10], Batch [1500], Loss: 0.2417\n",
            "Epoch [2/10], Batch [1505], Loss: 0.4329\n",
            "Epoch [2/10], Batch [1510], Loss: 0.3446\n",
            "Epoch [2/10], Batch [1515], Loss: 0.3421\n",
            "Epoch [2/10], Batch [1520], Loss: 0.3227\n",
            "Epoch [2/10], Batch [1525], Loss: 0.3882\n",
            "Epoch [2/10], Batch [1530], Loss: 0.4094\n",
            "Epoch [2/10], Batch [1535], Loss: 0.5230\n",
            "Epoch [2/10], Batch [1540], Loss: 0.4432\n",
            "Epoch [2/10], Batch [1545], Loss: 0.4386\n",
            "Epoch [2/10], Batch [1550], Loss: 0.4330\n",
            "Epoch [2/10], Batch [1555], Loss: 0.2876\n",
            "Epoch [2/10], Batch [1560], Loss: 0.4048\n",
            "Epoch [2/10], Batch [1565], Loss: 0.4188\n",
            "Epoch [2/10], Batch [1570], Loss: 0.5710\n",
            "Epoch [2/10], Batch [1575], Loss: 0.5074\n",
            "Epoch [2/10], Batch [1580], Loss: 0.3773\n",
            "Epoch [2/10], Batch [1585], Loss: 0.3596\n",
            "Epoch [2/10], Batch [1590], Loss: 0.5327\n",
            "Epoch [2/10], Batch [1595], Loss: 0.3573\n",
            "Epoch [2/10], Batch [1600], Loss: 0.4033\n",
            "Epoch [2/10], Batch [1605], Loss: 0.5232\n",
            "Epoch [2/10], Batch [1610], Loss: 0.3401\n",
            "Epoch [2/10], Batch [1615], Loss: 0.4676\n",
            "Epoch [2/10], Batch [1620], Loss: 0.4233\n",
            "Epoch [2/10], Batch [1625], Loss: 0.4034\n",
            "Epoch [2/10], Batch [1630], Loss: 0.3114\n",
            "Epoch [2/10], Batch [1635], Loss: 0.3253\n",
            "Epoch [2/10], Batch [1640], Loss: 0.4816\n",
            "Epoch [2/10], Batch [1645], Loss: 0.4204\n",
            "Epoch [2/10], Batch [1650], Loss: 0.3929\n",
            "Epoch [2/10], Batch [1655], Loss: 0.4699\n",
            "Epoch [2/10], Batch [1660], Loss: 0.3349\n",
            "Epoch [2/10], Batch [1665], Loss: 0.2991\n",
            "Epoch [2/10], Batch [1670], Loss: 0.3211\n",
            "Epoch [2/10], Batch [1675], Loss: 0.5693\n",
            "Epoch [2/10], Batch [1680], Loss: 0.4534\n",
            "Epoch [2/10], Batch [1685], Loss: 0.5142\n",
            "Epoch [2/10], Batch [1690], Loss: 0.4047\n",
            "Epoch [2/10], Batch [1695], Loss: 0.3113\n",
            "Epoch [2/10], Batch [1700], Loss: 0.3588\n",
            "Epoch [2/10], Batch [1705], Loss: 0.3309\n",
            "Epoch [2/10], Batch [1710], Loss: 0.5817\n",
            "Epoch [2/10], Batch [1715], Loss: 0.4071\n",
            "Epoch [2/10], Batch [1720], Loss: 0.4318\n",
            "Epoch [2/10], Batch [1725], Loss: 0.5114\n",
            "Epoch [2/10], Batch [1730], Loss: 0.3619\n",
            "Epoch [2/10], Batch [1735], Loss: 0.3562\n",
            "Epoch [2/10], Batch [1740], Loss: 0.3592\n",
            "Epoch [2/10], Batch [1745], Loss: 0.3389\n",
            "Epoch [2/10], Batch [1750], Loss: 0.5420\n",
            "Epoch [2/10], Batch [1755], Loss: 0.3286\n",
            "Epoch [2/10], Batch [1760], Loss: 0.2224\n",
            "Epoch [2/10], Batch [1765], Loss: 0.4096\n",
            "Epoch [2/10], Batch [1770], Loss: 0.3468\n",
            "Epoch [2/10], Batch [1775], Loss: 0.3832\n",
            "Epoch [2/10], Batch [1780], Loss: 0.4453\n",
            "Epoch [2/10], Batch [1785], Loss: 0.4153\n",
            "Epoch [2/10], Batch [1790], Loss: 0.5182\n",
            "Epoch [2/10], Batch [1795], Loss: 0.3710\n",
            "Epoch [2/10], Batch [1800], Loss: 0.3592\n",
            "Epoch [2/10], Batch [1805], Loss: 0.4482\n",
            "Epoch [2/10], Batch [1810], Loss: 0.3148\n",
            "Epoch [2/10], Batch [1815], Loss: 0.3708\n",
            "Epoch [2/10], Batch [1820], Loss: 0.4259\n",
            "Epoch [2/10], Batch [1825], Loss: 0.4368\n",
            "Epoch [2/10], Batch [1830], Loss: 0.3915\n",
            "Epoch [2/10], Batch [1835], Loss: 0.3458\n",
            "Epoch [2/10], Batch [1840], Loss: 0.3368\n",
            "Epoch [2/10], Batch [1845], Loss: 0.3529\n",
            "Epoch [2/10], Batch [1850], Loss: 0.2570\n",
            "Epoch [2/10], Batch [1855], Loss: 0.2478\n",
            "Epoch [2/10], Batch [1860], Loss: 0.3377\n",
            "Epoch [2/10], Batch [1865], Loss: 0.4097\n",
            "Epoch [2/10], Batch [1870], Loss: 0.3467\n",
            "Epoch [2/10], Batch [1875], Loss: 0.4071\n",
            "Epoch [2/10], Batch [1880], Loss: 0.5592\n",
            "Epoch [2/10], Batch [1885], Loss: 0.4743\n",
            "Epoch [2/10], Batch [1890], Loss: 0.4744\n",
            "Epoch [2/10], Batch [1895], Loss: 0.4489\n",
            "Epoch [2/10], Batch [1900], Loss: 0.5741\n",
            "Epoch [2/10], Batch [1905], Loss: 0.2744\n",
            "Epoch [2/10], Batch [1910], Loss: 0.4150\n",
            "Epoch [2/10], Batch [1915], Loss: 0.3254\n",
            "Epoch [2/10], Batch [1920], Loss: 0.4411\n",
            "Epoch [2/10], Batch [1925], Loss: 0.4006\n",
            "Epoch [2/10], Batch [1930], Loss: 0.3464\n",
            "Epoch [2/10], Batch [1935], Loss: 0.2988\n",
            "Epoch [2/10], Batch [1940], Loss: 0.3739\n",
            "Epoch [2/10], Batch [1945], Loss: 0.3458\n",
            "Epoch [2/10], Batch [1950], Loss: 0.4976\n",
            "Epoch [2/10], Batch [1955], Loss: 0.3862\n",
            "Epoch [2/10], Batch [1960], Loss: 0.5205\n",
            "Epoch [2/10], Batch [1965], Loss: 0.5821\n",
            "Epoch [2/10], Batch [1970], Loss: 0.6645\n",
            "Epoch [2/10], Batch [1975], Loss: 0.7389\n",
            "Epoch [2/10], Batch [1980], Loss: 0.3872\n",
            "Epoch [2/10], Batch [1985], Loss: 0.4694\n",
            "Epoch [2/10], Batch [1990], Loss: 0.3871\n",
            "Epoch [2/10], Batch [1995], Loss: 0.3233\n",
            "Epoch [2/10], Batch [2000], Loss: 0.2820\n",
            "Epoch [2/10], Batch [2005], Loss: 0.4870\n",
            "Epoch [2/10], Batch [2010], Loss: 0.4852\n",
            "Epoch [2/10], Batch [2015], Loss: 0.5265\n",
            "Epoch [2/10], Batch [2020], Loss: 0.3650\n",
            "Epoch [2/10], Batch [2025], Loss: 0.3171\n",
            "Epoch [2/10], Batch [2030], Loss: 0.3799\n",
            "Epoch [2/10], Batch [2035], Loss: 0.3073\n",
            "Epoch [2/10], Batch [2040], Loss: 0.3697\n",
            "Epoch [2/10], Batch [2045], Loss: 0.3339\n",
            "Epoch [2/10], Batch [2050], Loss: 0.4811\n",
            "Epoch [2/10], Batch [2055], Loss: 0.3944\n",
            "Epoch [2/10], Batch [2060], Loss: 0.4699\n",
            "Epoch [2/10], Average Loss: 0.4229\n",
            "Epoch [3/10], Batch [0], Loss: 0.4783\n",
            "Epoch [3/10], Batch [5], Loss: 0.4144\n",
            "Epoch [3/10], Batch [10], Loss: 0.3625\n",
            "Epoch [3/10], Batch [15], Loss: 0.4587\n",
            "Epoch [3/10], Batch [20], Loss: 0.3959\n",
            "Epoch [3/10], Batch [25], Loss: 0.5435\n",
            "Epoch [3/10], Batch [30], Loss: 0.4427\n",
            "Epoch [3/10], Batch [35], Loss: 0.5422\n",
            "Epoch [3/10], Batch [40], Loss: 0.2807\n",
            "Epoch [3/10], Batch [45], Loss: 0.3559\n",
            "Epoch [3/10], Batch [50], Loss: 0.5155\n",
            "Epoch [3/10], Batch [55], Loss: 0.4159\n",
            "Epoch [3/10], Batch [60], Loss: 0.4568\n",
            "Epoch [3/10], Batch [65], Loss: 0.3865\n",
            "Epoch [3/10], Batch [70], Loss: 0.4770\n",
            "Epoch [3/10], Batch [75], Loss: 0.5040\n",
            "Epoch [3/10], Batch [80], Loss: 0.4045\n",
            "Epoch [3/10], Batch [85], Loss: 0.4933\n",
            "Epoch [3/10], Batch [90], Loss: 0.5351\n",
            "Epoch [3/10], Batch [95], Loss: 0.6194\n",
            "Epoch [3/10], Batch [100], Loss: 0.2386\n",
            "Epoch [3/10], Batch [105], Loss: 0.3471\n",
            "Epoch [3/10], Batch [110], Loss: 0.3917\n",
            "Epoch [3/10], Batch [115], Loss: 0.5028\n",
            "Epoch [3/10], Batch [120], Loss: 0.3719\n",
            "Epoch [3/10], Batch [125], Loss: 0.4832\n",
            "Epoch [3/10], Batch [130], Loss: 0.3632\n",
            "Epoch [3/10], Batch [135], Loss: 0.4731\n",
            "Epoch [3/10], Batch [140], Loss: 0.3683\n",
            "Epoch [3/10], Batch [145], Loss: 0.4060\n",
            "Epoch [3/10], Batch [150], Loss: 0.4112\n",
            "Epoch [3/10], Batch [155], Loss: 0.4769\n",
            "Epoch [3/10], Batch [160], Loss: 0.4534\n",
            "Epoch [3/10], Batch [165], Loss: 0.3158\n",
            "Epoch [3/10], Batch [170], Loss: 0.2088\n",
            "Epoch [3/10], Batch [175], Loss: 0.4351\n",
            "Epoch [3/10], Batch [180], Loss: 0.4793\n",
            "Epoch [3/10], Batch [185], Loss: 0.4265\n",
            "Epoch [3/10], Batch [190], Loss: 0.2903\n",
            "Epoch [3/10], Batch [195], Loss: 0.5271\n",
            "Epoch [3/10], Batch [200], Loss: 0.5612\n",
            "Epoch [3/10], Batch [205], Loss: 0.4521\n",
            "Epoch [3/10], Batch [210], Loss: 0.2650\n",
            "Epoch [3/10], Batch [215], Loss: 0.4975\n",
            "Epoch [3/10], Batch [220], Loss: 0.5000\n",
            "Epoch [3/10], Batch [225], Loss: 0.6250\n",
            "Epoch [3/10], Batch [230], Loss: 0.4559\n",
            "Epoch [3/10], Batch [235], Loss: 0.3378\n",
            "Epoch [3/10], Batch [240], Loss: 0.3681\n",
            "Epoch [3/10], Batch [245], Loss: 0.2608\n",
            "Epoch [3/10], Batch [250], Loss: 0.3519\n",
            "Epoch [3/10], Batch [255], Loss: 0.3346\n",
            "Epoch [3/10], Batch [260], Loss: 0.3552\n",
            "Epoch [3/10], Batch [265], Loss: 0.3123\n",
            "Epoch [3/10], Batch [270], Loss: 0.3847\n",
            "Epoch [3/10], Batch [275], Loss: 0.4178\n",
            "Epoch [3/10], Batch [280], Loss: 0.3547\n",
            "Epoch [3/10], Batch [285], Loss: 0.5359\n",
            "Epoch [3/10], Batch [290], Loss: 0.3638\n",
            "Epoch [3/10], Batch [295], Loss: 0.3199\n",
            "Epoch [3/10], Batch [300], Loss: 0.2878\n",
            "Epoch [3/10], Batch [305], Loss: 0.5601\n",
            "Epoch [3/10], Batch [310], Loss: 0.2747\n",
            "Epoch [3/10], Batch [315], Loss: 0.3233\n",
            "Epoch [3/10], Batch [320], Loss: 0.3090\n",
            "Epoch [3/10], Batch [325], Loss: 0.1963\n",
            "Epoch [3/10], Batch [330], Loss: 0.3553\n",
            "Epoch [3/10], Batch [335], Loss: 0.3629\n",
            "Epoch [3/10], Batch [340], Loss: 0.4664\n",
            "Epoch [3/10], Batch [345], Loss: 0.3918\n",
            "Epoch [3/10], Batch [350], Loss: 0.4239\n",
            "Epoch [3/10], Batch [355], Loss: 0.3753\n",
            "Epoch [3/10], Batch [360], Loss: 0.3269\n",
            "Epoch [3/10], Batch [365], Loss: 0.2561\n",
            "Epoch [3/10], Batch [370], Loss: 0.4005\n",
            "Epoch [3/10], Batch [375], Loss: 0.2928\n",
            "Epoch [3/10], Batch [380], Loss: 0.3062\n",
            "Epoch [3/10], Batch [385], Loss: 0.4031\n",
            "Epoch [3/10], Batch [390], Loss: 0.4915\n",
            "Epoch [3/10], Batch [395], Loss: 0.3348\n",
            "Epoch [3/10], Batch [400], Loss: 0.2508\n",
            "Epoch [3/10], Batch [405], Loss: 0.2272\n",
            "Epoch [3/10], Batch [410], Loss: 0.3016\n",
            "Epoch [3/10], Batch [415], Loss: 0.4485\n",
            "Epoch [3/10], Batch [420], Loss: 0.3569\n",
            "Epoch [3/10], Batch [425], Loss: 0.5444\n",
            "Epoch [3/10], Batch [430], Loss: 0.2900\n",
            "Epoch [3/10], Batch [435], Loss: 0.4299\n",
            "Epoch [3/10], Batch [440], Loss: 0.6260\n",
            "Epoch [3/10], Batch [445], Loss: 0.7142\n",
            "Epoch [3/10], Batch [450], Loss: 0.4590\n",
            "Epoch [3/10], Batch [455], Loss: 0.3667\n",
            "Epoch [3/10], Batch [460], Loss: 0.3268\n",
            "Epoch [3/10], Batch [465], Loss: 0.2977\n",
            "Epoch [3/10], Batch [470], Loss: 0.3704\n",
            "Epoch [3/10], Batch [475], Loss: 0.5292\n",
            "Epoch [3/10], Batch [480], Loss: 0.3702\n",
            "Epoch [3/10], Batch [485], Loss: 0.5730\n",
            "Epoch [3/10], Batch [490], Loss: 0.3557\n",
            "Epoch [3/10], Batch [495], Loss: 0.2794\n",
            "Epoch [3/10], Batch [500], Loss: 0.5559\n",
            "Epoch [3/10], Batch [505], Loss: 0.3239\n",
            "Epoch [3/10], Batch [510], Loss: 0.2676\n",
            "Epoch [3/10], Batch [515], Loss: 0.4498\n",
            "Epoch [3/10], Batch [520], Loss: 0.3645\n",
            "Epoch [3/10], Batch [525], Loss: 0.3840\n",
            "Epoch [3/10], Batch [530], Loss: 0.3025\n",
            "Epoch [3/10], Batch [535], Loss: 0.3325\n",
            "Epoch [3/10], Batch [540], Loss: 0.3580\n",
            "Epoch [3/10], Batch [545], Loss: 0.4270\n",
            "Epoch [3/10], Batch [550], Loss: 0.3145\n",
            "Epoch [3/10], Batch [555], Loss: 0.4091\n",
            "Epoch [3/10], Batch [560], Loss: 0.3315\n",
            "Epoch [3/10], Batch [565], Loss: 0.3718\n",
            "Epoch [3/10], Batch [570], Loss: 0.3836\n",
            "Epoch [3/10], Batch [575], Loss: 0.5136\n",
            "Epoch [3/10], Batch [580], Loss: 0.5217\n",
            "Epoch [3/10], Batch [585], Loss: 0.4236\n",
            "Epoch [3/10], Batch [590], Loss: 0.4318\n",
            "Epoch [3/10], Batch [595], Loss: 0.3095\n",
            "Epoch [3/10], Batch [600], Loss: 0.5027\n",
            "Epoch [3/10], Batch [605], Loss: 0.5036\n",
            "Epoch [3/10], Batch [610], Loss: 0.4678\n",
            "Epoch [3/10], Batch [615], Loss: 0.4441\n",
            "Epoch [3/10], Batch [620], Loss: 0.3392\n",
            "Epoch [3/10], Batch [625], Loss: 0.5130\n",
            "Epoch [3/10], Batch [630], Loss: 0.5008\n",
            "Epoch [3/10], Batch [635], Loss: 0.5287\n",
            "Epoch [3/10], Batch [640], Loss: 0.3304\n",
            "Epoch [3/10], Batch [645], Loss: 0.5301\n",
            "Epoch [3/10], Batch [650], Loss: 0.7017\n",
            "Epoch [3/10], Batch [655], Loss: 0.4616\n",
            "Epoch [3/10], Batch [660], Loss: 0.5347\n",
            "Epoch [3/10], Batch [665], Loss: 0.5316\n",
            "Epoch [3/10], Batch [670], Loss: 0.3378\n",
            "Epoch [3/10], Batch [675], Loss: 0.5146\n",
            "Epoch [3/10], Batch [680], Loss: 0.4655\n",
            "Epoch [3/10], Batch [685], Loss: 0.2623\n",
            "Epoch [3/10], Batch [690], Loss: 0.3891\n",
            "Epoch [3/10], Batch [695], Loss: 0.4603\n",
            "Epoch [3/10], Batch [700], Loss: 0.3880\n",
            "Epoch [3/10], Batch [705], Loss: 0.3098\n",
            "Epoch [3/10], Batch [710], Loss: 0.3898\n",
            "Epoch [3/10], Batch [715], Loss: 0.4072\n",
            "Epoch [3/10], Batch [720], Loss: 0.4672\n",
            "Epoch [3/10], Batch [725], Loss: 0.5913\n",
            "Epoch [3/10], Batch [730], Loss: 0.3339\n",
            "Epoch [3/10], Batch [735], Loss: 0.4399\n",
            "Epoch [3/10], Batch [740], Loss: 0.4379\n",
            "Epoch [3/10], Batch [745], Loss: 0.4302\n",
            "Epoch [3/10], Batch [750], Loss: 0.5675\n",
            "Epoch [3/10], Batch [755], Loss: 0.2841\n",
            "Epoch [3/10], Batch [760], Loss: 0.3262\n",
            "Epoch [3/10], Batch [765], Loss: 0.3580\n",
            "Epoch [3/10], Batch [770], Loss: 0.4048\n",
            "Epoch [3/10], Batch [775], Loss: 0.2833\n",
            "Epoch [3/10], Batch [780], Loss: 0.3362\n",
            "Epoch [3/10], Batch [785], Loss: 0.3580\n",
            "Epoch [3/10], Batch [790], Loss: 0.3177\n",
            "Epoch [3/10], Batch [795], Loss: 0.4013\n",
            "Epoch [3/10], Batch [800], Loss: 0.2684\n",
            "Epoch [3/10], Batch [805], Loss: 0.6312\n",
            "Epoch [3/10], Batch [810], Loss: 0.5102\n",
            "Epoch [3/10], Batch [815], Loss: 0.3333\n",
            "Epoch [3/10], Batch [820], Loss: 0.2864\n",
            "Epoch [3/10], Batch [825], Loss: 0.4532\n",
            "Epoch [3/10], Batch [830], Loss: 0.5141\n",
            "Epoch [3/10], Batch [835], Loss: 0.4324\n",
            "Epoch [3/10], Batch [840], Loss: 0.3420\n",
            "Epoch [3/10], Batch [845], Loss: 0.3106\n",
            "Epoch [3/10], Batch [850], Loss: 0.3794\n",
            "Epoch [3/10], Batch [855], Loss: 0.5512\n",
            "Epoch [3/10], Batch [860], Loss: 0.5620\n",
            "Epoch [3/10], Batch [865], Loss: 0.3861\n",
            "Epoch [3/10], Batch [870], Loss: 0.3435\n",
            "Epoch [3/10], Batch [875], Loss: 0.3732\n",
            "Epoch [3/10], Batch [880], Loss: 0.3549\n",
            "Epoch [3/10], Batch [885], Loss: 0.4569\n",
            "Epoch [3/10], Batch [890], Loss: 0.4896\n",
            "Epoch [3/10], Batch [895], Loss: 0.3609\n",
            "Epoch [3/10], Batch [900], Loss: 0.4145\n",
            "Epoch [3/10], Batch [905], Loss: 0.4043\n",
            "Epoch [3/10], Batch [910], Loss: 0.4191\n",
            "Epoch [3/10], Batch [915], Loss: 0.4568\n",
            "Epoch [3/10], Batch [920], Loss: 0.4288\n",
            "Epoch [3/10], Batch [925], Loss: 0.4344\n",
            "Epoch [3/10], Batch [930], Loss: 0.2423\n",
            "Epoch [3/10], Batch [935], Loss: 0.5872\n",
            "Epoch [3/10], Batch [940], Loss: 0.3676\n",
            "Epoch [3/10], Batch [945], Loss: 0.2709\n",
            "Epoch [3/10], Batch [950], Loss: 0.3684\n",
            "Epoch [3/10], Batch [955], Loss: 0.3345\n",
            "Epoch [3/10], Batch [960], Loss: 0.3321\n",
            "Epoch [3/10], Batch [965], Loss: 0.4420\n",
            "Epoch [3/10], Batch [970], Loss: 0.2260\n",
            "Epoch [3/10], Batch [975], Loss: 0.3260\n",
            "Epoch [3/10], Batch [980], Loss: 0.3275\n",
            "Epoch [3/10], Batch [985], Loss: 0.4672\n",
            "Epoch [3/10], Batch [990], Loss: 0.3714\n",
            "Epoch [3/10], Batch [995], Loss: 0.3508\n",
            "Epoch [3/10], Batch [1000], Loss: 0.3410\n",
            "Epoch [3/10], Batch [1005], Loss: 0.4086\n",
            "Epoch [3/10], Batch [1010], Loss: 0.4963\n",
            "Epoch [3/10], Batch [1015], Loss: 0.4041\n",
            "Epoch [3/10], Batch [1020], Loss: 0.3221\n",
            "Epoch [3/10], Batch [1025], Loss: 0.5629\n",
            "Epoch [3/10], Batch [1030], Loss: 0.4067\n",
            "Epoch [3/10], Batch [1035], Loss: 0.2947\n",
            "Epoch [3/10], Batch [1040], Loss: 0.4383\n",
            "Epoch [3/10], Batch [1045], Loss: 0.3320\n",
            "Epoch [3/10], Batch [1050], Loss: 0.3687\n",
            "Epoch [3/10], Batch [1055], Loss: 0.4470\n",
            "Epoch [3/10], Batch [1060], Loss: 0.3265\n",
            "Epoch [3/10], Batch [1065], Loss: 0.2534\n",
            "Epoch [3/10], Batch [1070], Loss: 0.3430\n",
            "Epoch [3/10], Batch [1075], Loss: 0.3681\n",
            "Epoch [3/10], Batch [1080], Loss: 0.4457\n",
            "Epoch [3/10], Batch [1085], Loss: 0.3999\n",
            "Epoch [3/10], Batch [1090], Loss: 0.5998\n",
            "Epoch [3/10], Batch [1095], Loss: 0.5407\n",
            "Epoch [3/10], Batch [1100], Loss: 0.4812\n",
            "Epoch [3/10], Batch [1105], Loss: 0.3832\n",
            "Epoch [3/10], Batch [1110], Loss: 0.3607\n",
            "Epoch [3/10], Batch [1115], Loss: 0.3235\n",
            "Epoch [3/10], Batch [1120], Loss: 0.3533\n",
            "Epoch [3/10], Batch [1125], Loss: 0.5097\n",
            "Epoch [3/10], Batch [1130], Loss: 0.2911\n",
            "Epoch [3/10], Batch [1135], Loss: 0.5019\n",
            "Epoch [3/10], Batch [1140], Loss: 0.3161\n",
            "Epoch [3/10], Batch [1145], Loss: 0.3975\n",
            "Epoch [3/10], Batch [1150], Loss: 0.5236\n",
            "Epoch [3/10], Batch [1155], Loss: 0.2801\n",
            "Epoch [3/10], Batch [1160], Loss: 0.3145\n",
            "Epoch [3/10], Batch [1165], Loss: 0.3320\n",
            "Epoch [3/10], Batch [1170], Loss: 0.2631\n",
            "Epoch [3/10], Batch [1175], Loss: 0.2255\n",
            "Epoch [3/10], Batch [1180], Loss: 0.3716\n",
            "Epoch [3/10], Batch [1185], Loss: 0.3836\n",
            "Epoch [3/10], Batch [1190], Loss: 0.5577\n",
            "Epoch [3/10], Batch [1195], Loss: 0.4596\n",
            "Epoch [3/10], Batch [1200], Loss: 0.3320\n",
            "Epoch [3/10], Batch [1205], Loss: 0.4400\n",
            "Epoch [3/10], Batch [1210], Loss: 0.3742\n",
            "Epoch [3/10], Batch [1215], Loss: 0.4417\n",
            "Epoch [3/10], Batch [1220], Loss: 0.3957\n",
            "Epoch [3/10], Batch [1225], Loss: 0.4205\n",
            "Epoch [3/10], Batch [1230], Loss: 0.4841\n",
            "Epoch [3/10], Batch [1235], Loss: 0.3183\n",
            "Epoch [3/10], Batch [1240], Loss: 0.4690\n",
            "Epoch [3/10], Batch [1245], Loss: 0.3333\n",
            "Epoch [3/10], Batch [1250], Loss: 0.4180\n",
            "Epoch [3/10], Batch [1255], Loss: 0.5206\n",
            "Epoch [3/10], Batch [1260], Loss: 0.3642\n",
            "Epoch [3/10], Batch [1265], Loss: 0.3540\n",
            "Epoch [3/10], Batch [1270], Loss: 0.2952\n",
            "Epoch [3/10], Batch [1275], Loss: 0.3342\n",
            "Epoch [3/10], Batch [1280], Loss: 0.3810\n",
            "Epoch [3/10], Batch [1285], Loss: 0.2320\n",
            "Epoch [3/10], Batch [1290], Loss: 0.3361\n",
            "Epoch [3/10], Batch [1295], Loss: 0.3894\n",
            "Epoch [3/10], Batch [1300], Loss: 0.3903\n",
            "Epoch [3/10], Batch [1305], Loss: 0.3765\n",
            "Epoch [3/10], Batch [1310], Loss: 0.2955\n",
            "Epoch [3/10], Batch [1315], Loss: 0.4863\n",
            "Epoch [3/10], Batch [1320], Loss: 0.3568\n",
            "Epoch [3/10], Batch [1325], Loss: 0.3705\n",
            "Epoch [3/10], Batch [1330], Loss: 0.4067\n",
            "Epoch [3/10], Batch [1335], Loss: 0.3399\n",
            "Epoch [3/10], Batch [1340], Loss: 0.4109\n",
            "Epoch [3/10], Batch [1345], Loss: 0.3684\n",
            "Epoch [3/10], Batch [1350], Loss: 0.4778\n",
            "Epoch [3/10], Batch [1355], Loss: 0.6501\n",
            "Epoch [3/10], Batch [1360], Loss: 0.3739\n",
            "Epoch [3/10], Batch [1365], Loss: 0.4257\n",
            "Epoch [3/10], Batch [1370], Loss: 0.4145\n",
            "Epoch [3/10], Batch [1375], Loss: 0.3941\n",
            "Epoch [3/10], Batch [1380], Loss: 0.5070\n",
            "Epoch [3/10], Batch [1385], Loss: 0.5971\n",
            "Epoch [3/10], Batch [1390], Loss: 0.6408\n",
            "Epoch [3/10], Batch [1395], Loss: 0.3926\n",
            "Epoch [3/10], Batch [1400], Loss: 0.4601\n",
            "Epoch [3/10], Batch [1405], Loss: 0.3333\n",
            "Epoch [3/10], Batch [1410], Loss: 0.3142\n",
            "Epoch [3/10], Batch [1415], Loss: 0.3789\n",
            "Epoch [3/10], Batch [1420], Loss: 0.2711\n",
            "Epoch [3/10], Batch [1425], Loss: 0.3295\n",
            "Epoch [3/10], Batch [1430], Loss: 0.6634\n",
            "Epoch [3/10], Batch [1435], Loss: 0.5878\n",
            "Epoch [3/10], Batch [1440], Loss: 0.4479\n",
            "Epoch [3/10], Batch [1445], Loss: 0.5373\n",
            "Epoch [3/10], Batch [1450], Loss: 0.4065\n",
            "Epoch [3/10], Batch [1455], Loss: 0.3273\n",
            "Epoch [3/10], Batch [1460], Loss: 0.2659\n",
            "Epoch [3/10], Batch [1465], Loss: 0.4554\n",
            "Epoch [3/10], Batch [1470], Loss: 0.3730\n",
            "Epoch [3/10], Batch [1475], Loss: 0.2307\n",
            "Epoch [3/10], Batch [1480], Loss: 0.3870\n",
            "Epoch [3/10], Batch [1485], Loss: 0.5355\n",
            "Epoch [3/10], Batch [1490], Loss: 0.5005\n",
            "Epoch [3/10], Batch [1495], Loss: 0.4674\n",
            "Epoch [3/10], Batch [1500], Loss: 0.2207\n",
            "Epoch [3/10], Batch [1505], Loss: 0.4239\n",
            "Epoch [3/10], Batch [1510], Loss: 0.3344\n",
            "Epoch [3/10], Batch [1515], Loss: 0.3166\n",
            "Epoch [3/10], Batch [1520], Loss: 0.3122\n",
            "Epoch [3/10], Batch [1525], Loss: 0.3835\n",
            "Epoch [3/10], Batch [1530], Loss: 0.3974\n",
            "Epoch [3/10], Batch [1535], Loss: 0.4925\n",
            "Epoch [3/10], Batch [1540], Loss: 0.4356\n",
            "Epoch [3/10], Batch [1545], Loss: 0.4137\n",
            "Epoch [3/10], Batch [1550], Loss: 0.4160\n",
            "Epoch [3/10], Batch [1555], Loss: 0.2850\n",
            "Epoch [3/10], Batch [1560], Loss: 0.3749\n",
            "Epoch [3/10], Batch [1565], Loss: 0.4083\n",
            "Epoch [3/10], Batch [1570], Loss: 0.5391\n",
            "Epoch [3/10], Batch [1575], Loss: 0.5085\n",
            "Epoch [3/10], Batch [1580], Loss: 0.3837\n",
            "Epoch [3/10], Batch [1585], Loss: 0.3356\n",
            "Epoch [3/10], Batch [1590], Loss: 0.4955\n",
            "Epoch [3/10], Batch [1595], Loss: 0.3449\n",
            "Epoch [3/10], Batch [1600], Loss: 0.3848\n",
            "Epoch [3/10], Batch [1605], Loss: 0.5186\n",
            "Epoch [3/10], Batch [1610], Loss: 0.3360\n",
            "Epoch [3/10], Batch [1615], Loss: 0.4659\n",
            "Epoch [3/10], Batch [1620], Loss: 0.4136\n",
            "Epoch [3/10], Batch [1625], Loss: 0.3847\n",
            "Epoch [3/10], Batch [1630], Loss: 0.2904\n",
            "Epoch [3/10], Batch [1635], Loss: 0.3160\n",
            "Epoch [3/10], Batch [1640], Loss: 0.4479\n",
            "Epoch [3/10], Batch [1645], Loss: 0.4061\n",
            "Epoch [3/10], Batch [1650], Loss: 0.3796\n",
            "Epoch [3/10], Batch [1655], Loss: 0.4385\n",
            "Epoch [3/10], Batch [1660], Loss: 0.3094\n",
            "Epoch [3/10], Batch [1665], Loss: 0.2919\n",
            "Epoch [3/10], Batch [1670], Loss: 0.3125\n",
            "Epoch [3/10], Batch [1675], Loss: 0.5616\n",
            "Epoch [3/10], Batch [1680], Loss: 0.4399\n",
            "Epoch [3/10], Batch [1685], Loss: 0.4923\n",
            "Epoch [3/10], Batch [1690], Loss: 0.4096\n",
            "Epoch [3/10], Batch [1695], Loss: 0.2986\n",
            "Epoch [3/10], Batch [1700], Loss: 0.3372\n",
            "Epoch [3/10], Batch [1705], Loss: 0.3236\n",
            "Epoch [3/10], Batch [1710], Loss: 0.5550\n",
            "Epoch [3/10], Batch [1715], Loss: 0.4188\n",
            "Epoch [3/10], Batch [1720], Loss: 0.4099\n",
            "Epoch [3/10], Batch [1725], Loss: 0.4843\n",
            "Epoch [3/10], Batch [1730], Loss: 0.3430\n",
            "Epoch [3/10], Batch [1735], Loss: 0.3191\n",
            "Epoch [3/10], Batch [1740], Loss: 0.3536\n",
            "Epoch [3/10], Batch [1745], Loss: 0.3402\n",
            "Epoch [3/10], Batch [1750], Loss: 0.5390\n",
            "Epoch [3/10], Batch [1755], Loss: 0.3218\n",
            "Epoch [3/10], Batch [1760], Loss: 0.2297\n",
            "Epoch [3/10], Batch [1765], Loss: 0.3966\n",
            "Epoch [3/10], Batch [1770], Loss: 0.3340\n",
            "Epoch [3/10], Batch [1775], Loss: 0.3795\n",
            "Epoch [3/10], Batch [1780], Loss: 0.4165\n",
            "Epoch [3/10], Batch [1785], Loss: 0.3974\n",
            "Epoch [3/10], Batch [1790], Loss: 0.5059\n",
            "Epoch [3/10], Batch [1795], Loss: 0.3618\n",
            "Epoch [3/10], Batch [1800], Loss: 0.3436\n",
            "Epoch [3/10], Batch [1805], Loss: 0.4424\n",
            "Epoch [3/10], Batch [1810], Loss: 0.3187\n",
            "Epoch [3/10], Batch [1815], Loss: 0.3595\n",
            "Epoch [3/10], Batch [1820], Loss: 0.4041\n",
            "Epoch [3/10], Batch [1825], Loss: 0.4245\n",
            "Epoch [3/10], Batch [1830], Loss: 0.3713\n",
            "Epoch [3/10], Batch [1835], Loss: 0.3323\n",
            "Epoch [3/10], Batch [1840], Loss: 0.3228\n",
            "Epoch [3/10], Batch [1845], Loss: 0.3337\n",
            "Epoch [3/10], Batch [1850], Loss: 0.2381\n",
            "Epoch [3/10], Batch [1855], Loss: 0.2550\n",
            "Epoch [3/10], Batch [1860], Loss: 0.3293\n",
            "Epoch [3/10], Batch [1865], Loss: 0.4047\n",
            "Epoch [3/10], Batch [1870], Loss: 0.3181\n",
            "Epoch [3/10], Batch [1875], Loss: 0.3955\n",
            "Epoch [3/10], Batch [1880], Loss: 0.5470\n",
            "Epoch [3/10], Batch [1885], Loss: 0.4452\n",
            "Epoch [3/10], Batch [1890], Loss: 0.4469\n",
            "Epoch [3/10], Batch [1895], Loss: 0.4329\n",
            "Epoch [3/10], Batch [1900], Loss: 0.5343\n",
            "Epoch [3/10], Batch [1905], Loss: 0.2613\n",
            "Epoch [3/10], Batch [1910], Loss: 0.3841\n",
            "Epoch [3/10], Batch [1915], Loss: 0.3088\n",
            "Epoch [3/10], Batch [1920], Loss: 0.4289\n",
            "Epoch [3/10], Batch [1925], Loss: 0.3853\n",
            "Epoch [3/10], Batch [1930], Loss: 0.3455\n",
            "Epoch [3/10], Batch [1935], Loss: 0.2980\n",
            "Epoch [3/10], Batch [1940], Loss: 0.3702\n",
            "Epoch [3/10], Batch [1945], Loss: 0.3427\n",
            "Epoch [3/10], Batch [1950], Loss: 0.4728\n",
            "Epoch [3/10], Batch [1955], Loss: 0.3533\n",
            "Epoch [3/10], Batch [1960], Loss: 0.4958\n",
            "Epoch [3/10], Batch [1965], Loss: 0.5628\n",
            "Epoch [3/10], Batch [1970], Loss: 0.6445\n",
            "Epoch [3/10], Batch [1975], Loss: 0.7170\n",
            "Epoch [3/10], Batch [1980], Loss: 0.3675\n",
            "Epoch [3/10], Batch [1985], Loss: 0.4437\n",
            "Epoch [3/10], Batch [1990], Loss: 0.3623\n",
            "Epoch [3/10], Batch [1995], Loss: 0.3119\n",
            "Epoch [3/10], Batch [2000], Loss: 0.2859\n",
            "Epoch [3/10], Batch [2005], Loss: 0.4197\n",
            "Epoch [3/10], Batch [2010], Loss: 0.4831\n",
            "Epoch [3/10], Batch [2015], Loss: 0.5246\n",
            "Epoch [3/10], Batch [2020], Loss: 0.3655\n",
            "Epoch [3/10], Batch [2025], Loss: 0.3266\n",
            "Epoch [3/10], Batch [2030], Loss: 0.3660\n",
            "Epoch [3/10], Batch [2035], Loss: 0.3091\n",
            "Epoch [3/10], Batch [2040], Loss: 0.3500\n",
            "Epoch [3/10], Batch [2045], Loss: 0.3236\n",
            "Epoch [3/10], Batch [2050], Loss: 0.4741\n",
            "Epoch [3/10], Batch [2055], Loss: 0.3764\n",
            "Epoch [3/10], Batch [2060], Loss: 0.4484\n",
            "Epoch [3/10], Average Loss: 0.4085\n",
            "Epoch [4/10], Batch [0], Loss: 0.4559\n",
            "Epoch [4/10], Batch [5], Loss: 0.3714\n",
            "Epoch [4/10], Batch [10], Loss: 0.3537\n",
            "Epoch [4/10], Batch [15], Loss: 0.4458\n",
            "Epoch [4/10], Batch [20], Loss: 0.3759\n",
            "Epoch [4/10], Batch [25], Loss: 0.5313\n",
            "Epoch [4/10], Batch [30], Loss: 0.4641\n",
            "Epoch [4/10], Batch [35], Loss: 0.5402\n",
            "Epoch [4/10], Batch [40], Loss: 0.2873\n",
            "Epoch [4/10], Batch [45], Loss: 0.3588\n",
            "Epoch [4/10], Batch [50], Loss: 0.4844\n",
            "Epoch [4/10], Batch [55], Loss: 0.4026\n",
            "Epoch [4/10], Batch [60], Loss: 0.4271\n",
            "Epoch [4/10], Batch [65], Loss: 0.3548\n",
            "Epoch [4/10], Batch [70], Loss: 0.4578\n",
            "Epoch [4/10], Batch [75], Loss: 0.4836\n",
            "Epoch [4/10], Batch [80], Loss: 0.3775\n",
            "Epoch [4/10], Batch [85], Loss: 0.4887\n",
            "Epoch [4/10], Batch [90], Loss: 0.5084\n",
            "Epoch [4/10], Batch [95], Loss: 0.5995\n",
            "Epoch [4/10], Batch [100], Loss: 0.2503\n",
            "Epoch [4/10], Batch [105], Loss: 0.3352\n",
            "Epoch [4/10], Batch [110], Loss: 0.3876\n",
            "Epoch [4/10], Batch [115], Loss: 0.4779\n",
            "Epoch [4/10], Batch [120], Loss: 0.3727\n",
            "Epoch [4/10], Batch [125], Loss: 0.4677\n",
            "Epoch [4/10], Batch [130], Loss: 0.3577\n",
            "Epoch [4/10], Batch [135], Loss: 0.4828\n",
            "Epoch [4/10], Batch [140], Loss: 0.3671\n",
            "Epoch [4/10], Batch [145], Loss: 0.3947\n",
            "Epoch [4/10], Batch [150], Loss: 0.4129\n",
            "Epoch [4/10], Batch [155], Loss: 0.4460\n",
            "Epoch [4/10], Batch [160], Loss: 0.4185\n",
            "Epoch [4/10], Batch [165], Loss: 0.2929\n",
            "Epoch [4/10], Batch [170], Loss: 0.1990\n",
            "Epoch [4/10], Batch [175], Loss: 0.4224\n",
            "Epoch [4/10], Batch [180], Loss: 0.4406\n",
            "Epoch [4/10], Batch [185], Loss: 0.4298\n",
            "Epoch [4/10], Batch [190], Loss: 0.2848\n",
            "Epoch [4/10], Batch [195], Loss: 0.5347\n",
            "Epoch [4/10], Batch [200], Loss: 0.5403\n",
            "Epoch [4/10], Batch [205], Loss: 0.4545\n",
            "Epoch [4/10], Batch [210], Loss: 0.2607\n",
            "Epoch [4/10], Batch [215], Loss: 0.4865\n",
            "Epoch [4/10], Batch [220], Loss: 0.4819\n",
            "Epoch [4/10], Batch [225], Loss: 0.6271\n",
            "Epoch [4/10], Batch [230], Loss: 0.4394\n",
            "Epoch [4/10], Batch [235], Loss: 0.3345\n",
            "Epoch [4/10], Batch [240], Loss: 0.3767\n",
            "Epoch [4/10], Batch [245], Loss: 0.2564\n",
            "Epoch [4/10], Batch [250], Loss: 0.3528\n",
            "Epoch [4/10], Batch [255], Loss: 0.3407\n",
            "Epoch [4/10], Batch [260], Loss: 0.3359\n",
            "Epoch [4/10], Batch [265], Loss: 0.3006\n",
            "Epoch [4/10], Batch [270], Loss: 0.3629\n",
            "Epoch [4/10], Batch [275], Loss: 0.4240\n",
            "Epoch [4/10], Batch [280], Loss: 0.3348\n",
            "Epoch [4/10], Batch [285], Loss: 0.5195\n",
            "Epoch [4/10], Batch [290], Loss: 0.3498\n",
            "Epoch [4/10], Batch [295], Loss: 0.3144\n",
            "Epoch [4/10], Batch [300], Loss: 0.2728\n",
            "Epoch [4/10], Batch [305], Loss: 0.5529\n",
            "Epoch [4/10], Batch [310], Loss: 0.2547\n",
            "Epoch [4/10], Batch [315], Loss: 0.3114\n",
            "Epoch [4/10], Batch [320], Loss: 0.3047\n",
            "Epoch [4/10], Batch [325], Loss: 0.1902\n",
            "Epoch [4/10], Batch [330], Loss: 0.3519\n",
            "Epoch [4/10], Batch [335], Loss: 0.3460\n",
            "Epoch [4/10], Batch [340], Loss: 0.4559\n",
            "Epoch [4/10], Batch [345], Loss: 0.3572\n",
            "Epoch [4/10], Batch [350], Loss: 0.4123\n",
            "Epoch [4/10], Batch [355], Loss: 0.3772\n",
            "Epoch [4/10], Batch [360], Loss: 0.3241\n",
            "Epoch [4/10], Batch [365], Loss: 0.2489\n",
            "Epoch [4/10], Batch [370], Loss: 0.3931\n",
            "Epoch [4/10], Batch [375], Loss: 0.2957\n",
            "Epoch [4/10], Batch [380], Loss: 0.3098\n",
            "Epoch [4/10], Batch [385], Loss: 0.4020\n",
            "Epoch [4/10], Batch [390], Loss: 0.4738\n",
            "Epoch [4/10], Batch [395], Loss: 0.3266\n",
            "Epoch [4/10], Batch [400], Loss: 0.2500\n",
            "Epoch [4/10], Batch [405], Loss: 0.2201\n",
            "Epoch [4/10], Batch [410], Loss: 0.2938\n",
            "Epoch [4/10], Batch [415], Loss: 0.4384\n",
            "Epoch [4/10], Batch [420], Loss: 0.3574\n",
            "Epoch [4/10], Batch [425], Loss: 0.5080\n",
            "Epoch [4/10], Batch [430], Loss: 0.2737\n",
            "Epoch [4/10], Batch [435], Loss: 0.4309\n",
            "Epoch [4/10], Batch [440], Loss: 0.6093\n",
            "Epoch [4/10], Batch [445], Loss: 0.6810\n",
            "Epoch [4/10], Batch [450], Loss: 0.4443\n",
            "Epoch [4/10], Batch [455], Loss: 0.3551\n",
            "Epoch [4/10], Batch [460], Loss: 0.3192\n",
            "Epoch [4/10], Batch [465], Loss: 0.2920\n",
            "Epoch [4/10], Batch [470], Loss: 0.3709\n",
            "Epoch [4/10], Batch [475], Loss: 0.5195\n",
            "Epoch [4/10], Batch [480], Loss: 0.3505\n",
            "Epoch [4/10], Batch [485], Loss: 0.5470\n",
            "Epoch [4/10], Batch [490], Loss: 0.3436\n",
            "Epoch [4/10], Batch [495], Loss: 0.2691\n",
            "Epoch [4/10], Batch [500], Loss: 0.5527\n",
            "Epoch [4/10], Batch [505], Loss: 0.2965\n",
            "Epoch [4/10], Batch [510], Loss: 0.2526\n",
            "Epoch [4/10], Batch [515], Loss: 0.4161\n",
            "Epoch [4/10], Batch [520], Loss: 0.3774\n",
            "Epoch [4/10], Batch [525], Loss: 0.3841\n",
            "Epoch [4/10], Batch [530], Loss: 0.2937\n",
            "Epoch [4/10], Batch [535], Loss: 0.3143\n",
            "Epoch [4/10], Batch [540], Loss: 0.3536\n",
            "Epoch [4/10], Batch [545], Loss: 0.4369\n",
            "Epoch [4/10], Batch [550], Loss: 0.2919\n",
            "Epoch [4/10], Batch [555], Loss: 0.3965\n",
            "Epoch [4/10], Batch [560], Loss: 0.3043\n",
            "Epoch [4/10], Batch [565], Loss: 0.3494\n",
            "Epoch [4/10], Batch [570], Loss: 0.3556\n",
            "Epoch [4/10], Batch [575], Loss: 0.4611\n",
            "Epoch [4/10], Batch [580], Loss: 0.5016\n",
            "Epoch [4/10], Batch [585], Loss: 0.4198\n",
            "Epoch [4/10], Batch [590], Loss: 0.4329\n",
            "Epoch [4/10], Batch [595], Loss: 0.3111\n",
            "Epoch [4/10], Batch [600], Loss: 0.4606\n",
            "Epoch [4/10], Batch [605], Loss: 0.4783\n",
            "Epoch [4/10], Batch [610], Loss: 0.4752\n",
            "Epoch [4/10], Batch [615], Loss: 0.4335\n",
            "Epoch [4/10], Batch [620], Loss: 0.3234\n",
            "Epoch [4/10], Batch [625], Loss: 0.4847\n",
            "Epoch [4/10], Batch [630], Loss: 0.4677\n",
            "Epoch [4/10], Batch [635], Loss: 0.5286\n",
            "Epoch [4/10], Batch [640], Loss: 0.3262\n",
            "Epoch [4/10], Batch [645], Loss: 0.5050\n",
            "Epoch [4/10], Batch [650], Loss: 0.7031\n",
            "Epoch [4/10], Batch [655], Loss: 0.4600\n",
            "Epoch [4/10], Batch [660], Loss: 0.5154\n",
            "Epoch [4/10], Batch [665], Loss: 0.5216\n",
            "Epoch [4/10], Batch [670], Loss: 0.3303\n",
            "Epoch [4/10], Batch [675], Loss: 0.4932\n",
            "Epoch [4/10], Batch [680], Loss: 0.4668\n",
            "Epoch [4/10], Batch [685], Loss: 0.2592\n",
            "Epoch [4/10], Batch [690], Loss: 0.3614\n",
            "Epoch [4/10], Batch [695], Loss: 0.4302\n",
            "Epoch [4/10], Batch [700], Loss: 0.3695\n",
            "Epoch [4/10], Batch [705], Loss: 0.3112\n",
            "Epoch [4/10], Batch [710], Loss: 0.3857\n",
            "Epoch [4/10], Batch [715], Loss: 0.3821\n",
            "Epoch [4/10], Batch [720], Loss: 0.4630\n",
            "Epoch [4/10], Batch [725], Loss: 0.5690\n",
            "Epoch [4/10], Batch [730], Loss: 0.3581\n",
            "Epoch [4/10], Batch [735], Loss: 0.4308\n",
            "Epoch [4/10], Batch [740], Loss: 0.4043\n",
            "Epoch [4/10], Batch [745], Loss: 0.4252\n",
            "Epoch [4/10], Batch [750], Loss: 0.5408\n",
            "Epoch [4/10], Batch [755], Loss: 0.2841\n",
            "Epoch [4/10], Batch [760], Loss: 0.2960\n",
            "Epoch [4/10], Batch [765], Loss: 0.3496\n",
            "Epoch [4/10], Batch [770], Loss: 0.3744\n",
            "Epoch [4/10], Batch [775], Loss: 0.2753\n",
            "Epoch [4/10], Batch [780], Loss: 0.3270\n",
            "Epoch [4/10], Batch [785], Loss: 0.3326\n",
            "Epoch [4/10], Batch [790], Loss: 0.3060\n",
            "Epoch [4/10], Batch [795], Loss: 0.4008\n",
            "Epoch [4/10], Batch [800], Loss: 0.2393\n",
            "Epoch [4/10], Batch [805], Loss: 0.5979\n",
            "Epoch [4/10], Batch [810], Loss: 0.4945\n",
            "Epoch [4/10], Batch [815], Loss: 0.3051\n",
            "Epoch [4/10], Batch [820], Loss: 0.2797\n",
            "Epoch [4/10], Batch [825], Loss: 0.4456\n",
            "Epoch [4/10], Batch [830], Loss: 0.5134\n",
            "Epoch [4/10], Batch [835], Loss: 0.3985\n",
            "Epoch [4/10], Batch [840], Loss: 0.3268\n",
            "Epoch [4/10], Batch [845], Loss: 0.2931\n",
            "Epoch [4/10], Batch [850], Loss: 0.3576\n",
            "Epoch [4/10], Batch [855], Loss: 0.5474\n",
            "Epoch [4/10], Batch [860], Loss: 0.5491\n",
            "Epoch [4/10], Batch [865], Loss: 0.3748\n",
            "Epoch [4/10], Batch [870], Loss: 0.3578\n",
            "Epoch [4/10], Batch [875], Loss: 0.3544\n",
            "Epoch [4/10], Batch [880], Loss: 0.3300\n",
            "Epoch [4/10], Batch [885], Loss: 0.4230\n",
            "Epoch [4/10], Batch [890], Loss: 0.4770\n",
            "Epoch [4/10], Batch [895], Loss: 0.3510\n",
            "Epoch [4/10], Batch [900], Loss: 0.4137\n",
            "Epoch [4/10], Batch [905], Loss: 0.3946\n",
            "Epoch [4/10], Batch [910], Loss: 0.3983\n",
            "Epoch [4/10], Batch [915], Loss: 0.4505\n",
            "Epoch [4/10], Batch [920], Loss: 0.4525\n",
            "Epoch [4/10], Batch [925], Loss: 0.4022\n",
            "Epoch [4/10], Batch [930], Loss: 0.2421\n",
            "Epoch [4/10], Batch [935], Loss: 0.5895\n",
            "Epoch [4/10], Batch [940], Loss: 0.3638\n",
            "Epoch [4/10], Batch [945], Loss: 0.2459\n",
            "Epoch [4/10], Batch [950], Loss: 0.3649\n",
            "Epoch [4/10], Batch [955], Loss: 0.2995\n",
            "Epoch [4/10], Batch [960], Loss: 0.3101\n",
            "Epoch [4/10], Batch [965], Loss: 0.4509\n",
            "Epoch [4/10], Batch [970], Loss: 0.2224\n",
            "Epoch [4/10], Batch [975], Loss: 0.3207\n",
            "Epoch [4/10], Batch [980], Loss: 0.3321\n",
            "Epoch [4/10], Batch [985], Loss: 0.4665\n",
            "Epoch [4/10], Batch [990], Loss: 0.3683\n",
            "Epoch [4/10], Batch [995], Loss: 0.3345\n",
            "Epoch [4/10], Batch [1000], Loss: 0.3341\n",
            "Epoch [4/10], Batch [1005], Loss: 0.4039\n",
            "Epoch [4/10], Batch [1010], Loss: 0.4714\n",
            "Epoch [4/10], Batch [1015], Loss: 0.3775\n",
            "Epoch [4/10], Batch [1020], Loss: 0.3121\n",
            "Epoch [4/10], Batch [1025], Loss: 0.5547\n",
            "Epoch [4/10], Batch [1030], Loss: 0.3639\n",
            "Epoch [4/10], Batch [1035], Loss: 0.2918\n",
            "Epoch [4/10], Batch [1040], Loss: 0.4228\n",
            "Epoch [4/10], Batch [1045], Loss: 0.3248\n",
            "Epoch [4/10], Batch [1050], Loss: 0.3437\n",
            "Epoch [4/10], Batch [1055], Loss: 0.4552\n",
            "Epoch [4/10], Batch [1060], Loss: 0.3368\n",
            "Epoch [4/10], Batch [1065], Loss: 0.2374\n",
            "Epoch [4/10], Batch [1070], Loss: 0.3278\n",
            "Epoch [4/10], Batch [1075], Loss: 0.3401\n",
            "Epoch [4/10], Batch [1080], Loss: 0.4254\n",
            "Epoch [4/10], Batch [1085], Loss: 0.3556\n",
            "Epoch [4/10], Batch [1090], Loss: 0.5575\n",
            "Epoch [4/10], Batch [1095], Loss: 0.5334\n",
            "Epoch [4/10], Batch [1100], Loss: 0.4906\n",
            "Epoch [4/10], Batch [1105], Loss: 0.3903\n",
            "Epoch [4/10], Batch [1110], Loss: 0.3581\n",
            "Epoch [4/10], Batch [1115], Loss: 0.2927\n",
            "Epoch [4/10], Batch [1120], Loss: 0.3332\n",
            "Epoch [4/10], Batch [1125], Loss: 0.4836\n",
            "Epoch [4/10], Batch [1130], Loss: 0.2905\n",
            "Epoch [4/10], Batch [1135], Loss: 0.5010\n",
            "Epoch [4/10], Batch [1140], Loss: 0.3009\n",
            "Epoch [4/10], Batch [1145], Loss: 0.3939\n",
            "Epoch [4/10], Batch [1150], Loss: 0.5217\n",
            "Epoch [4/10], Batch [1155], Loss: 0.2566\n",
            "Epoch [4/10], Batch [1160], Loss: 0.3180\n",
            "Epoch [4/10], Batch [1165], Loss: 0.3410\n",
            "Epoch [4/10], Batch [1170], Loss: 0.2482\n",
            "Epoch [4/10], Batch [1175], Loss: 0.2199\n",
            "Epoch [4/10], Batch [1180], Loss: 0.3725\n",
            "Epoch [4/10], Batch [1185], Loss: 0.3747\n",
            "Epoch [4/10], Batch [1190], Loss: 0.5655\n",
            "Epoch [4/10], Batch [1195], Loss: 0.4577\n",
            "Epoch [4/10], Batch [1200], Loss: 0.3314\n",
            "Epoch [4/10], Batch [1205], Loss: 0.4407\n",
            "Epoch [4/10], Batch [1210], Loss: 0.3652\n",
            "Epoch [4/10], Batch [1215], Loss: 0.4097\n",
            "Epoch [4/10], Batch [1220], Loss: 0.3939\n",
            "Epoch [4/10], Batch [1225], Loss: 0.4083\n",
            "Epoch [4/10], Batch [1230], Loss: 0.4576\n",
            "Epoch [4/10], Batch [1235], Loss: 0.3199\n",
            "Epoch [4/10], Batch [1240], Loss: 0.4407\n",
            "Epoch [4/10], Batch [1245], Loss: 0.3430\n",
            "Epoch [4/10], Batch [1250], Loss: 0.3964\n",
            "Epoch [4/10], Batch [1255], Loss: 0.5196\n",
            "Epoch [4/10], Batch [1260], Loss: 0.3658\n",
            "Epoch [4/10], Batch [1265], Loss: 0.3512\n",
            "Epoch [4/10], Batch [1270], Loss: 0.2913\n",
            "Epoch [4/10], Batch [1275], Loss: 0.3209\n",
            "Epoch [4/10], Batch [1280], Loss: 0.3677\n",
            "Epoch [4/10], Batch [1285], Loss: 0.2324\n",
            "Epoch [4/10], Batch [1290], Loss: 0.3263\n",
            "Epoch [4/10], Batch [1295], Loss: 0.3778\n",
            "Epoch [4/10], Batch [1300], Loss: 0.3622\n",
            "Epoch [4/10], Batch [1305], Loss: 0.3482\n",
            "Epoch [4/10], Batch [1310], Loss: 0.2876\n",
            "Epoch [4/10], Batch [1315], Loss: 0.4961\n",
            "Epoch [4/10], Batch [1320], Loss: 0.3404\n",
            "Epoch [4/10], Batch [1325], Loss: 0.3676\n",
            "Epoch [4/10], Batch [1330], Loss: 0.4000\n",
            "Epoch [4/10], Batch [1335], Loss: 0.3473\n",
            "Epoch [4/10], Batch [1340], Loss: 0.3882\n",
            "Epoch [4/10], Batch [1345], Loss: 0.3686\n",
            "Epoch [4/10], Batch [1350], Loss: 0.4711\n",
            "Epoch [4/10], Batch [1355], Loss: 0.6388\n",
            "Epoch [4/10], Batch [1360], Loss: 0.3626\n",
            "Epoch [4/10], Batch [1365], Loss: 0.4237\n",
            "Epoch [4/10], Batch [1370], Loss: 0.4204\n",
            "Epoch [4/10], Batch [1375], Loss: 0.3999\n",
            "Epoch [4/10], Batch [1380], Loss: 0.4974\n",
            "Epoch [4/10], Batch [1385], Loss: 0.5509\n",
            "Epoch [4/10], Batch [1390], Loss: 0.6439\n",
            "Epoch [4/10], Batch [1395], Loss: 0.3794\n",
            "Epoch [4/10], Batch [1400], Loss: 0.4567\n",
            "Epoch [4/10], Batch [1405], Loss: 0.3156\n",
            "Epoch [4/10], Batch [1410], Loss: 0.2994\n",
            "Epoch [4/10], Batch [1415], Loss: 0.3606\n",
            "Epoch [4/10], Batch [1420], Loss: 0.2624\n",
            "Epoch [4/10], Batch [1425], Loss: 0.3228\n",
            "Epoch [4/10], Batch [1430], Loss: 0.6246\n",
            "Epoch [4/10], Batch [1435], Loss: 0.5631\n",
            "Epoch [4/10], Batch [1440], Loss: 0.4252\n",
            "Epoch [4/10], Batch [1445], Loss: 0.4987\n",
            "Epoch [4/10], Batch [1450], Loss: 0.3908\n",
            "Epoch [4/10], Batch [1455], Loss: 0.3027\n",
            "Epoch [4/10], Batch [1460], Loss: 0.2401\n",
            "Epoch [4/10], Batch [1465], Loss: 0.4224\n",
            "Epoch [4/10], Batch [1470], Loss: 0.3907\n",
            "Epoch [4/10], Batch [1475], Loss: 0.2475\n",
            "Epoch [4/10], Batch [1480], Loss: 0.3840\n",
            "Epoch [4/10], Batch [1485], Loss: 0.5004\n",
            "Epoch [4/10], Batch [1490], Loss: 0.4651\n",
            "Epoch [4/10], Batch [1495], Loss: 0.4430\n",
            "Epoch [4/10], Batch [1500], Loss: 0.2098\n",
            "Epoch [4/10], Batch [1505], Loss: 0.4288\n",
            "Epoch [4/10], Batch [1510], Loss: 0.3210\n",
            "Epoch [4/10], Batch [1515], Loss: 0.3098\n",
            "Epoch [4/10], Batch [1520], Loss: 0.2998\n",
            "Epoch [4/10], Batch [1525], Loss: 0.3673\n",
            "Epoch [4/10], Batch [1530], Loss: 0.3840\n",
            "Epoch [4/10], Batch [1535], Loss: 0.4644\n",
            "Epoch [4/10], Batch [1540], Loss: 0.4365\n",
            "Epoch [4/10], Batch [1545], Loss: 0.4045\n",
            "Epoch [4/10], Batch [1550], Loss: 0.3805\n",
            "Epoch [4/10], Batch [1555], Loss: 0.2745\n",
            "Epoch [4/10], Batch [1560], Loss: 0.3650\n",
            "Epoch [4/10], Batch [1565], Loss: 0.4198\n",
            "Epoch [4/10], Batch [1570], Loss: 0.5185\n",
            "Epoch [4/10], Batch [1575], Loss: 0.4703\n",
            "Epoch [4/10], Batch [1580], Loss: 0.3677\n",
            "Epoch [4/10], Batch [1585], Loss: 0.3207\n",
            "Epoch [4/10], Batch [1590], Loss: 0.4846\n",
            "Epoch [4/10], Batch [1595], Loss: 0.3474\n",
            "Epoch [4/10], Batch [1600], Loss: 0.3870\n",
            "Epoch [4/10], Batch [1605], Loss: 0.5151\n",
            "Epoch [4/10], Batch [1610], Loss: 0.3161\n",
            "Epoch [4/10], Batch [1615], Loss: 0.4375\n",
            "Epoch [4/10], Batch [1620], Loss: 0.4259\n",
            "Epoch [4/10], Batch [1625], Loss: 0.3725\n",
            "Epoch [4/10], Batch [1630], Loss: 0.2868\n",
            "Epoch [4/10], Batch [1635], Loss: 0.2968\n",
            "Epoch [4/10], Batch [1640], Loss: 0.4378\n",
            "Epoch [4/10], Batch [1645], Loss: 0.3935\n",
            "Epoch [4/10], Batch [1650], Loss: 0.3483\n",
            "Epoch [4/10], Batch [1655], Loss: 0.3962\n",
            "Epoch [4/10], Batch [1660], Loss: 0.3022\n",
            "Epoch [4/10], Batch [1665], Loss: 0.2865\n",
            "Epoch [4/10], Batch [1670], Loss: 0.3116\n",
            "Epoch [4/10], Batch [1675], Loss: 0.5477\n",
            "Epoch [4/10], Batch [1680], Loss: 0.4439\n",
            "Epoch [4/10], Batch [1685], Loss: 0.4847\n",
            "Epoch [4/10], Batch [1690], Loss: 0.3921\n",
            "Epoch [4/10], Batch [1695], Loss: 0.2939\n",
            "Epoch [4/10], Batch [1700], Loss: 0.3416\n",
            "Epoch [4/10], Batch [1705], Loss: 0.2951\n",
            "Epoch [4/10], Batch [1710], Loss: 0.5419\n",
            "Epoch [4/10], Batch [1715], Loss: 0.4269\n",
            "Epoch [4/10], Batch [1720], Loss: 0.3998\n",
            "Epoch [4/10], Batch [1725], Loss: 0.4692\n",
            "Epoch [4/10], Batch [1730], Loss: 0.3338\n",
            "Epoch [4/10], Batch [1735], Loss: 0.3082\n",
            "Epoch [4/10], Batch [1740], Loss: 0.3266\n",
            "Epoch [4/10], Batch [1745], Loss: 0.3286\n",
            "Epoch [4/10], Batch [1750], Loss: 0.5196\n",
            "Epoch [4/10], Batch [1755], Loss: 0.3148\n",
            "Epoch [4/10], Batch [1760], Loss: 0.2251\n",
            "Epoch [4/10], Batch [1765], Loss: 0.3865\n",
            "Epoch [4/10], Batch [1770], Loss: 0.3363\n",
            "Epoch [4/10], Batch [1775], Loss: 0.3777\n",
            "Epoch [4/10], Batch [1780], Loss: 0.4111\n",
            "Epoch [4/10], Batch [1785], Loss: 0.3969\n",
            "Epoch [4/10], Batch [1790], Loss: 0.4775\n",
            "Epoch [4/10], Batch [1795], Loss: 0.3647\n",
            "Epoch [4/10], Batch [1800], Loss: 0.3325\n",
            "Epoch [4/10], Batch [1805], Loss: 0.4312\n",
            "Epoch [4/10], Batch [1810], Loss: 0.3018\n",
            "Epoch [4/10], Batch [1815], Loss: 0.3470\n",
            "Epoch [4/10], Batch [1820], Loss: 0.3905\n",
            "Epoch [4/10], Batch [1825], Loss: 0.4005\n",
            "Epoch [4/10], Batch [1830], Loss: 0.3568\n",
            "Epoch [4/10], Batch [1835], Loss: 0.3145\n",
            "Epoch [4/10], Batch [1840], Loss: 0.3188\n",
            "Epoch [4/10], Batch [1845], Loss: 0.3003\n",
            "Epoch [4/10], Batch [1850], Loss: 0.2293\n",
            "Epoch [4/10], Batch [1855], Loss: 0.2339\n",
            "Epoch [4/10], Batch [1860], Loss: 0.3089\n",
            "Epoch [4/10], Batch [1865], Loss: 0.3986\n",
            "Epoch [4/10], Batch [1870], Loss: 0.3140\n",
            "Epoch [4/10], Batch [1875], Loss: 0.3620\n",
            "Epoch [4/10], Batch [1880], Loss: 0.5268\n",
            "Epoch [4/10], Batch [1885], Loss: 0.4424\n",
            "Epoch [4/10], Batch [1890], Loss: 0.4331\n",
            "Epoch [4/10], Batch [1895], Loss: 0.4212\n",
            "Epoch [4/10], Batch [1900], Loss: 0.5240\n",
            "Epoch [4/10], Batch [1905], Loss: 0.2621\n",
            "Epoch [4/10], Batch [1910], Loss: 0.3810\n",
            "Epoch [4/10], Batch [1915], Loss: 0.3142\n",
            "Epoch [4/10], Batch [1920], Loss: 0.4161\n",
            "Epoch [4/10], Batch [1925], Loss: 0.3646\n",
            "Epoch [4/10], Batch [1930], Loss: 0.3343\n",
            "Epoch [4/10], Batch [1935], Loss: 0.3079\n",
            "Epoch [4/10], Batch [1940], Loss: 0.3646\n",
            "Epoch [4/10], Batch [1945], Loss: 0.3362\n",
            "Epoch [4/10], Batch [1950], Loss: 0.4375\n",
            "Epoch [4/10], Batch [1955], Loss: 0.3227\n",
            "Epoch [4/10], Batch [1960], Loss: 0.4897\n",
            "Epoch [4/10], Batch [1965], Loss: 0.5440\n",
            "Epoch [4/10], Batch [1970], Loss: 0.6453\n",
            "Epoch [4/10], Batch [1975], Loss: 0.6974\n",
            "Epoch [4/10], Batch [1980], Loss: 0.3558\n",
            "Epoch [4/10], Batch [1985], Loss: 0.4148\n",
            "Epoch [4/10], Batch [1990], Loss: 0.3425\n",
            "Epoch [4/10], Batch [1995], Loss: 0.2878\n",
            "Epoch [4/10], Batch [2000], Loss: 0.2668\n",
            "Epoch [4/10], Batch [2005], Loss: 0.3612\n",
            "Epoch [4/10], Batch [2010], Loss: 0.4700\n",
            "Epoch [4/10], Batch [2015], Loss: 0.5198\n",
            "Epoch [4/10], Batch [2020], Loss: 0.3768\n",
            "Epoch [4/10], Batch [2025], Loss: 0.3193\n",
            "Epoch [4/10], Batch [2030], Loss: 0.3609\n",
            "Epoch [4/10], Batch [2035], Loss: 0.2818\n",
            "Epoch [4/10], Batch [2040], Loss: 0.3402\n",
            "Epoch [4/10], Batch [2045], Loss: 0.3035\n",
            "Epoch [4/10], Batch [2050], Loss: 0.4621\n",
            "Epoch [4/10], Batch [2055], Loss: 0.3704\n",
            "Epoch [4/10], Batch [2060], Loss: 0.4356\n",
            "Epoch [4/10], Average Loss: 0.3964\n",
            "Epoch [5/10], Batch [0], Loss: 0.4460\n",
            "Epoch [5/10], Batch [5], Loss: 0.3416\n",
            "Epoch [5/10], Batch [10], Loss: 0.3528\n",
            "Epoch [5/10], Batch [15], Loss: 0.4256\n",
            "Epoch [5/10], Batch [20], Loss: 0.3813\n",
            "Epoch [5/10], Batch [25], Loss: 0.5056\n",
            "Epoch [5/10], Batch [30], Loss: 0.4381\n",
            "Epoch [5/10], Batch [35], Loss: 0.5481\n",
            "Epoch [5/10], Batch [40], Loss: 0.2877\n",
            "Epoch [5/10], Batch [45], Loss: 0.3511\n",
            "Epoch [5/10], Batch [50], Loss: 0.4626\n",
            "Epoch [5/10], Batch [55], Loss: 0.3945\n",
            "Epoch [5/10], Batch [60], Loss: 0.4185\n",
            "Epoch [5/10], Batch [65], Loss: 0.3308\n",
            "Epoch [5/10], Batch [70], Loss: 0.4551\n",
            "Epoch [5/10], Batch [75], Loss: 0.4689\n",
            "Epoch [5/10], Batch [80], Loss: 0.3629\n",
            "Epoch [5/10], Batch [85], Loss: 0.4868\n",
            "Epoch [5/10], Batch [90], Loss: 0.4843\n",
            "Epoch [5/10], Batch [95], Loss: 0.5925\n",
            "Epoch [5/10], Batch [100], Loss: 0.2465\n",
            "Epoch [5/10], Batch [105], Loss: 0.3343\n",
            "Epoch [5/10], Batch [110], Loss: 0.4036\n",
            "Epoch [5/10], Batch [115], Loss: 0.4712\n",
            "Epoch [5/10], Batch [120], Loss: 0.3445\n",
            "Epoch [5/10], Batch [125], Loss: 0.4533\n",
            "Epoch [5/10], Batch [130], Loss: 0.3566\n",
            "Epoch [5/10], Batch [135], Loss: 0.4652\n",
            "Epoch [5/10], Batch [140], Loss: 0.3659\n",
            "Epoch [5/10], Batch [145], Loss: 0.3688\n",
            "Epoch [5/10], Batch [150], Loss: 0.3795\n",
            "Epoch [5/10], Batch [155], Loss: 0.4155\n",
            "Epoch [5/10], Batch [160], Loss: 0.4032\n",
            "Epoch [5/10], Batch [165], Loss: 0.2962\n",
            "Epoch [5/10], Batch [170], Loss: 0.2115\n",
            "Epoch [5/10], Batch [175], Loss: 0.4082\n",
            "Epoch [5/10], Batch [180], Loss: 0.4131\n",
            "Epoch [5/10], Batch [185], Loss: 0.4218\n",
            "Epoch [5/10], Batch [190], Loss: 0.2634\n",
            "Epoch [5/10], Batch [195], Loss: 0.5107\n",
            "Epoch [5/10], Batch [200], Loss: 0.5185\n",
            "Epoch [5/10], Batch [205], Loss: 0.4472\n",
            "Epoch [5/10], Batch [210], Loss: 0.2665\n",
            "Epoch [5/10], Batch [215], Loss: 0.4753\n",
            "Epoch [5/10], Batch [220], Loss: 0.4862\n",
            "Epoch [5/10], Batch [225], Loss: 0.6172\n",
            "Epoch [5/10], Batch [230], Loss: 0.4197\n",
            "Epoch [5/10], Batch [235], Loss: 0.3476\n",
            "Epoch [5/10], Batch [240], Loss: 0.3637\n",
            "Epoch [5/10], Batch [245], Loss: 0.2532\n",
            "Epoch [5/10], Batch [250], Loss: 0.3401\n",
            "Epoch [5/10], Batch [255], Loss: 0.3282\n",
            "Epoch [5/10], Batch [260], Loss: 0.3303\n",
            "Epoch [5/10], Batch [265], Loss: 0.2881\n",
            "Epoch [5/10], Batch [270], Loss: 0.3444\n",
            "Epoch [5/10], Batch [275], Loss: 0.4048\n",
            "Epoch [5/10], Batch [280], Loss: 0.3252\n",
            "Epoch [5/10], Batch [285], Loss: 0.5050\n",
            "Epoch [5/10], Batch [290], Loss: 0.3212\n",
            "Epoch [5/10], Batch [295], Loss: 0.2956\n",
            "Epoch [5/10], Batch [300], Loss: 0.2575\n",
            "Epoch [5/10], Batch [305], Loss: 0.5359\n",
            "Epoch [5/10], Batch [310], Loss: 0.2593\n",
            "Epoch [5/10], Batch [315], Loss: 0.2952\n",
            "Epoch [5/10], Batch [320], Loss: 0.2890\n",
            "Epoch [5/10], Batch [325], Loss: 0.1748\n",
            "Epoch [5/10], Batch [330], Loss: 0.3614\n",
            "Epoch [5/10], Batch [335], Loss: 0.3310\n",
            "Epoch [5/10], Batch [340], Loss: 0.4483\n",
            "Epoch [5/10], Batch [345], Loss: 0.3644\n",
            "Epoch [5/10], Batch [350], Loss: 0.3964\n",
            "Epoch [5/10], Batch [355], Loss: 0.3769\n",
            "Epoch [5/10], Batch [360], Loss: 0.3111\n",
            "Epoch [5/10], Batch [365], Loss: 0.2417\n",
            "Epoch [5/10], Batch [370], Loss: 0.4076\n",
            "Epoch [5/10], Batch [375], Loss: 0.2943\n",
            "Epoch [5/10], Batch [380], Loss: 0.2973\n",
            "Epoch [5/10], Batch [385], Loss: 0.3986\n",
            "Epoch [5/10], Batch [390], Loss: 0.4652\n",
            "Epoch [5/10], Batch [395], Loss: 0.3213\n",
            "Epoch [5/10], Batch [400], Loss: 0.2517\n",
            "Epoch [5/10], Batch [405], Loss: 0.2155\n",
            "Epoch [5/10], Batch [410], Loss: 0.2864\n",
            "Epoch [5/10], Batch [415], Loss: 0.4210\n",
            "Epoch [5/10], Batch [420], Loss: 0.3570\n",
            "Epoch [5/10], Batch [425], Loss: 0.4870\n",
            "Epoch [5/10], Batch [430], Loss: 0.2538\n",
            "Epoch [5/10], Batch [435], Loss: 0.4292\n",
            "Epoch [5/10], Batch [440], Loss: 0.5852\n",
            "Epoch [5/10], Batch [445], Loss: 0.6580\n",
            "Epoch [5/10], Batch [450], Loss: 0.4316\n",
            "Epoch [5/10], Batch [455], Loss: 0.3374\n",
            "Epoch [5/10], Batch [460], Loss: 0.2959\n",
            "Epoch [5/10], Batch [465], Loss: 0.2897\n",
            "Epoch [5/10], Batch [470], Loss: 0.3583\n",
            "Epoch [5/10], Batch [475], Loss: 0.5017\n",
            "Epoch [5/10], Batch [480], Loss: 0.3050\n",
            "Epoch [5/10], Batch [485], Loss: 0.5379\n",
            "Epoch [5/10], Batch [490], Loss: 0.3184\n",
            "Epoch [5/10], Batch [495], Loss: 0.2505\n",
            "Epoch [5/10], Batch [500], Loss: 0.5389\n",
            "Epoch [5/10], Batch [505], Loss: 0.2930\n",
            "Epoch [5/10], Batch [510], Loss: 0.2373\n",
            "Epoch [5/10], Batch [515], Loss: 0.3774\n",
            "Epoch [5/10], Batch [520], Loss: 0.3682\n",
            "Epoch [5/10], Batch [525], Loss: 0.3923\n",
            "Epoch [5/10], Batch [530], Loss: 0.2738\n",
            "Epoch [5/10], Batch [535], Loss: 0.3055\n",
            "Epoch [5/10], Batch [540], Loss: 0.3230\n",
            "Epoch [5/10], Batch [545], Loss: 0.4293\n",
            "Epoch [5/10], Batch [550], Loss: 0.2845\n",
            "Epoch [5/10], Batch [555], Loss: 0.3574\n",
            "Epoch [5/10], Batch [560], Loss: 0.3017\n",
            "Epoch [5/10], Batch [565], Loss: 0.3343\n",
            "Epoch [5/10], Batch [570], Loss: 0.3416\n",
            "Epoch [5/10], Batch [575], Loss: 0.4514\n",
            "Epoch [5/10], Batch [580], Loss: 0.4708\n",
            "Epoch [5/10], Batch [585], Loss: 0.4120\n",
            "Epoch [5/10], Batch [590], Loss: 0.4285\n",
            "Epoch [5/10], Batch [595], Loss: 0.2864\n",
            "Epoch [5/10], Batch [600], Loss: 0.4536\n",
            "Epoch [5/10], Batch [605], Loss: 0.4552\n",
            "Epoch [5/10], Batch [610], Loss: 0.4701\n",
            "Epoch [5/10], Batch [615], Loss: 0.4297\n",
            "Epoch [5/10], Batch [620], Loss: 0.3076\n",
            "Epoch [5/10], Batch [625], Loss: 0.4602\n",
            "Epoch [5/10], Batch [630], Loss: 0.4549\n",
            "Epoch [5/10], Batch [635], Loss: 0.5511\n",
            "Epoch [5/10], Batch [640], Loss: 0.3061\n",
            "Epoch [5/10], Batch [645], Loss: 0.4771\n",
            "Epoch [5/10], Batch [650], Loss: 0.6864\n",
            "Epoch [5/10], Batch [655], Loss: 0.4446\n",
            "Epoch [5/10], Batch [660], Loss: 0.5103\n",
            "Epoch [5/10], Batch [665], Loss: 0.5100\n",
            "Epoch [5/10], Batch [670], Loss: 0.3139\n",
            "Epoch [5/10], Batch [675], Loss: 0.5149\n",
            "Epoch [5/10], Batch [680], Loss: 0.4501\n",
            "Epoch [5/10], Batch [685], Loss: 0.2347\n",
            "Epoch [5/10], Batch [690], Loss: 0.3691\n",
            "Epoch [5/10], Batch [695], Loss: 0.3928\n",
            "Epoch [5/10], Batch [700], Loss: 0.3612\n",
            "Epoch [5/10], Batch [705], Loss: 0.2779\n",
            "Epoch [5/10], Batch [710], Loss: 0.3702\n",
            "Epoch [5/10], Batch [715], Loss: 0.3706\n",
            "Epoch [5/10], Batch [720], Loss: 0.4480\n",
            "Epoch [5/10], Batch [725], Loss: 0.5534\n",
            "Epoch [5/10], Batch [730], Loss: 0.3535\n",
            "Epoch [5/10], Batch [735], Loss: 0.4218\n",
            "Epoch [5/10], Batch [740], Loss: 0.3845\n",
            "Epoch [5/10], Batch [745], Loss: 0.4249\n",
            "Epoch [5/10], Batch [750], Loss: 0.5286\n",
            "Epoch [5/10], Batch [755], Loss: 0.2783\n",
            "Epoch [5/10], Batch [760], Loss: 0.2817\n",
            "Epoch [5/10], Batch [765], Loss: 0.3399\n",
            "Epoch [5/10], Batch [770], Loss: 0.3606\n",
            "Epoch [5/10], Batch [775], Loss: 0.2564\n",
            "Epoch [5/10], Batch [780], Loss: 0.3137\n",
            "Epoch [5/10], Batch [785], Loss: 0.3388\n",
            "Epoch [5/10], Batch [790], Loss: 0.2911\n",
            "Epoch [5/10], Batch [795], Loss: 0.3921\n",
            "Epoch [5/10], Batch [800], Loss: 0.2103\n",
            "Epoch [5/10], Batch [805], Loss: 0.5567\n",
            "Epoch [5/10], Batch [810], Loss: 0.4922\n",
            "Epoch [5/10], Batch [815], Loss: 0.2834\n",
            "Epoch [5/10], Batch [820], Loss: 0.2806\n",
            "Epoch [5/10], Batch [825], Loss: 0.4233\n",
            "Epoch [5/10], Batch [830], Loss: 0.4659\n",
            "Epoch [5/10], Batch [835], Loss: 0.3920\n",
            "Epoch [5/10], Batch [840], Loss: 0.2897\n",
            "Epoch [5/10], Batch [845], Loss: 0.2750\n",
            "Epoch [5/10], Batch [850], Loss: 0.3505\n",
            "Epoch [5/10], Batch [855], Loss: 0.5357\n",
            "Epoch [5/10], Batch [860], Loss: 0.5051\n",
            "Epoch [5/10], Batch [865], Loss: 0.3726\n",
            "Epoch [5/10], Batch [870], Loss: 0.3467\n",
            "Epoch [5/10], Batch [875], Loss: 0.3492\n",
            "Epoch [5/10], Batch [880], Loss: 0.3073\n",
            "Epoch [5/10], Batch [885], Loss: 0.4174\n",
            "Epoch [5/10], Batch [890], Loss: 0.4516\n",
            "Epoch [5/10], Batch [895], Loss: 0.3282\n",
            "Epoch [5/10], Batch [900], Loss: 0.4081\n",
            "Epoch [5/10], Batch [905], Loss: 0.3816\n",
            "Epoch [5/10], Batch [910], Loss: 0.4045\n",
            "Epoch [5/10], Batch [915], Loss: 0.4474\n",
            "Epoch [5/10], Batch [920], Loss: 0.4339\n",
            "Epoch [5/10], Batch [925], Loss: 0.3857\n",
            "Epoch [5/10], Batch [930], Loss: 0.2370\n",
            "Epoch [5/10], Batch [935], Loss: 0.5577\n",
            "Epoch [5/10], Batch [940], Loss: 0.3566\n",
            "Epoch [5/10], Batch [945], Loss: 0.2387\n",
            "Epoch [5/10], Batch [950], Loss: 0.3438\n",
            "Epoch [5/10], Batch [955], Loss: 0.2860\n",
            "Epoch [5/10], Batch [960], Loss: 0.2952\n",
            "Epoch [5/10], Batch [965], Loss: 0.4044\n",
            "Epoch [5/10], Batch [970], Loss: 0.2193\n",
            "Epoch [5/10], Batch [975], Loss: 0.3106\n",
            "Epoch [5/10], Batch [980], Loss: 0.3203\n",
            "Epoch [5/10], Batch [985], Loss: 0.4492\n",
            "Epoch [5/10], Batch [990], Loss: 0.3590\n",
            "Epoch [5/10], Batch [995], Loss: 0.3315\n",
            "Epoch [5/10], Batch [1000], Loss: 0.3444\n",
            "Epoch [5/10], Batch [1005], Loss: 0.4081\n",
            "Epoch [5/10], Batch [1010], Loss: 0.4259\n",
            "Epoch [5/10], Batch [1015], Loss: 0.3835\n",
            "Epoch [5/10], Batch [1020], Loss: 0.2891\n",
            "Epoch [5/10], Batch [1025], Loss: 0.5353\n",
            "Epoch [5/10], Batch [1030], Loss: 0.3442\n",
            "Epoch [5/10], Batch [1035], Loss: 0.3021\n",
            "Epoch [5/10], Batch [1040], Loss: 0.4087\n",
            "Epoch [5/10], Batch [1045], Loss: 0.3065\n",
            "Epoch [5/10], Batch [1050], Loss: 0.3345\n",
            "Epoch [5/10], Batch [1055], Loss: 0.4435\n",
            "Epoch [5/10], Batch [1060], Loss: 0.3380\n",
            "Epoch [5/10], Batch [1065], Loss: 0.2403\n",
            "Epoch [5/10], Batch [1070], Loss: 0.3223\n",
            "Epoch [5/10], Batch [1075], Loss: 0.3550\n",
            "Epoch [5/10], Batch [1080], Loss: 0.4224\n",
            "Epoch [5/10], Batch [1085], Loss: 0.3515\n",
            "Epoch [5/10], Batch [1090], Loss: 0.5113\n",
            "Epoch [5/10], Batch [1095], Loss: 0.5461\n",
            "Epoch [5/10], Batch [1100], Loss: 0.4547\n",
            "Epoch [5/10], Batch [1105], Loss: 0.3853\n",
            "Epoch [5/10], Batch [1110], Loss: 0.3293\n",
            "Epoch [5/10], Batch [1115], Loss: 0.2917\n",
            "Epoch [5/10], Batch [1120], Loss: 0.3334\n",
            "Epoch [5/10], Batch [1125], Loss: 0.4674\n",
            "Epoch [5/10], Batch [1130], Loss: 0.2606\n",
            "Epoch [5/10], Batch [1135], Loss: 0.4825\n",
            "Epoch [5/10], Batch [1140], Loss: 0.2884\n",
            "Epoch [5/10], Batch [1145], Loss: 0.3697\n",
            "Epoch [5/10], Batch [1150], Loss: 0.5256\n",
            "Epoch [5/10], Batch [1155], Loss: 0.2453\n",
            "Epoch [5/10], Batch [1160], Loss: 0.3055\n",
            "Epoch [5/10], Batch [1165], Loss: 0.3311\n",
            "Epoch [5/10], Batch [1170], Loss: 0.2425\n",
            "Epoch [5/10], Batch [1175], Loss: 0.2098\n",
            "Epoch [5/10], Batch [1180], Loss: 0.3785\n",
            "Epoch [5/10], Batch [1185], Loss: 0.3792\n",
            "Epoch [5/10], Batch [1190], Loss: 0.5516\n",
            "Epoch [5/10], Batch [1195], Loss: 0.4307\n",
            "Epoch [5/10], Batch [1200], Loss: 0.3043\n",
            "Epoch [5/10], Batch [1205], Loss: 0.4411\n",
            "Epoch [5/10], Batch [1210], Loss: 0.3498\n",
            "Epoch [5/10], Batch [1215], Loss: 0.3796\n",
            "Epoch [5/10], Batch [1220], Loss: 0.4007\n",
            "Epoch [5/10], Batch [1225], Loss: 0.4218\n",
            "Epoch [5/10], Batch [1230], Loss: 0.4649\n",
            "Epoch [5/10], Batch [1235], Loss: 0.3138\n",
            "Epoch [5/10], Batch [1240], Loss: 0.4249\n",
            "Epoch [5/10], Batch [1245], Loss: 0.3395\n",
            "Epoch [5/10], Batch [1250], Loss: 0.3832\n",
            "Epoch [5/10], Batch [1255], Loss: 0.5093\n",
            "Epoch [5/10], Batch [1260], Loss: 0.3361\n",
            "Epoch [5/10], Batch [1265], Loss: 0.3558\n",
            "Epoch [5/10], Batch [1270], Loss: 0.2906\n",
            "Epoch [5/10], Batch [1275], Loss: 0.3248\n",
            "Epoch [5/10], Batch [1280], Loss: 0.3394\n",
            "Epoch [5/10], Batch [1285], Loss: 0.2377\n",
            "Epoch [5/10], Batch [1290], Loss: 0.2938\n",
            "Epoch [5/10], Batch [1295], Loss: 0.3826\n",
            "Epoch [5/10], Batch [1300], Loss: 0.3566\n",
            "Epoch [5/10], Batch [1305], Loss: 0.2926\n",
            "Epoch [5/10], Batch [1310], Loss: 0.2604\n",
            "Epoch [5/10], Batch [1315], Loss: 0.4948\n",
            "Epoch [5/10], Batch [1320], Loss: 0.3359\n",
            "Epoch [5/10], Batch [1325], Loss: 0.3633\n",
            "Epoch [5/10], Batch [1330], Loss: 0.4012\n",
            "Epoch [5/10], Batch [1335], Loss: 0.3383\n",
            "Epoch [5/10], Batch [1340], Loss: 0.3705\n",
            "Epoch [5/10], Batch [1345], Loss: 0.3523\n",
            "Epoch [5/10], Batch [1350], Loss: 0.4633\n",
            "Epoch [5/10], Batch [1355], Loss: 0.6126\n",
            "Epoch [5/10], Batch [1360], Loss: 0.3569\n",
            "Epoch [5/10], Batch [1365], Loss: 0.3901\n",
            "Epoch [5/10], Batch [1370], Loss: 0.3888\n",
            "Epoch [5/10], Batch [1375], Loss: 0.3880\n",
            "Epoch [5/10], Batch [1380], Loss: 0.5060\n",
            "Epoch [5/10], Batch [1385], Loss: 0.4987\n",
            "Epoch [5/10], Batch [1390], Loss: 0.6562\n",
            "Epoch [5/10], Batch [1395], Loss: 0.3373\n",
            "Epoch [5/10], Batch [1400], Loss: 0.4466\n",
            "Epoch [5/10], Batch [1405], Loss: 0.2806\n",
            "Epoch [5/10], Batch [1410], Loss: 0.2936\n",
            "Epoch [5/10], Batch [1415], Loss: 0.3473\n",
            "Epoch [5/10], Batch [1420], Loss: 0.2541\n",
            "Epoch [5/10], Batch [1425], Loss: 0.2999\n",
            "Epoch [5/10], Batch [1430], Loss: 0.6190\n",
            "Epoch [5/10], Batch [1435], Loss: 0.5506\n",
            "Epoch [5/10], Batch [1440], Loss: 0.3786\n",
            "Epoch [5/10], Batch [1445], Loss: 0.4920\n",
            "Epoch [5/10], Batch [1450], Loss: 0.3905\n",
            "Epoch [5/10], Batch [1455], Loss: 0.3085\n",
            "Epoch [5/10], Batch [1460], Loss: 0.2246\n",
            "Epoch [5/10], Batch [1465], Loss: 0.3952\n",
            "Epoch [5/10], Batch [1470], Loss: 0.3505\n",
            "Epoch [5/10], Batch [1475], Loss: 0.2323\n",
            "Epoch [5/10], Batch [1480], Loss: 0.3672\n",
            "Epoch [5/10], Batch [1485], Loss: 0.4829\n",
            "Epoch [5/10], Batch [1490], Loss: 0.4820\n",
            "Epoch [5/10], Batch [1495], Loss: 0.4446\n",
            "Epoch [5/10], Batch [1500], Loss: 0.2012\n",
            "Epoch [5/10], Batch [1505], Loss: 0.4174\n",
            "Epoch [5/10], Batch [1510], Loss: 0.3013\n",
            "Epoch [5/10], Batch [1515], Loss: 0.3034\n",
            "Epoch [5/10], Batch [1520], Loss: 0.2822\n",
            "Epoch [5/10], Batch [1525], Loss: 0.3663\n",
            "Epoch [5/10], Batch [1530], Loss: 0.3840\n",
            "Epoch [5/10], Batch [1535], Loss: 0.4403\n",
            "Epoch [5/10], Batch [1540], Loss: 0.4204\n",
            "Epoch [5/10], Batch [1545], Loss: 0.3926\n",
            "Epoch [5/10], Batch [1550], Loss: 0.3740\n",
            "Epoch [5/10], Batch [1555], Loss: 0.2777\n",
            "Epoch [5/10], Batch [1560], Loss: 0.3487\n",
            "Epoch [5/10], Batch [1565], Loss: 0.4082\n",
            "Epoch [5/10], Batch [1570], Loss: 0.4927\n",
            "Epoch [5/10], Batch [1575], Loss: 0.4655\n",
            "Epoch [5/10], Batch [1580], Loss: 0.3481\n",
            "Epoch [5/10], Batch [1585], Loss: 0.3078\n",
            "Epoch [5/10], Batch [1590], Loss: 0.4432\n",
            "Epoch [5/10], Batch [1595], Loss: 0.3410\n",
            "Epoch [5/10], Batch [1600], Loss: 0.3715\n",
            "Epoch [5/10], Batch [1605], Loss: 0.5115\n",
            "Epoch [5/10], Batch [1610], Loss: 0.3053\n",
            "Epoch [5/10], Batch [1615], Loss: 0.4442\n",
            "Epoch [5/10], Batch [1620], Loss: 0.4011\n",
            "Epoch [5/10], Batch [1625], Loss: 0.3428\n",
            "Epoch [5/10], Batch [1630], Loss: 0.2658\n",
            "Epoch [5/10], Batch [1635], Loss: 0.2942\n",
            "Epoch [5/10], Batch [1640], Loss: 0.4182\n",
            "Epoch [5/10], Batch [1645], Loss: 0.3790\n",
            "Epoch [5/10], Batch [1650], Loss: 0.3184\n",
            "Epoch [5/10], Batch [1655], Loss: 0.4000\n",
            "Epoch [5/10], Batch [1660], Loss: 0.2609\n",
            "Epoch [5/10], Batch [1665], Loss: 0.2794\n",
            "Epoch [5/10], Batch [1670], Loss: 0.3062\n",
            "Epoch [5/10], Batch [1675], Loss: 0.5381\n",
            "Epoch [5/10], Batch [1680], Loss: 0.4420\n",
            "Epoch [5/10], Batch [1685], Loss: 0.4583\n",
            "Epoch [5/10], Batch [1690], Loss: 0.3891\n",
            "Epoch [5/10], Batch [1695], Loss: 0.2645\n",
            "Epoch [5/10], Batch [1700], Loss: 0.3451\n",
            "Epoch [5/10], Batch [1705], Loss: 0.2959\n",
            "Epoch [5/10], Batch [1710], Loss: 0.5236\n",
            "Epoch [5/10], Batch [1715], Loss: 0.4141\n",
            "Epoch [5/10], Batch [1720], Loss: 0.3683\n",
            "Epoch [5/10], Batch [1725], Loss: 0.4871\n",
            "Epoch [5/10], Batch [1730], Loss: 0.3207\n",
            "Epoch [5/10], Batch [1735], Loss: 0.2930\n",
            "Epoch [5/10], Batch [1740], Loss: 0.3202\n",
            "Epoch [5/10], Batch [1745], Loss: 0.3307\n",
            "Epoch [5/10], Batch [1750], Loss: 0.5214\n",
            "Epoch [5/10], Batch [1755], Loss: 0.3141\n",
            "Epoch [5/10], Batch [1760], Loss: 0.2080\n",
            "Epoch [5/10], Batch [1765], Loss: 0.3690\n",
            "Epoch [5/10], Batch [1770], Loss: 0.3241\n",
            "Epoch [5/10], Batch [1775], Loss: 0.3707\n",
            "Epoch [5/10], Batch [1780], Loss: 0.4342\n",
            "Epoch [5/10], Batch [1785], Loss: 0.3848\n",
            "Epoch [5/10], Batch [1790], Loss: 0.4486\n",
            "Epoch [5/10], Batch [1795], Loss: 0.3601\n",
            "Epoch [5/10], Batch [1800], Loss: 0.3263\n",
            "Epoch [5/10], Batch [1805], Loss: 0.4310\n",
            "Epoch [5/10], Batch [1810], Loss: 0.2888\n",
            "Epoch [5/10], Batch [1815], Loss: 0.3392\n",
            "Epoch [5/10], Batch [1820], Loss: 0.3765\n",
            "Epoch [5/10], Batch [1825], Loss: 0.4018\n",
            "Epoch [5/10], Batch [1830], Loss: 0.3470\n",
            "Epoch [5/10], Batch [1835], Loss: 0.3000\n",
            "Epoch [5/10], Batch [1840], Loss: 0.3125\n",
            "Epoch [5/10], Batch [1845], Loss: 0.2880\n",
            "Epoch [5/10], Batch [1850], Loss: 0.2201\n",
            "Epoch [5/10], Batch [1855], Loss: 0.2207\n",
            "Epoch [5/10], Batch [1860], Loss: 0.2918\n",
            "Epoch [5/10], Batch [1865], Loss: 0.3676\n",
            "Epoch [5/10], Batch [1870], Loss: 0.2985\n",
            "Epoch [5/10], Batch [1875], Loss: 0.3591\n",
            "Epoch [5/10], Batch [1880], Loss: 0.5366\n",
            "Epoch [5/10], Batch [1885], Loss: 0.4216\n",
            "Epoch [5/10], Batch [1890], Loss: 0.4224\n",
            "Epoch [5/10], Batch [1895], Loss: 0.4201\n",
            "Epoch [5/10], Batch [1900], Loss: 0.4865\n",
            "Epoch [5/10], Batch [1905], Loss: 0.2510\n",
            "Epoch [5/10], Batch [1910], Loss: 0.3668\n",
            "Epoch [5/10], Batch [1915], Loss: 0.2986\n",
            "Epoch [5/10], Batch [1920], Loss: 0.3952\n",
            "Epoch [5/10], Batch [1925], Loss: 0.3551\n",
            "Epoch [5/10], Batch [1930], Loss: 0.3328\n",
            "Epoch [5/10], Batch [1935], Loss: 0.3041\n",
            "Epoch [5/10], Batch [1940], Loss: 0.3621\n",
            "Epoch [5/10], Batch [1945], Loss: 0.3247\n",
            "Epoch [5/10], Batch [1950], Loss: 0.4315\n",
            "Epoch [5/10], Batch [1955], Loss: 0.3070\n",
            "Epoch [5/10], Batch [1960], Loss: 0.4698\n",
            "Epoch [5/10], Batch [1965], Loss: 0.5334\n",
            "Epoch [5/10], Batch [1970], Loss: 0.6019\n",
            "Epoch [5/10], Batch [1975], Loss: 0.6818\n",
            "Epoch [5/10], Batch [1980], Loss: 0.3388\n",
            "Epoch [5/10], Batch [1985], Loss: 0.3851\n",
            "Epoch [5/10], Batch [1990], Loss: 0.3389\n",
            "Epoch [5/10], Batch [1995], Loss: 0.2672\n",
            "Epoch [5/10], Batch [2000], Loss: 0.2602\n",
            "Epoch [5/10], Batch [2005], Loss: 0.3350\n",
            "Epoch [5/10], Batch [2010], Loss: 0.4435\n",
            "Epoch [5/10], Batch [2015], Loss: 0.5144\n",
            "Epoch [5/10], Batch [2020], Loss: 0.3922\n",
            "Epoch [5/10], Batch [2025], Loss: 0.3203\n",
            "Epoch [5/10], Batch [2030], Loss: 0.3401\n",
            "Epoch [5/10], Batch [2035], Loss: 0.2854\n",
            "Epoch [5/10], Batch [2040], Loss: 0.3300\n",
            "Epoch [5/10], Batch [2045], Loss: 0.2703\n",
            "Epoch [5/10], Batch [2050], Loss: 0.4428\n",
            "Epoch [5/10], Batch [2055], Loss: 0.3478\n",
            "Epoch [5/10], Batch [2060], Loss: 0.4369\n",
            "Epoch [5/10], Average Loss: 0.3839\n",
            "Epoch [6/10], Batch [0], Loss: 0.4076\n",
            "Epoch [6/10], Batch [5], Loss: 0.3196\n",
            "Epoch [6/10], Batch [10], Loss: 0.3479\n",
            "Epoch [6/10], Batch [15], Loss: 0.4165\n",
            "Epoch [6/10], Batch [20], Loss: 0.3691\n",
            "Epoch [6/10], Batch [25], Loss: 0.5003\n",
            "Epoch [6/10], Batch [30], Loss: 0.4298\n",
            "Epoch [6/10], Batch [35], Loss: 0.5389\n",
            "Epoch [6/10], Batch [40], Loss: 0.2768\n",
            "Epoch [6/10], Batch [45], Loss: 0.3351\n",
            "Epoch [6/10], Batch [50], Loss: 0.4605\n",
            "Epoch [6/10], Batch [55], Loss: 0.3963\n",
            "Epoch [6/10], Batch [60], Loss: 0.4103\n",
            "Epoch [6/10], Batch [65], Loss: 0.3102\n",
            "Epoch [6/10], Batch [70], Loss: 0.4419\n",
            "Epoch [6/10], Batch [75], Loss: 0.4559\n",
            "Epoch [6/10], Batch [80], Loss: 0.3664\n",
            "Epoch [6/10], Batch [85], Loss: 0.4527\n",
            "Epoch [6/10], Batch [90], Loss: 0.4619\n",
            "Epoch [6/10], Batch [95], Loss: 0.5830\n",
            "Epoch [6/10], Batch [100], Loss: 0.2518\n",
            "Epoch [6/10], Batch [105], Loss: 0.3237\n",
            "Epoch [6/10], Batch [110], Loss: 0.3933\n",
            "Epoch [6/10], Batch [115], Loss: 0.4537\n",
            "Epoch [6/10], Batch [120], Loss: 0.3591\n",
            "Epoch [6/10], Batch [125], Loss: 0.4372\n",
            "Epoch [6/10], Batch [130], Loss: 0.3362\n",
            "Epoch [6/10], Batch [135], Loss: 0.4905\n",
            "Epoch [6/10], Batch [140], Loss: 0.3626\n",
            "Epoch [6/10], Batch [145], Loss: 0.3700\n",
            "Epoch [6/10], Batch [150], Loss: 0.3769\n",
            "Epoch [6/10], Batch [155], Loss: 0.3838\n",
            "Epoch [6/10], Batch [160], Loss: 0.3528\n",
            "Epoch [6/10], Batch [165], Loss: 0.2754\n",
            "Epoch [6/10], Batch [170], Loss: 0.2042\n",
            "Epoch [6/10], Batch [175], Loss: 0.3864\n",
            "Epoch [6/10], Batch [180], Loss: 0.3763\n",
            "Epoch [6/10], Batch [185], Loss: 0.4152\n",
            "Epoch [6/10], Batch [190], Loss: 0.2699\n",
            "Epoch [6/10], Batch [195], Loss: 0.5223\n",
            "Epoch [6/10], Batch [200], Loss: 0.5040\n",
            "Epoch [6/10], Batch [205], Loss: 0.4342\n",
            "Epoch [6/10], Batch [210], Loss: 0.2494\n",
            "Epoch [6/10], Batch [215], Loss: 0.4633\n",
            "Epoch [6/10], Batch [220], Loss: 0.4828\n",
            "Epoch [6/10], Batch [225], Loss: 0.6021\n",
            "Epoch [6/10], Batch [230], Loss: 0.3859\n",
            "Epoch [6/10], Batch [235], Loss: 0.3471\n",
            "Epoch [6/10], Batch [240], Loss: 0.3589\n",
            "Epoch [6/10], Batch [245], Loss: 0.2390\n",
            "Epoch [6/10], Batch [250], Loss: 0.3322\n",
            "Epoch [6/10], Batch [255], Loss: 0.3300\n",
            "Epoch [6/10], Batch [260], Loss: 0.3196\n",
            "Epoch [6/10], Batch [265], Loss: 0.2735\n",
            "Epoch [6/10], Batch [270], Loss: 0.3386\n",
            "Epoch [6/10], Batch [275], Loss: 0.4061\n",
            "Epoch [6/10], Batch [280], Loss: 0.3225\n",
            "Epoch [6/10], Batch [285], Loss: 0.4954\n",
            "Epoch [6/10], Batch [290], Loss: 0.3198\n",
            "Epoch [6/10], Batch [295], Loss: 0.2875\n",
            "Epoch [6/10], Batch [300], Loss: 0.2400\n",
            "Epoch [6/10], Batch [305], Loss: 0.5306\n",
            "Epoch [6/10], Batch [310], Loss: 0.2268\n",
            "Epoch [6/10], Batch [315], Loss: 0.2969\n",
            "Epoch [6/10], Batch [320], Loss: 0.2739\n",
            "Epoch [6/10], Batch [325], Loss: 0.1772\n",
            "Epoch [6/10], Batch [330], Loss: 0.3624\n",
            "Epoch [6/10], Batch [335], Loss: 0.2915\n",
            "Epoch [6/10], Batch [340], Loss: 0.4376\n",
            "Epoch [6/10], Batch [345], Loss: 0.3351\n",
            "Epoch [6/10], Batch [350], Loss: 0.3931\n",
            "Epoch [6/10], Batch [355], Loss: 0.3784\n",
            "Epoch [6/10], Batch [360], Loss: 0.3009\n",
            "Epoch [6/10], Batch [365], Loss: 0.2330\n",
            "Epoch [6/10], Batch [370], Loss: 0.3881\n",
            "Epoch [6/10], Batch [375], Loss: 0.2821\n",
            "Epoch [6/10], Batch [380], Loss: 0.2888\n",
            "Epoch [6/10], Batch [385], Loss: 0.3954\n",
            "Epoch [6/10], Batch [390], Loss: 0.4696\n",
            "Epoch [6/10], Batch [395], Loss: 0.3042\n",
            "Epoch [6/10], Batch [400], Loss: 0.2416\n",
            "Epoch [6/10], Batch [405], Loss: 0.2051\n",
            "Epoch [6/10], Batch [410], Loss: 0.2837\n",
            "Epoch [6/10], Batch [415], Loss: 0.4195\n",
            "Epoch [6/10], Batch [420], Loss: 0.3303\n",
            "Epoch [6/10], Batch [425], Loss: 0.4742\n",
            "Epoch [6/10], Batch [430], Loss: 0.2369\n",
            "Epoch [6/10], Batch [435], Loss: 0.4032\n",
            "Epoch [6/10], Batch [440], Loss: 0.5992\n",
            "Epoch [6/10], Batch [445], Loss: 0.6501\n",
            "Epoch [6/10], Batch [450], Loss: 0.4155\n",
            "Epoch [6/10], Batch [455], Loss: 0.3405\n",
            "Epoch [6/10], Batch [460], Loss: 0.2743\n",
            "Epoch [6/10], Batch [465], Loss: 0.2719\n",
            "Epoch [6/10], Batch [470], Loss: 0.3455\n",
            "Epoch [6/10], Batch [475], Loss: 0.4755\n",
            "Epoch [6/10], Batch [480], Loss: 0.2873\n",
            "Epoch [6/10], Batch [485], Loss: 0.5215\n",
            "Epoch [6/10], Batch [490], Loss: 0.3116\n",
            "Epoch [6/10], Batch [495], Loss: 0.2377\n",
            "Epoch [6/10], Batch [500], Loss: 0.5139\n",
            "Epoch [6/10], Batch [505], Loss: 0.2607\n",
            "Epoch [6/10], Batch [510], Loss: 0.2270\n",
            "Epoch [6/10], Batch [515], Loss: 0.3411\n",
            "Epoch [6/10], Batch [520], Loss: 0.3562\n",
            "Epoch [6/10], Batch [525], Loss: 0.3622\n",
            "Epoch [6/10], Batch [530], Loss: 0.2554\n",
            "Epoch [6/10], Batch [535], Loss: 0.3023\n",
            "Epoch [6/10], Batch [540], Loss: 0.3391\n",
            "Epoch [6/10], Batch [545], Loss: 0.4094\n",
            "Epoch [6/10], Batch [550], Loss: 0.2812\n",
            "Epoch [6/10], Batch [555], Loss: 0.3604\n",
            "Epoch [6/10], Batch [560], Loss: 0.3080\n",
            "Epoch [6/10], Batch [565], Loss: 0.3206\n",
            "Epoch [6/10], Batch [570], Loss: 0.3252\n",
            "Epoch [6/10], Batch [575], Loss: 0.4121\n",
            "Epoch [6/10], Batch [580], Loss: 0.4761\n",
            "Epoch [6/10], Batch [585], Loss: 0.3888\n",
            "Epoch [6/10], Batch [590], Loss: 0.4084\n",
            "Epoch [6/10], Batch [595], Loss: 0.2670\n",
            "Epoch [6/10], Batch [600], Loss: 0.4294\n",
            "Epoch [6/10], Batch [605], Loss: 0.4432\n",
            "Epoch [6/10], Batch [610], Loss: 0.4571\n",
            "Epoch [6/10], Batch [615], Loss: 0.4183\n",
            "Epoch [6/10], Batch [620], Loss: 0.3065\n",
            "Epoch [6/10], Batch [625], Loss: 0.4363\n",
            "Epoch [6/10], Batch [630], Loss: 0.4378\n",
            "Epoch [6/10], Batch [635], Loss: 0.5289\n",
            "Epoch [6/10], Batch [640], Loss: 0.2860\n",
            "Epoch [6/10], Batch [645], Loss: 0.4657\n",
            "Epoch [6/10], Batch [650], Loss: 0.6860\n",
            "Epoch [6/10], Batch [655], Loss: 0.4307\n",
            "Epoch [6/10], Batch [660], Loss: 0.4896\n",
            "Epoch [6/10], Batch [665], Loss: 0.4988\n",
            "Epoch [6/10], Batch [670], Loss: 0.2946\n",
            "Epoch [6/10], Batch [675], Loss: 0.5043\n",
            "Epoch [6/10], Batch [680], Loss: 0.4542\n",
            "Epoch [6/10], Batch [685], Loss: 0.2242\n",
            "Epoch [6/10], Batch [690], Loss: 0.3465\n",
            "Epoch [6/10], Batch [695], Loss: 0.3567\n",
            "Epoch [6/10], Batch [700], Loss: 0.3476\n",
            "Epoch [6/10], Batch [705], Loss: 0.2754\n",
            "Epoch [6/10], Batch [710], Loss: 0.3488\n",
            "Epoch [6/10], Batch [715], Loss: 0.3790\n",
            "Epoch [6/10], Batch [720], Loss: 0.4154\n",
            "Epoch [6/10], Batch [725], Loss: 0.5309\n",
            "Epoch [6/10], Batch [730], Loss: 0.3615\n",
            "Epoch [6/10], Batch [735], Loss: 0.3865\n",
            "Epoch [6/10], Batch [740], Loss: 0.3629\n",
            "Epoch [6/10], Batch [745], Loss: 0.3998\n",
            "Epoch [6/10], Batch [750], Loss: 0.5089\n",
            "Epoch [6/10], Batch [755], Loss: 0.2806\n",
            "Epoch [6/10], Batch [760], Loss: 0.2454\n",
            "Epoch [6/10], Batch [765], Loss: 0.3281\n",
            "Epoch [6/10], Batch [770], Loss: 0.3330\n",
            "Epoch [6/10], Batch [775], Loss: 0.2569\n",
            "Epoch [6/10], Batch [780], Loss: 0.3070\n",
            "Epoch [6/10], Batch [785], Loss: 0.3265\n",
            "Epoch [6/10], Batch [790], Loss: 0.2833\n",
            "Epoch [6/10], Batch [795], Loss: 0.3769\n",
            "Epoch [6/10], Batch [800], Loss: 0.2007\n",
            "Epoch [6/10], Batch [805], Loss: 0.5318\n",
            "Epoch [6/10], Batch [810], Loss: 0.4617\n",
            "Epoch [6/10], Batch [815], Loss: 0.2701\n",
            "Epoch [6/10], Batch [820], Loss: 0.2539\n",
            "Epoch [6/10], Batch [825], Loss: 0.3945\n",
            "Epoch [6/10], Batch [830], Loss: 0.4492\n",
            "Epoch [6/10], Batch [835], Loss: 0.3738\n",
            "Epoch [6/10], Batch [840], Loss: 0.2764\n",
            "Epoch [6/10], Batch [845], Loss: 0.2530\n",
            "Epoch [6/10], Batch [850], Loss: 0.3261\n",
            "Epoch [6/10], Batch [855], Loss: 0.5275\n",
            "Epoch [6/10], Batch [860], Loss: 0.5046\n",
            "Epoch [6/10], Batch [865], Loss: 0.3395\n",
            "Epoch [6/10], Batch [870], Loss: 0.3512\n",
            "Epoch [6/10], Batch [875], Loss: 0.3622\n",
            "Epoch [6/10], Batch [880], Loss: 0.3036\n",
            "Epoch [6/10], Batch [885], Loss: 0.3811\n",
            "Epoch [6/10], Batch [890], Loss: 0.4431\n",
            "Epoch [6/10], Batch [895], Loss: 0.3163\n",
            "Epoch [6/10], Batch [900], Loss: 0.3861\n",
            "Epoch [6/10], Batch [905], Loss: 0.3613\n",
            "Epoch [6/10], Batch [910], Loss: 0.3988\n",
            "Epoch [6/10], Batch [915], Loss: 0.4550\n",
            "Epoch [6/10], Batch [920], Loss: 0.4301\n",
            "Epoch [6/10], Batch [925], Loss: 0.3930\n",
            "Epoch [6/10], Batch [930], Loss: 0.2318\n",
            "Epoch [6/10], Batch [935], Loss: 0.5345\n",
            "Epoch [6/10], Batch [940], Loss: 0.3437\n",
            "Epoch [6/10], Batch [945], Loss: 0.2111\n",
            "Epoch [6/10], Batch [950], Loss: 0.3294\n",
            "Epoch [6/10], Batch [955], Loss: 0.2835\n",
            "Epoch [6/10], Batch [960], Loss: 0.2933\n",
            "Epoch [6/10], Batch [965], Loss: 0.3986\n",
            "Epoch [6/10], Batch [970], Loss: 0.2068\n",
            "Epoch [6/10], Batch [975], Loss: 0.3077\n",
            "Epoch [6/10], Batch [980], Loss: 0.3231\n",
            "Epoch [6/10], Batch [985], Loss: 0.4401\n",
            "Epoch [6/10], Batch [990], Loss: 0.3712\n",
            "Epoch [6/10], Batch [995], Loss: 0.3044\n",
            "Epoch [6/10], Batch [1000], Loss: 0.3123\n",
            "Epoch [6/10], Batch [1005], Loss: 0.3893\n",
            "Epoch [6/10], Batch [1010], Loss: 0.4186\n",
            "Epoch [6/10], Batch [1015], Loss: 0.3839\n",
            "Epoch [6/10], Batch [1020], Loss: 0.2993\n",
            "Epoch [6/10], Batch [1025], Loss: 0.5060\n",
            "Epoch [6/10], Batch [1030], Loss: 0.3136\n",
            "Epoch [6/10], Batch [1035], Loss: 0.3048\n",
            "Epoch [6/10], Batch [1040], Loss: 0.3896\n",
            "Epoch [6/10], Batch [1045], Loss: 0.2949\n",
            "Epoch [6/10], Batch [1050], Loss: 0.3036\n",
            "Epoch [6/10], Batch [1055], Loss: 0.4303\n",
            "Epoch [6/10], Batch [1060], Loss: 0.3515\n",
            "Epoch [6/10], Batch [1065], Loss: 0.2292\n",
            "Epoch [6/10], Batch [1070], Loss: 0.3029\n",
            "Epoch [6/10], Batch [1075], Loss: 0.3284\n",
            "Epoch [6/10], Batch [1080], Loss: 0.4052\n",
            "Epoch [6/10], Batch [1085], Loss: 0.3390\n",
            "Epoch [6/10], Batch [1090], Loss: 0.4736\n",
            "Epoch [6/10], Batch [1095], Loss: 0.5405\n",
            "Epoch [6/10], Batch [1100], Loss: 0.4515\n",
            "Epoch [6/10], Batch [1105], Loss: 0.3818\n",
            "Epoch [6/10], Batch [1110], Loss: 0.3036\n",
            "Epoch [6/10], Batch [1115], Loss: 0.2809\n",
            "Epoch [6/10], Batch [1120], Loss: 0.3209\n",
            "Epoch [6/10], Batch [1125], Loss: 0.4573\n",
            "Epoch [6/10], Batch [1130], Loss: 0.2569\n",
            "Epoch [6/10], Batch [1135], Loss: 0.4690\n",
            "Epoch [6/10], Batch [1140], Loss: 0.2832\n",
            "Epoch [6/10], Batch [1145], Loss: 0.3708\n",
            "Epoch [6/10], Batch [1150], Loss: 0.5053\n",
            "Epoch [6/10], Batch [1155], Loss: 0.2427\n",
            "Epoch [6/10], Batch [1160], Loss: 0.3019\n",
            "Epoch [6/10], Batch [1165], Loss: 0.3363\n",
            "Epoch [6/10], Batch [1170], Loss: 0.2288\n",
            "Epoch [6/10], Batch [1175], Loss: 0.2212\n",
            "Epoch [6/10], Batch [1180], Loss: 0.3720\n",
            "Epoch [6/10], Batch [1185], Loss: 0.3771\n",
            "Epoch [6/10], Batch [1190], Loss: 0.5344\n",
            "Epoch [6/10], Batch [1195], Loss: 0.4124\n",
            "Epoch [6/10], Batch [1200], Loss: 0.2963\n",
            "Epoch [6/10], Batch [1205], Loss: 0.4218\n",
            "Epoch [6/10], Batch [1210], Loss: 0.3543\n",
            "Epoch [6/10], Batch [1215], Loss: 0.3540\n",
            "Epoch [6/10], Batch [1220], Loss: 0.3971\n",
            "Epoch [6/10], Batch [1225], Loss: 0.4144\n",
            "Epoch [6/10], Batch [1230], Loss: 0.4487\n",
            "Epoch [6/10], Batch [1235], Loss: 0.3196\n",
            "Epoch [6/10], Batch [1240], Loss: 0.4083\n",
            "Epoch [6/10], Batch [1245], Loss: 0.3401\n",
            "Epoch [6/10], Batch [1250], Loss: 0.3630\n",
            "Epoch [6/10], Batch [1255], Loss: 0.4864\n",
            "Epoch [6/10], Batch [1260], Loss: 0.3281\n",
            "Epoch [6/10], Batch [1265], Loss: 0.3438\n",
            "Epoch [6/10], Batch [1270], Loss: 0.2778\n",
            "Epoch [6/10], Batch [1275], Loss: 0.3072\n",
            "Epoch [6/10], Batch [1280], Loss: 0.3461\n",
            "Epoch [6/10], Batch [1285], Loss: 0.2204\n",
            "Epoch [6/10], Batch [1290], Loss: 0.2841\n",
            "Epoch [6/10], Batch [1295], Loss: 0.3651\n",
            "Epoch [6/10], Batch [1300], Loss: 0.3458\n",
            "Epoch [6/10], Batch [1305], Loss: 0.2671\n",
            "Epoch [6/10], Batch [1310], Loss: 0.2482\n",
            "Epoch [6/10], Batch [1315], Loss: 0.4960\n",
            "Epoch [6/10], Batch [1320], Loss: 0.3296\n",
            "Epoch [6/10], Batch [1325], Loss: 0.3554\n",
            "Epoch [6/10], Batch [1330], Loss: 0.4030\n",
            "Epoch [6/10], Batch [1335], Loss: 0.3273\n",
            "Epoch [6/10], Batch [1340], Loss: 0.3474\n",
            "Epoch [6/10], Batch [1345], Loss: 0.3519\n",
            "Epoch [6/10], Batch [1350], Loss: 0.4392\n",
            "Epoch [6/10], Batch [1355], Loss: 0.5887\n",
            "Epoch [6/10], Batch [1360], Loss: 0.3400\n",
            "Epoch [6/10], Batch [1365], Loss: 0.3753\n",
            "Epoch [6/10], Batch [1370], Loss: 0.3822\n",
            "Epoch [6/10], Batch [1375], Loss: 0.3710\n",
            "Epoch [6/10], Batch [1380], Loss: 0.4564\n",
            "Epoch [6/10], Batch [1385], Loss: 0.4924\n",
            "Epoch [6/10], Batch [1390], Loss: 0.6537\n",
            "Epoch [6/10], Batch [1395], Loss: 0.3149\n",
            "Epoch [6/10], Batch [1400], Loss: 0.4380\n",
            "Epoch [6/10], Batch [1405], Loss: 0.2693\n",
            "Epoch [6/10], Batch [1410], Loss: 0.2849\n",
            "Epoch [6/10], Batch [1415], Loss: 0.3273\n",
            "Epoch [6/10], Batch [1420], Loss: 0.2676\n",
            "Epoch [6/10], Batch [1425], Loss: 0.2874\n",
            "Epoch [6/10], Batch [1430], Loss: 0.5805\n",
            "Epoch [6/10], Batch [1435], Loss: 0.5349\n",
            "Epoch [6/10], Batch [1440], Loss: 0.3350\n",
            "Epoch [6/10], Batch [1445], Loss: 0.4472\n",
            "Epoch [6/10], Batch [1450], Loss: 0.3913\n",
            "Epoch [6/10], Batch [1455], Loss: 0.3046\n",
            "Epoch [6/10], Batch [1460], Loss: 0.2108\n",
            "Epoch [6/10], Batch [1465], Loss: 0.3825\n",
            "Epoch [6/10], Batch [1470], Loss: 0.3863\n",
            "Epoch [6/10], Batch [1475], Loss: 0.2288\n",
            "Epoch [6/10], Batch [1480], Loss: 0.3503\n",
            "Epoch [6/10], Batch [1485], Loss: 0.4372\n",
            "Epoch [6/10], Batch [1490], Loss: 0.4694\n",
            "Epoch [6/10], Batch [1495], Loss: 0.4160\n",
            "Epoch [6/10], Batch [1500], Loss: 0.1895\n",
            "Epoch [6/10], Batch [1505], Loss: 0.4049\n",
            "Epoch [6/10], Batch [1510], Loss: 0.2931\n",
            "Epoch [6/10], Batch [1515], Loss: 0.2829\n",
            "Epoch [6/10], Batch [1520], Loss: 0.3067\n",
            "Epoch [6/10], Batch [1525], Loss: 0.3551\n",
            "Epoch [6/10], Batch [1530], Loss: 0.3657\n",
            "Epoch [6/10], Batch [1535], Loss: 0.4170\n",
            "Epoch [6/10], Batch [1540], Loss: 0.4157\n",
            "Epoch [6/10], Batch [1545], Loss: 0.3656\n",
            "Epoch [6/10], Batch [1550], Loss: 0.3528\n",
            "Epoch [6/10], Batch [1555], Loss: 0.2860\n",
            "Epoch [6/10], Batch [1560], Loss: 0.3260\n",
            "Epoch [6/10], Batch [1565], Loss: 0.4006\n",
            "Epoch [6/10], Batch [1570], Loss: 0.4590\n",
            "Epoch [6/10], Batch [1575], Loss: 0.4541\n",
            "Epoch [6/10], Batch [1580], Loss: 0.3330\n",
            "Epoch [6/10], Batch [1585], Loss: 0.3032\n",
            "Epoch [6/10], Batch [1590], Loss: 0.4505\n",
            "Epoch [6/10], Batch [1595], Loss: 0.3225\n",
            "Epoch [6/10], Batch [1600], Loss: 0.3644\n",
            "Epoch [6/10], Batch [1605], Loss: 0.4925\n",
            "Epoch [6/10], Batch [1610], Loss: 0.2941\n",
            "Epoch [6/10], Batch [1615], Loss: 0.4372\n",
            "Epoch [6/10], Batch [1620], Loss: 0.3572\n",
            "Epoch [6/10], Batch [1625], Loss: 0.3100\n",
            "Epoch [6/10], Batch [1630], Loss: 0.2435\n",
            "Epoch [6/10], Batch [1635], Loss: 0.2674\n",
            "Epoch [6/10], Batch [1640], Loss: 0.3832\n",
            "Epoch [6/10], Batch [1645], Loss: 0.3694\n",
            "Epoch [6/10], Batch [1650], Loss: 0.3027\n",
            "Epoch [6/10], Batch [1655], Loss: 0.3911\n",
            "Epoch [6/10], Batch [1660], Loss: 0.2497\n",
            "Epoch [6/10], Batch [1665], Loss: 0.2718\n",
            "Epoch [6/10], Batch [1670], Loss: 0.2968\n",
            "Epoch [6/10], Batch [1675], Loss: 0.5180\n",
            "Epoch [6/10], Batch [1680], Loss: 0.4395\n",
            "Epoch [6/10], Batch [1685], Loss: 0.4387\n",
            "Epoch [6/10], Batch [1690], Loss: 0.3672\n",
            "Epoch [6/10], Batch [1695], Loss: 0.2703\n",
            "Epoch [6/10], Batch [1700], Loss: 0.3207\n",
            "Epoch [6/10], Batch [1705], Loss: 0.2752\n",
            "Epoch [6/10], Batch [1710], Loss: 0.4914\n",
            "Epoch [6/10], Batch [1715], Loss: 0.4135\n",
            "Epoch [6/10], Batch [1720], Loss: 0.3677\n",
            "Epoch [6/10], Batch [1725], Loss: 0.4365\n",
            "Epoch [6/10], Batch [1730], Loss: 0.3212\n",
            "Epoch [6/10], Batch [1735], Loss: 0.2877\n",
            "Epoch [6/10], Batch [1740], Loss: 0.2801\n",
            "Epoch [6/10], Batch [1745], Loss: 0.3062\n",
            "Epoch [6/10], Batch [1750], Loss: 0.5101\n",
            "Epoch [6/10], Batch [1755], Loss: 0.3042\n",
            "Epoch [6/10], Batch [1760], Loss: 0.2023\n",
            "Epoch [6/10], Batch [1765], Loss: 0.3549\n",
            "Epoch [6/10], Batch [1770], Loss: 0.3109\n",
            "Epoch [6/10], Batch [1775], Loss: 0.3831\n",
            "Epoch [6/10], Batch [1780], Loss: 0.4228\n",
            "Epoch [6/10], Batch [1785], Loss: 0.3791\n",
            "Epoch [6/10], Batch [1790], Loss: 0.4033\n",
            "Epoch [6/10], Batch [1795], Loss: 0.3664\n",
            "Epoch [6/10], Batch [1800], Loss: 0.3047\n",
            "Epoch [6/10], Batch [1805], Loss: 0.4076\n",
            "Epoch [6/10], Batch [1810], Loss: 0.2720\n",
            "Epoch [6/10], Batch [1815], Loss: 0.3190\n",
            "Epoch [6/10], Batch [1820], Loss: 0.3571\n",
            "Epoch [6/10], Batch [1825], Loss: 0.3810\n",
            "Epoch [6/10], Batch [1830], Loss: 0.3292\n",
            "Epoch [6/10], Batch [1835], Loss: 0.2856\n",
            "Epoch [6/10], Batch [1840], Loss: 0.3015\n",
            "Epoch [6/10], Batch [1845], Loss: 0.2785\n",
            "Epoch [6/10], Batch [1850], Loss: 0.2178\n",
            "Epoch [6/10], Batch [1855], Loss: 0.2111\n",
            "Epoch [6/10], Batch [1860], Loss: 0.2719\n",
            "Epoch [6/10], Batch [1865], Loss: 0.3702\n",
            "Epoch [6/10], Batch [1870], Loss: 0.2844\n",
            "Epoch [6/10], Batch [1875], Loss: 0.3235\n",
            "Epoch [6/10], Batch [1880], Loss: 0.5037\n",
            "Epoch [6/10], Batch [1885], Loss: 0.4098\n",
            "Epoch [6/10], Batch [1890], Loss: 0.3946\n",
            "Epoch [6/10], Batch [1895], Loss: 0.3837\n",
            "Epoch [6/10], Batch [1900], Loss: 0.4601\n",
            "Epoch [6/10], Batch [1905], Loss: 0.2556\n",
            "Epoch [6/10], Batch [1910], Loss: 0.3696\n",
            "Epoch [6/10], Batch [1915], Loss: 0.2896\n",
            "Epoch [6/10], Batch [1920], Loss: 0.3973\n",
            "Epoch [6/10], Batch [1925], Loss: 0.3394\n",
            "Epoch [6/10], Batch [1930], Loss: 0.3009\n",
            "Epoch [6/10], Batch [1935], Loss: 0.2985\n",
            "Epoch [6/10], Batch [1940], Loss: 0.3614\n",
            "Epoch [6/10], Batch [1945], Loss: 0.2958\n",
            "Epoch [6/10], Batch [1950], Loss: 0.4227\n",
            "Epoch [6/10], Batch [1955], Loss: 0.2866\n",
            "Epoch [6/10], Batch [1960], Loss: 0.4547\n",
            "Epoch [6/10], Batch [1965], Loss: 0.4895\n",
            "Epoch [6/10], Batch [1970], Loss: 0.5644\n",
            "Epoch [6/10], Batch [1975], Loss: 0.6450\n",
            "Epoch [6/10], Batch [1980], Loss: 0.3099\n",
            "Epoch [6/10], Batch [1985], Loss: 0.3456\n",
            "Epoch [6/10], Batch [1990], Loss: 0.3264\n",
            "Epoch [6/10], Batch [1995], Loss: 0.2561\n",
            "Epoch [6/10], Batch [2000], Loss: 0.2522\n",
            "Epoch [6/10], Batch [2005], Loss: 0.3082\n",
            "Epoch [6/10], Batch [2010], Loss: 0.4443\n",
            "Epoch [6/10], Batch [2015], Loss: 0.5127\n",
            "Epoch [6/10], Batch [2020], Loss: 0.3532\n",
            "Epoch [6/10], Batch [2025], Loss: 0.3331\n",
            "Epoch [6/10], Batch [2030], Loss: 0.3464\n",
            "Epoch [6/10], Batch [2035], Loss: 0.2703\n",
            "Epoch [6/10], Batch [2040], Loss: 0.3164\n",
            "Epoch [6/10], Batch [2045], Loss: 0.2662\n",
            "Epoch [6/10], Batch [2050], Loss: 0.4087\n",
            "Epoch [6/10], Batch [2055], Loss: 0.3493\n",
            "Epoch [6/10], Batch [2060], Loss: 0.4171\n",
            "Epoch [6/10], Average Loss: 0.3707\n",
            "Epoch [7/10], Batch [0], Loss: 0.3795\n",
            "Epoch [7/10], Batch [5], Loss: 0.3113\n",
            "Epoch [7/10], Batch [10], Loss: 0.3430\n",
            "Epoch [7/10], Batch [15], Loss: 0.3981\n",
            "Epoch [7/10], Batch [20], Loss: 0.3672\n",
            "Epoch [7/10], Batch [25], Loss: 0.4743\n",
            "Epoch [7/10], Batch [30], Loss: 0.4254\n",
            "Epoch [7/10], Batch [35], Loss: 0.5082\n",
            "Epoch [7/10], Batch [40], Loss: 0.2792\n",
            "Epoch [7/10], Batch [45], Loss: 0.3197\n",
            "Epoch [7/10], Batch [50], Loss: 0.4149\n",
            "Epoch [7/10], Batch [55], Loss: 0.3946\n",
            "Epoch [7/10], Batch [60], Loss: 0.3881\n",
            "Epoch [7/10], Batch [65], Loss: 0.3059\n",
            "Epoch [7/10], Batch [70], Loss: 0.4292\n",
            "Epoch [7/10], Batch [75], Loss: 0.4255\n",
            "Epoch [7/10], Batch [80], Loss: 0.3282\n",
            "Epoch [7/10], Batch [85], Loss: 0.4596\n",
            "Epoch [7/10], Batch [90], Loss: 0.4451\n",
            "Epoch [7/10], Batch [95], Loss: 0.5753\n",
            "Epoch [7/10], Batch [100], Loss: 0.2582\n",
            "Epoch [7/10], Batch [105], Loss: 0.3274\n",
            "Epoch [7/10], Batch [110], Loss: 0.3714\n",
            "Epoch [7/10], Batch [115], Loss: 0.4208\n",
            "Epoch [7/10], Batch [120], Loss: 0.3188\n",
            "Epoch [7/10], Batch [125], Loss: 0.4084\n",
            "Epoch [7/10], Batch [130], Loss: 0.3353\n",
            "Epoch [7/10], Batch [135], Loss: 0.4877\n",
            "Epoch [7/10], Batch [140], Loss: 0.3523\n",
            "Epoch [7/10], Batch [145], Loss: 0.3490\n",
            "Epoch [7/10], Batch [150], Loss: 0.3563\n",
            "Epoch [7/10], Batch [155], Loss: 0.3590\n",
            "Epoch [7/10], Batch [160], Loss: 0.3704\n",
            "Epoch [7/10], Batch [165], Loss: 0.2722\n",
            "Epoch [7/10], Batch [170], Loss: 0.2055\n",
            "Epoch [7/10], Batch [175], Loss: 0.3609\n",
            "Epoch [7/10], Batch [180], Loss: 0.3251\n",
            "Epoch [7/10], Batch [185], Loss: 0.4149\n",
            "Epoch [7/10], Batch [190], Loss: 0.2614\n",
            "Epoch [7/10], Batch [195], Loss: 0.5019\n",
            "Epoch [7/10], Batch [200], Loss: 0.5063\n",
            "Epoch [7/10], Batch [205], Loss: 0.4048\n",
            "Epoch [7/10], Batch [210], Loss: 0.2331\n",
            "Epoch [7/10], Batch [215], Loss: 0.4583\n",
            "Epoch [7/10], Batch [220], Loss: 0.4866\n",
            "Epoch [7/10], Batch [225], Loss: 0.6035\n",
            "Epoch [7/10], Batch [230], Loss: 0.3687\n",
            "Epoch [7/10], Batch [235], Loss: 0.3595\n",
            "Epoch [7/10], Batch [240], Loss: 0.3489\n",
            "Epoch [7/10], Batch [245], Loss: 0.2370\n",
            "Epoch [7/10], Batch [250], Loss: 0.3293\n",
            "Epoch [7/10], Batch [255], Loss: 0.3260\n",
            "Epoch [7/10], Batch [260], Loss: 0.3196\n",
            "Epoch [7/10], Batch [265], Loss: 0.2697\n",
            "Epoch [7/10], Batch [270], Loss: 0.3395\n",
            "Epoch [7/10], Batch [275], Loss: 0.3935\n",
            "Epoch [7/10], Batch [280], Loss: 0.3197\n",
            "Epoch [7/10], Batch [285], Loss: 0.4588\n",
            "Epoch [7/10], Batch [290], Loss: 0.2940\n",
            "Epoch [7/10], Batch [295], Loss: 0.2697\n",
            "Epoch [7/10], Batch [300], Loss: 0.2243\n",
            "Epoch [7/10], Batch [305], Loss: 0.5210\n",
            "Epoch [7/10], Batch [310], Loss: 0.2183\n",
            "Epoch [7/10], Batch [315], Loss: 0.2713\n",
            "Epoch [7/10], Batch [320], Loss: 0.2637\n",
            "Epoch [7/10], Batch [325], Loss: 0.1753\n",
            "Epoch [7/10], Batch [330], Loss: 0.3544\n",
            "Epoch [7/10], Batch [335], Loss: 0.2934\n",
            "Epoch [7/10], Batch [340], Loss: 0.4150\n",
            "Epoch [7/10], Batch [345], Loss: 0.3292\n",
            "Epoch [7/10], Batch [350], Loss: 0.3577\n",
            "Epoch [7/10], Batch [355], Loss: 0.3723\n",
            "Epoch [7/10], Batch [360], Loss: 0.2852\n",
            "Epoch [7/10], Batch [365], Loss: 0.2340\n",
            "Epoch [7/10], Batch [370], Loss: 0.3998\n",
            "Epoch [7/10], Batch [375], Loss: 0.2628\n",
            "Epoch [7/10], Batch [380], Loss: 0.2592\n",
            "Epoch [7/10], Batch [385], Loss: 0.3743\n",
            "Epoch [7/10], Batch [390], Loss: 0.4558\n",
            "Epoch [7/10], Batch [395], Loss: 0.2955\n",
            "Epoch [7/10], Batch [400], Loss: 0.2387\n",
            "Epoch [7/10], Batch [405], Loss: 0.2045\n",
            "Epoch [7/10], Batch [410], Loss: 0.2637\n",
            "Epoch [7/10], Batch [415], Loss: 0.4146\n",
            "Epoch [7/10], Batch [420], Loss: 0.3079\n",
            "Epoch [7/10], Batch [425], Loss: 0.4366\n",
            "Epoch [7/10], Batch [430], Loss: 0.2203\n",
            "Epoch [7/10], Batch [435], Loss: 0.3975\n",
            "Epoch [7/10], Batch [440], Loss: 0.5813\n",
            "Epoch [7/10], Batch [445], Loss: 0.6199\n",
            "Epoch [7/10], Batch [450], Loss: 0.3996\n",
            "Epoch [7/10], Batch [455], Loss: 0.3384\n",
            "Epoch [7/10], Batch [460], Loss: 0.2843\n",
            "Epoch [7/10], Batch [465], Loss: 0.2686\n",
            "Epoch [7/10], Batch [470], Loss: 0.3348\n",
            "Epoch [7/10], Batch [475], Loss: 0.4566\n",
            "Epoch [7/10], Batch [480], Loss: 0.2910\n",
            "Epoch [7/10], Batch [485], Loss: 0.4978\n",
            "Epoch [7/10], Batch [490], Loss: 0.2846\n",
            "Epoch [7/10], Batch [495], Loss: 0.2359\n",
            "Epoch [7/10], Batch [500], Loss: 0.4960\n",
            "Epoch [7/10], Batch [505], Loss: 0.2575\n",
            "Epoch [7/10], Batch [510], Loss: 0.2269\n",
            "Epoch [7/10], Batch [515], Loss: 0.3221\n",
            "Epoch [7/10], Batch [520], Loss: 0.3761\n",
            "Epoch [7/10], Batch [525], Loss: 0.3470\n",
            "Epoch [7/10], Batch [530], Loss: 0.2399\n",
            "Epoch [7/10], Batch [535], Loss: 0.2807\n",
            "Epoch [7/10], Batch [540], Loss: 0.3336\n",
            "Epoch [7/10], Batch [545], Loss: 0.4124\n",
            "Epoch [7/10], Batch [550], Loss: 0.2540\n",
            "Epoch [7/10], Batch [555], Loss: 0.3179\n",
            "Epoch [7/10], Batch [560], Loss: 0.3118\n",
            "Epoch [7/10], Batch [565], Loss: 0.3214\n",
            "Epoch [7/10], Batch [570], Loss: 0.3115\n",
            "Epoch [7/10], Batch [575], Loss: 0.3740\n",
            "Epoch [7/10], Batch [580], Loss: 0.4451\n",
            "Epoch [7/10], Batch [585], Loss: 0.3855\n",
            "Epoch [7/10], Batch [590], Loss: 0.3994\n",
            "Epoch [7/10], Batch [595], Loss: 0.2783\n",
            "Epoch [7/10], Batch [600], Loss: 0.3888\n",
            "Epoch [7/10], Batch [605], Loss: 0.4451\n",
            "Epoch [7/10], Batch [610], Loss: 0.4479\n",
            "Epoch [7/10], Batch [615], Loss: 0.4086\n",
            "Epoch [7/10], Batch [620], Loss: 0.2940\n",
            "Epoch [7/10], Batch [625], Loss: 0.4171\n",
            "Epoch [7/10], Batch [630], Loss: 0.4084\n",
            "Epoch [7/10], Batch [635], Loss: 0.5208\n",
            "Epoch [7/10], Batch [640], Loss: 0.2858\n",
            "Epoch [7/10], Batch [645], Loss: 0.4446\n",
            "Epoch [7/10], Batch [650], Loss: 0.6394\n",
            "Epoch [7/10], Batch [655], Loss: 0.4182\n",
            "Epoch [7/10], Batch [660], Loss: 0.4602\n",
            "Epoch [7/10], Batch [665], Loss: 0.4854\n",
            "Epoch [7/10], Batch [670], Loss: 0.2685\n",
            "Epoch [7/10], Batch [675], Loss: 0.4631\n",
            "Epoch [7/10], Batch [680], Loss: 0.4463\n",
            "Epoch [7/10], Batch [685], Loss: 0.2215\n",
            "Epoch [7/10], Batch [690], Loss: 0.3727\n",
            "Epoch [7/10], Batch [695], Loss: 0.3535\n",
            "Epoch [7/10], Batch [700], Loss: 0.3293\n",
            "Epoch [7/10], Batch [705], Loss: 0.2603\n",
            "Epoch [7/10], Batch [710], Loss: 0.3438\n",
            "Epoch [7/10], Batch [715], Loss: 0.3726\n",
            "Epoch [7/10], Batch [720], Loss: 0.4246\n",
            "Epoch [7/10], Batch [725], Loss: 0.4993\n",
            "Epoch [7/10], Batch [730], Loss: 0.3608\n",
            "Epoch [7/10], Batch [735], Loss: 0.3958\n",
            "Epoch [7/10], Batch [740], Loss: 0.3388\n",
            "Epoch [7/10], Batch [745], Loss: 0.3789\n",
            "Epoch [7/10], Batch [750], Loss: 0.4869\n",
            "Epoch [7/10], Batch [755], Loss: 0.2677\n",
            "Epoch [7/10], Batch [760], Loss: 0.2163\n",
            "Epoch [7/10], Batch [765], Loss: 0.3062\n",
            "Epoch [7/10], Batch [770], Loss: 0.3101\n",
            "Epoch [7/10], Batch [775], Loss: 0.2470\n",
            "Epoch [7/10], Batch [780], Loss: 0.3093\n",
            "Epoch [7/10], Batch [785], Loss: 0.3304\n",
            "Epoch [7/10], Batch [790], Loss: 0.2736\n",
            "Epoch [7/10], Batch [795], Loss: 0.3523\n",
            "Epoch [7/10], Batch [800], Loss: 0.1808\n",
            "Epoch [7/10], Batch [805], Loss: 0.5038\n",
            "Epoch [7/10], Batch [810], Loss: 0.4474\n",
            "Epoch [7/10], Batch [815], Loss: 0.2492\n",
            "Epoch [7/10], Batch [820], Loss: 0.2517\n",
            "Epoch [7/10], Batch [825], Loss: 0.3813\n",
            "Epoch [7/10], Batch [830], Loss: 0.4118\n",
            "Epoch [7/10], Batch [835], Loss: 0.3530\n",
            "Epoch [7/10], Batch [840], Loss: 0.2675\n",
            "Epoch [7/10], Batch [845], Loss: 0.2553\n",
            "Epoch [7/10], Batch [850], Loss: 0.3212\n",
            "Epoch [7/10], Batch [855], Loss: 0.4891\n",
            "Epoch [7/10], Batch [860], Loss: 0.4840\n",
            "Epoch [7/10], Batch [865], Loss: 0.3526\n",
            "Epoch [7/10], Batch [870], Loss: 0.3534\n",
            "Epoch [7/10], Batch [875], Loss: 0.3367\n",
            "Epoch [7/10], Batch [880], Loss: 0.2835\n",
            "Epoch [7/10], Batch [885], Loss: 0.3517\n",
            "Epoch [7/10], Batch [890], Loss: 0.4306\n",
            "Epoch [7/10], Batch [895], Loss: 0.2974\n",
            "Epoch [7/10], Batch [900], Loss: 0.3701\n",
            "Epoch [7/10], Batch [905], Loss: 0.3413\n",
            "Epoch [7/10], Batch [910], Loss: 0.3822\n",
            "Epoch [7/10], Batch [915], Loss: 0.4623\n",
            "Epoch [7/10], Batch [920], Loss: 0.3962\n",
            "Epoch [7/10], Batch [925], Loss: 0.3761\n",
            "Epoch [7/10], Batch [930], Loss: 0.2261\n",
            "Epoch [7/10], Batch [935], Loss: 0.5066\n",
            "Epoch [7/10], Batch [940], Loss: 0.3447\n",
            "Epoch [7/10], Batch [945], Loss: 0.2117\n",
            "Epoch [7/10], Batch [950], Loss: 0.3113\n",
            "Epoch [7/10], Batch [955], Loss: 0.2326\n",
            "Epoch [7/10], Batch [960], Loss: 0.3120\n",
            "Epoch [7/10], Batch [965], Loss: 0.4055\n",
            "Epoch [7/10], Batch [970], Loss: 0.1834\n",
            "Epoch [7/10], Batch [975], Loss: 0.2851\n",
            "Epoch [7/10], Batch [980], Loss: 0.3124\n",
            "Epoch [7/10], Batch [985], Loss: 0.4547\n",
            "Epoch [7/10], Batch [990], Loss: 0.3512\n",
            "Epoch [7/10], Batch [995], Loss: 0.3106\n",
            "Epoch [7/10], Batch [1000], Loss: 0.3088\n",
            "Epoch [7/10], Batch [1005], Loss: 0.4019\n",
            "Epoch [7/10], Batch [1010], Loss: 0.3727\n",
            "Epoch [7/10], Batch [1015], Loss: 0.3668\n",
            "Epoch [7/10], Batch [1020], Loss: 0.3010\n",
            "Epoch [7/10], Batch [1025], Loss: 0.4858\n",
            "Epoch [7/10], Batch [1030], Loss: 0.3141\n",
            "Epoch [7/10], Batch [1035], Loss: 0.2892\n",
            "Epoch [7/10], Batch [1040], Loss: 0.3743\n",
            "Epoch [7/10], Batch [1045], Loss: 0.2926\n",
            "Epoch [7/10], Batch [1050], Loss: 0.2806\n",
            "Epoch [7/10], Batch [1055], Loss: 0.4464\n",
            "Epoch [7/10], Batch [1060], Loss: 0.3483\n",
            "Epoch [7/10], Batch [1065], Loss: 0.2210\n",
            "Epoch [7/10], Batch [1070], Loss: 0.2803\n",
            "Epoch [7/10], Batch [1075], Loss: 0.3126\n",
            "Epoch [7/10], Batch [1080], Loss: 0.3740\n",
            "Epoch [7/10], Batch [1085], Loss: 0.3119\n",
            "Epoch [7/10], Batch [1090], Loss: 0.4176\n",
            "Epoch [7/10], Batch [1095], Loss: 0.5128\n",
            "Epoch [7/10], Batch [1100], Loss: 0.4355\n",
            "Epoch [7/10], Batch [1105], Loss: 0.3694\n",
            "Epoch [7/10], Batch [1110], Loss: 0.2906\n",
            "Epoch [7/10], Batch [1115], Loss: 0.2926\n",
            "Epoch [7/10], Batch [1120], Loss: 0.2936\n",
            "Epoch [7/10], Batch [1125], Loss: 0.4523\n",
            "Epoch [7/10], Batch [1130], Loss: 0.2514\n",
            "Epoch [7/10], Batch [1135], Loss: 0.4710\n",
            "Epoch [7/10], Batch [1140], Loss: 0.2634\n",
            "Epoch [7/10], Batch [1145], Loss: 0.3392\n",
            "Epoch [7/10], Batch [1150], Loss: 0.4990\n",
            "Epoch [7/10], Batch [1155], Loss: 0.2273\n",
            "Epoch [7/10], Batch [1160], Loss: 0.2954\n",
            "Epoch [7/10], Batch [1165], Loss: 0.3176\n",
            "Epoch [7/10], Batch [1170], Loss: 0.2162\n",
            "Epoch [7/10], Batch [1175], Loss: 0.2158\n",
            "Epoch [7/10], Batch [1180], Loss: 0.3626\n",
            "Epoch [7/10], Batch [1185], Loss: 0.3566\n",
            "Epoch [7/10], Batch [1190], Loss: 0.5329\n",
            "Epoch [7/10], Batch [1195], Loss: 0.4120\n",
            "Epoch [7/10], Batch [1200], Loss: 0.2868\n",
            "Epoch [7/10], Batch [1205], Loss: 0.3942\n",
            "Epoch [7/10], Batch [1210], Loss: 0.3542\n",
            "Epoch [7/10], Batch [1215], Loss: 0.3447\n",
            "Epoch [7/10], Batch [1220], Loss: 0.3899\n",
            "Epoch [7/10], Batch [1225], Loss: 0.4022\n",
            "Epoch [7/10], Batch [1230], Loss: 0.4280\n",
            "Epoch [7/10], Batch [1235], Loss: 0.3255\n",
            "Epoch [7/10], Batch [1240], Loss: 0.3972\n",
            "Epoch [7/10], Batch [1245], Loss: 0.3118\n",
            "Epoch [7/10], Batch [1250], Loss: 0.3436\n",
            "Epoch [7/10], Batch [1255], Loss: 0.4892\n",
            "Epoch [7/10], Batch [1260], Loss: 0.3063\n",
            "Epoch [7/10], Batch [1265], Loss: 0.3327\n",
            "Epoch [7/10], Batch [1270], Loss: 0.2726\n",
            "Epoch [7/10], Batch [1275], Loss: 0.2967\n",
            "Epoch [7/10], Batch [1280], Loss: 0.3348\n",
            "Epoch [7/10], Batch [1285], Loss: 0.2103\n",
            "Epoch [7/10], Batch [1290], Loss: 0.2745\n",
            "Epoch [7/10], Batch [1295], Loss: 0.3740\n",
            "Epoch [7/10], Batch [1300], Loss: 0.3298\n",
            "Epoch [7/10], Batch [1305], Loss: 0.2341\n",
            "Epoch [7/10], Batch [1310], Loss: 0.2348\n",
            "Epoch [7/10], Batch [1315], Loss: 0.4670\n",
            "Epoch [7/10], Batch [1320], Loss: 0.3215\n",
            "Epoch [7/10], Batch [1325], Loss: 0.3590\n",
            "Epoch [7/10], Batch [1330], Loss: 0.4119\n",
            "Epoch [7/10], Batch [1335], Loss: 0.3210\n",
            "Epoch [7/10], Batch [1340], Loss: 0.3217\n",
            "Epoch [7/10], Batch [1345], Loss: 0.3404\n",
            "Epoch [7/10], Batch [1350], Loss: 0.4346\n",
            "Epoch [7/10], Batch [1355], Loss: 0.5805\n",
            "Epoch [7/10], Batch [1360], Loss: 0.3371\n",
            "Epoch [7/10], Batch [1365], Loss: 0.3565\n",
            "Epoch [7/10], Batch [1370], Loss: 0.3858\n",
            "Epoch [7/10], Batch [1375], Loss: 0.3662\n",
            "Epoch [7/10], Batch [1380], Loss: 0.4397\n",
            "Epoch [7/10], Batch [1385], Loss: 0.4595\n",
            "Epoch [7/10], Batch [1390], Loss: 0.6383\n",
            "Epoch [7/10], Batch [1395], Loss: 0.2890\n",
            "Epoch [7/10], Batch [1400], Loss: 0.4175\n",
            "Epoch [7/10], Batch [1405], Loss: 0.2493\n",
            "Epoch [7/10], Batch [1410], Loss: 0.2600\n",
            "Epoch [7/10], Batch [1415], Loss: 0.2959\n",
            "Epoch [7/10], Batch [1420], Loss: 0.2361\n",
            "Epoch [7/10], Batch [1425], Loss: 0.2879\n",
            "Epoch [7/10], Batch [1430], Loss: 0.5489\n",
            "Epoch [7/10], Batch [1435], Loss: 0.5318\n",
            "Epoch [7/10], Batch [1440], Loss: 0.3237\n",
            "Epoch [7/10], Batch [1445], Loss: 0.4048\n",
            "Epoch [7/10], Batch [1450], Loss: 0.3890\n",
            "Epoch [7/10], Batch [1455], Loss: 0.2934\n",
            "Epoch [7/10], Batch [1460], Loss: 0.1984\n",
            "Epoch [7/10], Batch [1465], Loss: 0.3561\n",
            "Epoch [7/10], Batch [1470], Loss: 0.3496\n",
            "Epoch [7/10], Batch [1475], Loss: 0.2074\n",
            "Epoch [7/10], Batch [1480], Loss: 0.3642\n",
            "Epoch [7/10], Batch [1485], Loss: 0.4113\n",
            "Epoch [7/10], Batch [1490], Loss: 0.4360\n",
            "Epoch [7/10], Batch [1495], Loss: 0.4091\n",
            "Epoch [7/10], Batch [1500], Loss: 0.1787\n",
            "Epoch [7/10], Batch [1505], Loss: 0.3874\n",
            "Epoch [7/10], Batch [1510], Loss: 0.2823\n",
            "Epoch [7/10], Batch [1515], Loss: 0.2711\n",
            "Epoch [7/10], Batch [1520], Loss: 0.2961\n",
            "Epoch [7/10], Batch [1525], Loss: 0.3394\n",
            "Epoch [7/10], Batch [1530], Loss: 0.3833\n",
            "Epoch [7/10], Batch [1535], Loss: 0.3887\n",
            "Epoch [7/10], Batch [1540], Loss: 0.3871\n",
            "Epoch [7/10], Batch [1545], Loss: 0.3542\n",
            "Epoch [7/10], Batch [1550], Loss: 0.3322\n",
            "Epoch [7/10], Batch [1555], Loss: 0.2755\n",
            "Epoch [7/10], Batch [1560], Loss: 0.2967\n",
            "Epoch [7/10], Batch [1565], Loss: 0.3967\n",
            "Epoch [7/10], Batch [1570], Loss: 0.4261\n",
            "Epoch [7/10], Batch [1575], Loss: 0.4218\n",
            "Epoch [7/10], Batch [1580], Loss: 0.3323\n",
            "Epoch [7/10], Batch [1585], Loss: 0.2938\n",
            "Epoch [7/10], Batch [1590], Loss: 0.4212\n",
            "Epoch [7/10], Batch [1595], Loss: 0.3117\n",
            "Epoch [7/10], Batch [1600], Loss: 0.3397\n",
            "Epoch [7/10], Batch [1605], Loss: 0.4822\n",
            "Epoch [7/10], Batch [1610], Loss: 0.2711\n",
            "Epoch [7/10], Batch [1615], Loss: 0.4162\n",
            "Epoch [7/10], Batch [1620], Loss: 0.3560\n",
            "Epoch [7/10], Batch [1625], Loss: 0.3003\n",
            "Epoch [7/10], Batch [1630], Loss: 0.2451\n",
            "Epoch [7/10], Batch [1635], Loss: 0.2551\n",
            "Epoch [7/10], Batch [1640], Loss: 0.3801\n",
            "Epoch [7/10], Batch [1645], Loss: 0.3487\n",
            "Epoch [7/10], Batch [1650], Loss: 0.2866\n",
            "Epoch [7/10], Batch [1655], Loss: 0.3877\n",
            "Epoch [7/10], Batch [1660], Loss: 0.2268\n",
            "Epoch [7/10], Batch [1665], Loss: 0.2515\n",
            "Epoch [7/10], Batch [1670], Loss: 0.2883\n",
            "Epoch [7/10], Batch [1675], Loss: 0.5009\n",
            "Epoch [7/10], Batch [1680], Loss: 0.4666\n",
            "Epoch [7/10], Batch [1685], Loss: 0.4318\n",
            "Epoch [7/10], Batch [1690], Loss: 0.3851\n",
            "Epoch [7/10], Batch [1695], Loss: 0.2502\n",
            "Epoch [7/10], Batch [1700], Loss: 0.3236\n",
            "Epoch [7/10], Batch [1705], Loss: 0.2678\n",
            "Epoch [7/10], Batch [1710], Loss: 0.4500\n",
            "Epoch [7/10], Batch [1715], Loss: 0.3958\n",
            "Epoch [7/10], Batch [1720], Loss: 0.3397\n",
            "Epoch [7/10], Batch [1725], Loss: 0.4484\n",
            "Epoch [7/10], Batch [1730], Loss: 0.2974\n",
            "Epoch [7/10], Batch [1735], Loss: 0.2699\n",
            "Epoch [7/10], Batch [1740], Loss: 0.2628\n",
            "Epoch [7/10], Batch [1745], Loss: 0.3045\n",
            "Epoch [7/10], Batch [1750], Loss: 0.5056\n",
            "Epoch [7/10], Batch [1755], Loss: 0.2849\n",
            "Epoch [7/10], Batch [1760], Loss: 0.1960\n",
            "Epoch [7/10], Batch [1765], Loss: 0.3415\n",
            "Epoch [7/10], Batch [1770], Loss: 0.2874\n",
            "Epoch [7/10], Batch [1775], Loss: 0.3464\n",
            "Epoch [7/10], Batch [1780], Loss: 0.4090\n",
            "Epoch [7/10], Batch [1785], Loss: 0.3648\n",
            "Epoch [7/10], Batch [1790], Loss: 0.3693\n",
            "Epoch [7/10], Batch [1795], Loss: 0.3570\n",
            "Epoch [7/10], Batch [1800], Loss: 0.3004\n",
            "Epoch [7/10], Batch [1805], Loss: 0.4115\n",
            "Epoch [7/10], Batch [1810], Loss: 0.2596\n",
            "Epoch [7/10], Batch [1815], Loss: 0.3217\n",
            "Epoch [7/10], Batch [1820], Loss: 0.3423\n",
            "Epoch [7/10], Batch [1825], Loss: 0.3464\n",
            "Epoch [7/10], Batch [1830], Loss: 0.3114\n",
            "Epoch [7/10], Batch [1835], Loss: 0.2838\n",
            "Epoch [7/10], Batch [1840], Loss: 0.2907\n",
            "Epoch [7/10], Batch [1845], Loss: 0.2552\n",
            "Epoch [7/10], Batch [1850], Loss: 0.2261\n",
            "Epoch [7/10], Batch [1855], Loss: 0.2082\n",
            "Epoch [7/10], Batch [1860], Loss: 0.2626\n",
            "Epoch [7/10], Batch [1865], Loss: 0.3735\n",
            "Epoch [7/10], Batch [1870], Loss: 0.2847\n",
            "Epoch [7/10], Batch [1875], Loss: 0.3197\n",
            "Epoch [7/10], Batch [1880], Loss: 0.4861\n",
            "Epoch [7/10], Batch [1885], Loss: 0.3797\n",
            "Epoch [7/10], Batch [1890], Loss: 0.3662\n",
            "Epoch [7/10], Batch [1895], Loss: 0.3605\n",
            "Epoch [7/10], Batch [1900], Loss: 0.4228\n",
            "Epoch [7/10], Batch [1905], Loss: 0.2484\n",
            "Epoch [7/10], Batch [1910], Loss: 0.3464\n",
            "Epoch [7/10], Batch [1915], Loss: 0.2841\n",
            "Epoch [7/10], Batch [1920], Loss: 0.3699\n",
            "Epoch [7/10], Batch [1925], Loss: 0.3206\n",
            "Epoch [7/10], Batch [1930], Loss: 0.2963\n",
            "Epoch [7/10], Batch [1935], Loss: 0.2848\n",
            "Epoch [7/10], Batch [1940], Loss: 0.3542\n",
            "Epoch [7/10], Batch [1945], Loss: 0.2993\n",
            "Epoch [7/10], Batch [1950], Loss: 0.4014\n",
            "Epoch [7/10], Batch [1955], Loss: 0.2720\n",
            "Epoch [7/10], Batch [1960], Loss: 0.4311\n",
            "Epoch [7/10], Batch [1965], Loss: 0.4598\n",
            "Epoch [7/10], Batch [1970], Loss: 0.5625\n",
            "Epoch [7/10], Batch [1975], Loss: 0.6086\n",
            "Epoch [7/10], Batch [1980], Loss: 0.3063\n",
            "Epoch [7/10], Batch [1985], Loss: 0.3269\n",
            "Epoch [7/10], Batch [1990], Loss: 0.3193\n",
            "Epoch [7/10], Batch [1995], Loss: 0.2357\n",
            "Epoch [7/10], Batch [2000], Loss: 0.2317\n",
            "Epoch [7/10], Batch [2005], Loss: 0.2927\n",
            "Epoch [7/10], Batch [2010], Loss: 0.4103\n",
            "Epoch [7/10], Batch [2015], Loss: 0.4937\n",
            "Epoch [7/10], Batch [2020], Loss: 0.3743\n",
            "Epoch [7/10], Batch [2025], Loss: 0.2973\n",
            "Epoch [7/10], Batch [2030], Loss: 0.3239\n",
            "Epoch [7/10], Batch [2035], Loss: 0.2726\n",
            "Epoch [7/10], Batch [2040], Loss: 0.2851\n",
            "Epoch [7/10], Batch [2045], Loss: 0.2534\n",
            "Epoch [7/10], Batch [2050], Loss: 0.3839\n",
            "Epoch [7/10], Batch [2055], Loss: 0.3235\n",
            "Epoch [7/10], Batch [2060], Loss: 0.4075\n",
            "Epoch [7/10], Average Loss: 0.3567\n",
            "Epoch [8/10], Batch [0], Loss: 0.3439\n",
            "Epoch [8/10], Batch [5], Loss: 0.2986\n",
            "Epoch [8/10], Batch [10], Loss: 0.3255\n",
            "Epoch [8/10], Batch [15], Loss: 0.3786\n",
            "Epoch [8/10], Batch [20], Loss: 0.3464\n",
            "Epoch [8/10], Batch [25], Loss: 0.4612\n",
            "Epoch [8/10], Batch [30], Loss: 0.4207\n",
            "Epoch [8/10], Batch [35], Loss: 0.4949\n",
            "Epoch [8/10], Batch [40], Loss: 0.2653\n",
            "Epoch [8/10], Batch [45], Loss: 0.2922\n",
            "Epoch [8/10], Batch [50], Loss: 0.4037\n",
            "Epoch [8/10], Batch [55], Loss: 0.3826\n",
            "Epoch [8/10], Batch [60], Loss: 0.3750\n",
            "Epoch [8/10], Batch [65], Loss: 0.2720\n",
            "Epoch [8/10], Batch [70], Loss: 0.4240\n",
            "Epoch [8/10], Batch [75], Loss: 0.4100\n",
            "Epoch [8/10], Batch [80], Loss: 0.3188\n",
            "Epoch [8/10], Batch [85], Loss: 0.4291\n",
            "Epoch [8/10], Batch [90], Loss: 0.4184\n",
            "Epoch [8/10], Batch [95], Loss: 0.5887\n",
            "Epoch [8/10], Batch [100], Loss: 0.2549\n",
            "Epoch [8/10], Batch [105], Loss: 0.3316\n",
            "Epoch [8/10], Batch [110], Loss: 0.3581\n",
            "Epoch [8/10], Batch [115], Loss: 0.4382\n",
            "Epoch [8/10], Batch [120], Loss: 0.3041\n",
            "Epoch [8/10], Batch [125], Loss: 0.3994\n",
            "Epoch [8/10], Batch [130], Loss: 0.3255\n",
            "Epoch [8/10], Batch [135], Loss: 0.4700\n",
            "Epoch [8/10], Batch [140], Loss: 0.3273\n",
            "Epoch [8/10], Batch [145], Loss: 0.3353\n",
            "Epoch [8/10], Batch [150], Loss: 0.3442\n",
            "Epoch [8/10], Batch [155], Loss: 0.3440\n",
            "Epoch [8/10], Batch [160], Loss: 0.3278\n",
            "Epoch [8/10], Batch [165], Loss: 0.2649\n",
            "Epoch [8/10], Batch [170], Loss: 0.2062\n",
            "Epoch [8/10], Batch [175], Loss: 0.3571\n",
            "Epoch [8/10], Batch [180], Loss: 0.3048\n",
            "Epoch [8/10], Batch [185], Loss: 0.3910\n",
            "Epoch [8/10], Batch [190], Loss: 0.2419\n",
            "Epoch [8/10], Batch [195], Loss: 0.5057\n",
            "Epoch [8/10], Batch [200], Loss: 0.4966\n",
            "Epoch [8/10], Batch [205], Loss: 0.3857\n",
            "Epoch [8/10], Batch [210], Loss: 0.2232\n",
            "Epoch [8/10], Batch [215], Loss: 0.4177\n",
            "Epoch [8/10], Batch [220], Loss: 0.4575\n",
            "Epoch [8/10], Batch [225], Loss: 0.5645\n",
            "Epoch [8/10], Batch [230], Loss: 0.3577\n",
            "Epoch [8/10], Batch [235], Loss: 0.3484\n",
            "Epoch [8/10], Batch [240], Loss: 0.3364\n",
            "Epoch [8/10], Batch [245], Loss: 0.2280\n",
            "Epoch [8/10], Batch [250], Loss: 0.2983\n",
            "Epoch [8/10], Batch [255], Loss: 0.3314\n",
            "Epoch [8/10], Batch [260], Loss: 0.2935\n",
            "Epoch [8/10], Batch [265], Loss: 0.2471\n",
            "Epoch [8/10], Batch [270], Loss: 0.3148\n",
            "Epoch [8/10], Batch [275], Loss: 0.3808\n",
            "Epoch [8/10], Batch [280], Loss: 0.3017\n",
            "Epoch [8/10], Batch [285], Loss: 0.4602\n",
            "Epoch [8/10], Batch [290], Loss: 0.2893\n",
            "Epoch [8/10], Batch [295], Loss: 0.2645\n",
            "Epoch [8/10], Batch [300], Loss: 0.2200\n",
            "Epoch [8/10], Batch [305], Loss: 0.5087\n",
            "Epoch [8/10], Batch [310], Loss: 0.1987\n",
            "Epoch [8/10], Batch [315], Loss: 0.2518\n",
            "Epoch [8/10], Batch [320], Loss: 0.2559\n",
            "Epoch [8/10], Batch [325], Loss: 0.1554\n",
            "Epoch [8/10], Batch [330], Loss: 0.3165\n",
            "Epoch [8/10], Batch [335], Loss: 0.2684\n",
            "Epoch [8/10], Batch [340], Loss: 0.4175\n",
            "Epoch [8/10], Batch [345], Loss: 0.3180\n",
            "Epoch [8/10], Batch [350], Loss: 0.3504\n",
            "Epoch [8/10], Batch [355], Loss: 0.3591\n",
            "Epoch [8/10], Batch [360], Loss: 0.2660\n",
            "Epoch [8/10], Batch [365], Loss: 0.2253\n",
            "Epoch [8/10], Batch [370], Loss: 0.4048\n",
            "Epoch [8/10], Batch [375], Loss: 0.2566\n",
            "Epoch [8/10], Batch [380], Loss: 0.2530\n",
            "Epoch [8/10], Batch [385], Loss: 0.3672\n",
            "Epoch [8/10], Batch [390], Loss: 0.4472\n",
            "Epoch [8/10], Batch [395], Loss: 0.2835\n",
            "Epoch [8/10], Batch [400], Loss: 0.2404\n",
            "Epoch [8/10], Batch [405], Loss: 0.1851\n",
            "Epoch [8/10], Batch [410], Loss: 0.2483\n",
            "Epoch [8/10], Batch [415], Loss: 0.3967\n",
            "Epoch [8/10], Batch [420], Loss: 0.2951\n",
            "Epoch [8/10], Batch [425], Loss: 0.4262\n",
            "Epoch [8/10], Batch [430], Loss: 0.2065\n",
            "Epoch [8/10], Batch [435], Loss: 0.3909\n",
            "Epoch [8/10], Batch [440], Loss: 0.5479\n",
            "Epoch [8/10], Batch [445], Loss: 0.5938\n",
            "Epoch [8/10], Batch [450], Loss: 0.3741\n",
            "Epoch [8/10], Batch [455], Loss: 0.3179\n",
            "Epoch [8/10], Batch [460], Loss: 0.2419\n",
            "Epoch [8/10], Batch [465], Loss: 0.2835\n",
            "Epoch [8/10], Batch [470], Loss: 0.3316\n",
            "Epoch [8/10], Batch [475], Loss: 0.4476\n",
            "Epoch [8/10], Batch [480], Loss: 0.2610\n",
            "Epoch [8/10], Batch [485], Loss: 0.4760\n",
            "Epoch [8/10], Batch [490], Loss: 0.2778\n",
            "Epoch [8/10], Batch [495], Loss: 0.2264\n",
            "Epoch [8/10], Batch [500], Loss: 0.4849\n",
            "Epoch [8/10], Batch [505], Loss: 0.2470\n",
            "Epoch [8/10], Batch [510], Loss: 0.2179\n",
            "Epoch [8/10], Batch [515], Loss: 0.3023\n",
            "Epoch [8/10], Batch [520], Loss: 0.3680\n",
            "Epoch [8/10], Batch [525], Loss: 0.3192\n",
            "Epoch [8/10], Batch [530], Loss: 0.2341\n",
            "Epoch [8/10], Batch [535], Loss: 0.2674\n",
            "Epoch [8/10], Batch [540], Loss: 0.3161\n",
            "Epoch [8/10], Batch [545], Loss: 0.3882\n",
            "Epoch [8/10], Batch [550], Loss: 0.2435\n",
            "Epoch [8/10], Batch [555], Loss: 0.3036\n",
            "Epoch [8/10], Batch [560], Loss: 0.2915\n",
            "Epoch [8/10], Batch [565], Loss: 0.3050\n",
            "Epoch [8/10], Batch [570], Loss: 0.2936\n",
            "Epoch [8/10], Batch [575], Loss: 0.3457\n",
            "Epoch [8/10], Batch [580], Loss: 0.4408\n",
            "Epoch [8/10], Batch [585], Loss: 0.3738\n",
            "Epoch [8/10], Batch [590], Loss: 0.3823\n",
            "Epoch [8/10], Batch [595], Loss: 0.2590\n",
            "Epoch [8/10], Batch [600], Loss: 0.3607\n",
            "Epoch [8/10], Batch [605], Loss: 0.4081\n",
            "Epoch [8/10], Batch [610], Loss: 0.4388\n",
            "Epoch [8/10], Batch [615], Loss: 0.3679\n",
            "Epoch [8/10], Batch [620], Loss: 0.2869\n",
            "Epoch [8/10], Batch [625], Loss: 0.3885\n",
            "Epoch [8/10], Batch [630], Loss: 0.4116\n",
            "Epoch [8/10], Batch [635], Loss: 0.5152\n",
            "Epoch [8/10], Batch [640], Loss: 0.2728\n",
            "Epoch [8/10], Batch [645], Loss: 0.4466\n",
            "Epoch [8/10], Batch [650], Loss: 0.6232\n",
            "Epoch [8/10], Batch [655], Loss: 0.3920\n",
            "Epoch [8/10], Batch [660], Loss: 0.4573\n",
            "Epoch [8/10], Batch [665], Loss: 0.4738\n",
            "Epoch [8/10], Batch [670], Loss: 0.2734\n",
            "Epoch [8/10], Batch [675], Loss: 0.4607\n",
            "Epoch [8/10], Batch [680], Loss: 0.4371\n",
            "Epoch [8/10], Batch [685], Loss: 0.2105\n",
            "Epoch [8/10], Batch [690], Loss: 0.3427\n",
            "Epoch [8/10], Batch [695], Loss: 0.3261\n",
            "Epoch [8/10], Batch [700], Loss: 0.3273\n",
            "Epoch [8/10], Batch [705], Loss: 0.2400\n",
            "Epoch [8/10], Batch [710], Loss: 0.3365\n",
            "Epoch [8/10], Batch [715], Loss: 0.3683\n",
            "Epoch [8/10], Batch [720], Loss: 0.3917\n",
            "Epoch [8/10], Batch [725], Loss: 0.4727\n",
            "Epoch [8/10], Batch [730], Loss: 0.3350\n",
            "Epoch [8/10], Batch [735], Loss: 0.3800\n",
            "Epoch [8/10], Batch [740], Loss: 0.3315\n",
            "Epoch [8/10], Batch [745], Loss: 0.3698\n",
            "Epoch [8/10], Batch [750], Loss: 0.4772\n",
            "Epoch [8/10], Batch [755], Loss: 0.2673\n",
            "Epoch [8/10], Batch [760], Loss: 0.1917\n",
            "Epoch [8/10], Batch [765], Loss: 0.2989\n",
            "Epoch [8/10], Batch [770], Loss: 0.2821\n",
            "Epoch [8/10], Batch [775], Loss: 0.2507\n",
            "Epoch [8/10], Batch [780], Loss: 0.3081\n",
            "Epoch [8/10], Batch [785], Loss: 0.3321\n",
            "Epoch [8/10], Batch [790], Loss: 0.2787\n",
            "Epoch [8/10], Batch [795], Loss: 0.3438\n",
            "Epoch [8/10], Batch [800], Loss: 0.1624\n",
            "Epoch [8/10], Batch [805], Loss: 0.4676\n",
            "Epoch [8/10], Batch [810], Loss: 0.4573\n",
            "Epoch [8/10], Batch [815], Loss: 0.2547\n",
            "Epoch [8/10], Batch [820], Loss: 0.2401\n",
            "Epoch [8/10], Batch [825], Loss: 0.3541\n",
            "Epoch [8/10], Batch [830], Loss: 0.3830\n",
            "Epoch [8/10], Batch [835], Loss: 0.3405\n",
            "Epoch [8/10], Batch [840], Loss: 0.2641\n",
            "Epoch [8/10], Batch [845], Loss: 0.2360\n",
            "Epoch [8/10], Batch [850], Loss: 0.3083\n",
            "Epoch [8/10], Batch [855], Loss: 0.4948\n",
            "Epoch [8/10], Batch [860], Loss: 0.4403\n",
            "Epoch [8/10], Batch [865], Loss: 0.3354\n",
            "Epoch [8/10], Batch [870], Loss: 0.3351\n",
            "Epoch [8/10], Batch [875], Loss: 0.3250\n",
            "Epoch [8/10], Batch [880], Loss: 0.2757\n",
            "Epoch [8/10], Batch [885], Loss: 0.3468\n",
            "Epoch [8/10], Batch [890], Loss: 0.4155\n",
            "Epoch [8/10], Batch [895], Loss: 0.3095\n",
            "Epoch [8/10], Batch [900], Loss: 0.3687\n",
            "Epoch [8/10], Batch [905], Loss: 0.3387\n",
            "Epoch [8/10], Batch [910], Loss: 0.3624\n",
            "Epoch [8/10], Batch [915], Loss: 0.4552\n",
            "Epoch [8/10], Batch [920], Loss: 0.3775\n",
            "Epoch [8/10], Batch [925], Loss: 0.3657\n",
            "Epoch [8/10], Batch [930], Loss: 0.2213\n",
            "Epoch [8/10], Batch [935], Loss: 0.4800\n",
            "Epoch [8/10], Batch [940], Loss: 0.3250\n",
            "Epoch [8/10], Batch [945], Loss: 0.1920\n",
            "Epoch [8/10], Batch [950], Loss: 0.3003\n",
            "Epoch [8/10], Batch [955], Loss: 0.2251\n",
            "Epoch [8/10], Batch [960], Loss: 0.2946\n",
            "Epoch [8/10], Batch [965], Loss: 0.3832\n",
            "Epoch [8/10], Batch [970], Loss: 0.1771\n",
            "Epoch [8/10], Batch [975], Loss: 0.2820\n",
            "Epoch [8/10], Batch [980], Loss: 0.3126\n",
            "Epoch [8/10], Batch [985], Loss: 0.4694\n",
            "Epoch [8/10], Batch [990], Loss: 0.3814\n",
            "Epoch [8/10], Batch [995], Loss: 0.3000\n",
            "Epoch [8/10], Batch [1000], Loss: 0.2777\n",
            "Epoch [8/10], Batch [1005], Loss: 0.3447\n",
            "Epoch [8/10], Batch [1010], Loss: 0.3456\n",
            "Epoch [8/10], Batch [1015], Loss: 0.3428\n",
            "Epoch [8/10], Batch [1020], Loss: 0.3160\n",
            "Epoch [8/10], Batch [1025], Loss: 0.4582\n",
            "Epoch [8/10], Batch [1030], Loss: 0.2657\n",
            "Epoch [8/10], Batch [1035], Loss: 0.3076\n",
            "Epoch [8/10], Batch [1040], Loss: 0.3728\n",
            "Epoch [8/10], Batch [1045], Loss: 0.2666\n",
            "Epoch [8/10], Batch [1050], Loss: 0.2622\n",
            "Epoch [8/10], Batch [1055], Loss: 0.4239\n",
            "Epoch [8/10], Batch [1060], Loss: 0.3454\n",
            "Epoch [8/10], Batch [1065], Loss: 0.2089\n",
            "Epoch [8/10], Batch [1070], Loss: 0.2701\n",
            "Epoch [8/10], Batch [1075], Loss: 0.2908\n",
            "Epoch [8/10], Batch [1080], Loss: 0.3573\n",
            "Epoch [8/10], Batch [1085], Loss: 0.2927\n",
            "Epoch [8/10], Batch [1090], Loss: 0.3873\n",
            "Epoch [8/10], Batch [1095], Loss: 0.4998\n",
            "Epoch [8/10], Batch [1100], Loss: 0.4034\n",
            "Epoch [8/10], Batch [1105], Loss: 0.3664\n",
            "Epoch [8/10], Batch [1110], Loss: 0.2757\n",
            "Epoch [8/10], Batch [1115], Loss: 0.2938\n",
            "Epoch [8/10], Batch [1120], Loss: 0.2973\n",
            "Epoch [8/10], Batch [1125], Loss: 0.4439\n",
            "Epoch [8/10], Batch [1130], Loss: 0.2214\n",
            "Epoch [8/10], Batch [1135], Loss: 0.4683\n",
            "Epoch [8/10], Batch [1140], Loss: 0.2513\n",
            "Epoch [8/10], Batch [1145], Loss: 0.3393\n",
            "Epoch [8/10], Batch [1150], Loss: 0.5094\n",
            "Epoch [8/10], Batch [1155], Loss: 0.2159\n",
            "Epoch [8/10], Batch [1160], Loss: 0.2900\n",
            "Epoch [8/10], Batch [1165], Loss: 0.2960\n",
            "Epoch [8/10], Batch [1170], Loss: 0.2124\n",
            "Epoch [8/10], Batch [1175], Loss: 0.2051\n",
            "Epoch [8/10], Batch [1180], Loss: 0.3543\n",
            "Epoch [8/10], Batch [1185], Loss: 0.3671\n",
            "Epoch [8/10], Batch [1190], Loss: 0.5320\n",
            "Epoch [8/10], Batch [1195], Loss: 0.3864\n",
            "Epoch [8/10], Batch [1200], Loss: 0.2469\n",
            "Epoch [8/10], Batch [1205], Loss: 0.3977\n",
            "Epoch [8/10], Batch [1210], Loss: 0.3306\n",
            "Epoch [8/10], Batch [1215], Loss: 0.3304\n",
            "Epoch [8/10], Batch [1220], Loss: 0.3761\n",
            "Epoch [8/10], Batch [1225], Loss: 0.3617\n",
            "Epoch [8/10], Batch [1230], Loss: 0.3991\n",
            "Epoch [8/10], Batch [1235], Loss: 0.3157\n",
            "Epoch [8/10], Batch [1240], Loss: 0.3721\n",
            "Epoch [8/10], Batch [1245], Loss: 0.3243\n",
            "Epoch [8/10], Batch [1250], Loss: 0.3338\n",
            "Epoch [8/10], Batch [1255], Loss: 0.4306\n",
            "Epoch [8/10], Batch [1260], Loss: 0.2970\n",
            "Epoch [8/10], Batch [1265], Loss: 0.2931\n",
            "Epoch [8/10], Batch [1270], Loss: 0.2679\n",
            "Epoch [8/10], Batch [1275], Loss: 0.3015\n",
            "Epoch [8/10], Batch [1280], Loss: 0.3167\n",
            "Epoch [8/10], Batch [1285], Loss: 0.2010\n",
            "Epoch [8/10], Batch [1290], Loss: 0.2456\n",
            "Epoch [8/10], Batch [1295], Loss: 0.3644\n",
            "Epoch [8/10], Batch [1300], Loss: 0.3127\n",
            "Epoch [8/10], Batch [1305], Loss: 0.2273\n",
            "Epoch [8/10], Batch [1310], Loss: 0.2259\n",
            "Epoch [8/10], Batch [1315], Loss: 0.4487\n",
            "Epoch [8/10], Batch [1320], Loss: 0.3037\n",
            "Epoch [8/10], Batch [1325], Loss: 0.3410\n",
            "Epoch [8/10], Batch [1330], Loss: 0.4006\n",
            "Epoch [8/10], Batch [1335], Loss: 0.3052\n",
            "Epoch [8/10], Batch [1340], Loss: 0.2974\n",
            "Epoch [8/10], Batch [1345], Loss: 0.3177\n",
            "Epoch [8/10], Batch [1350], Loss: 0.4268\n",
            "Epoch [8/10], Batch [1355], Loss: 0.5519\n",
            "Epoch [8/10], Batch [1360], Loss: 0.3110\n",
            "Epoch [8/10], Batch [1365], Loss: 0.3336\n",
            "Epoch [8/10], Batch [1370], Loss: 0.3773\n",
            "Epoch [8/10], Batch [1375], Loss: 0.3562\n",
            "Epoch [8/10], Batch [1380], Loss: 0.4324\n",
            "Epoch [8/10], Batch [1385], Loss: 0.4405\n",
            "Epoch [8/10], Batch [1390], Loss: 0.6379\n",
            "Epoch [8/10], Batch [1395], Loss: 0.2464\n",
            "Epoch [8/10], Batch [1400], Loss: 0.3930\n",
            "Epoch [8/10], Batch [1405], Loss: 0.2273\n",
            "Epoch [8/10], Batch [1410], Loss: 0.2664\n",
            "Epoch [8/10], Batch [1415], Loss: 0.2509\n",
            "Epoch [8/10], Batch [1420], Loss: 0.2458\n",
            "Epoch [8/10], Batch [1425], Loss: 0.2861\n",
            "Epoch [8/10], Batch [1430], Loss: 0.5447\n",
            "Epoch [8/10], Batch [1435], Loss: 0.4964\n",
            "Epoch [8/10], Batch [1440], Loss: 0.2754\n",
            "Epoch [8/10], Batch [1445], Loss: 0.3568\n",
            "Epoch [8/10], Batch [1450], Loss: 0.3698\n",
            "Epoch [8/10], Batch [1455], Loss: 0.2767\n",
            "Epoch [8/10], Batch [1460], Loss: 0.1888\n",
            "Epoch [8/10], Batch [1465], Loss: 0.3571\n",
            "Epoch [8/10], Batch [1470], Loss: 0.3311\n",
            "Epoch [8/10], Batch [1475], Loss: 0.2107\n",
            "Epoch [8/10], Batch [1480], Loss: 0.3538\n",
            "Epoch [8/10], Batch [1485], Loss: 0.4021\n",
            "Epoch [8/10], Batch [1490], Loss: 0.4523\n",
            "Epoch [8/10], Batch [1495], Loss: 0.3725\n",
            "Epoch [8/10], Batch [1500], Loss: 0.1677\n",
            "Epoch [8/10], Batch [1505], Loss: 0.3755\n",
            "Epoch [8/10], Batch [1510], Loss: 0.2762\n",
            "Epoch [8/10], Batch [1515], Loss: 0.2496\n",
            "Epoch [8/10], Batch [1520], Loss: 0.2983\n",
            "Epoch [8/10], Batch [1525], Loss: 0.3346\n",
            "Epoch [8/10], Batch [1530], Loss: 0.3529\n",
            "Epoch [8/10], Batch [1535], Loss: 0.3594\n",
            "Epoch [8/10], Batch [1540], Loss: 0.3943\n",
            "Epoch [8/10], Batch [1545], Loss: 0.3176\n",
            "Epoch [8/10], Batch [1550], Loss: 0.3124\n",
            "Epoch [8/10], Batch [1555], Loss: 0.2802\n",
            "Epoch [8/10], Batch [1560], Loss: 0.2945\n",
            "Epoch [8/10], Batch [1565], Loss: 0.3803\n",
            "Epoch [8/10], Batch [1570], Loss: 0.4144\n",
            "Epoch [8/10], Batch [1575], Loss: 0.4027\n",
            "Epoch [8/10], Batch [1580], Loss: 0.3108\n",
            "Epoch [8/10], Batch [1585], Loss: 0.3191\n",
            "Epoch [8/10], Batch [1590], Loss: 0.3933\n",
            "Epoch [8/10], Batch [1595], Loss: 0.3187\n",
            "Epoch [8/10], Batch [1600], Loss: 0.3252\n",
            "Epoch [8/10], Batch [1605], Loss: 0.4756\n",
            "Epoch [8/10], Batch [1610], Loss: 0.2698\n",
            "Epoch [8/10], Batch [1615], Loss: 0.3826\n",
            "Epoch [8/10], Batch [1620], Loss: 0.3195\n",
            "Epoch [8/10], Batch [1625], Loss: 0.2721\n",
            "Epoch [8/10], Batch [1630], Loss: 0.2329\n",
            "Epoch [8/10], Batch [1635], Loss: 0.2352\n",
            "Epoch [8/10], Batch [1640], Loss: 0.3550\n",
            "Epoch [8/10], Batch [1645], Loss: 0.3583\n",
            "Epoch [8/10], Batch [1650], Loss: 0.2833\n",
            "Epoch [8/10], Batch [1655], Loss: 0.3799\n",
            "Epoch [8/10], Batch [1660], Loss: 0.2222\n",
            "Epoch [8/10], Batch [1665], Loss: 0.2451\n",
            "Epoch [8/10], Batch [1670], Loss: 0.2868\n",
            "Epoch [8/10], Batch [1675], Loss: 0.4877\n",
            "Epoch [8/10], Batch [1680], Loss: 0.4467\n",
            "Epoch [8/10], Batch [1685], Loss: 0.4243\n",
            "Epoch [8/10], Batch [1690], Loss: 0.3661\n",
            "Epoch [8/10], Batch [1695], Loss: 0.2381\n",
            "Epoch [8/10], Batch [1700], Loss: 0.3209\n",
            "Epoch [8/10], Batch [1705], Loss: 0.2607\n",
            "Epoch [8/10], Batch [1710], Loss: 0.4304\n",
            "Epoch [8/10], Batch [1715], Loss: 0.3804\n",
            "Epoch [8/10], Batch [1720], Loss: 0.3178\n",
            "Epoch [8/10], Batch [1725], Loss: 0.4341\n",
            "Epoch [8/10], Batch [1730], Loss: 0.2982\n",
            "Epoch [8/10], Batch [1735], Loss: 0.2494\n",
            "Epoch [8/10], Batch [1740], Loss: 0.2579\n",
            "Epoch [8/10], Batch [1745], Loss: 0.3017\n",
            "Epoch [8/10], Batch [1750], Loss: 0.4946\n",
            "Epoch [8/10], Batch [1755], Loss: 0.2891\n",
            "Epoch [8/10], Batch [1760], Loss: 0.1740\n",
            "Epoch [8/10], Batch [1765], Loss: 0.3392\n",
            "Epoch [8/10], Batch [1770], Loss: 0.2927\n",
            "Epoch [8/10], Batch [1775], Loss: 0.3206\n",
            "Epoch [8/10], Batch [1780], Loss: 0.3844\n",
            "Epoch [8/10], Batch [1785], Loss: 0.3468\n",
            "Epoch [8/10], Batch [1790], Loss: 0.3684\n",
            "Epoch [8/10], Batch [1795], Loss: 0.3343\n",
            "Epoch [8/10], Batch [1800], Loss: 0.2963\n",
            "Epoch [8/10], Batch [1805], Loss: 0.4084\n",
            "Epoch [8/10], Batch [1810], Loss: 0.2455\n",
            "Epoch [8/10], Batch [1815], Loss: 0.3334\n",
            "Epoch [8/10], Batch [1820], Loss: 0.3318\n",
            "Epoch [8/10], Batch [1825], Loss: 0.3329\n",
            "Epoch [8/10], Batch [1830], Loss: 0.2911\n",
            "Epoch [8/10], Batch [1835], Loss: 0.2540\n",
            "Epoch [8/10], Batch [1840], Loss: 0.2731\n",
            "Epoch [8/10], Batch [1845], Loss: 0.2401\n",
            "Epoch [8/10], Batch [1850], Loss: 0.2069\n",
            "Epoch [8/10], Batch [1855], Loss: 0.2012\n",
            "Epoch [8/10], Batch [1860], Loss: 0.2463\n",
            "Epoch [8/10], Batch [1865], Loss: 0.3750\n",
            "Epoch [8/10], Batch [1870], Loss: 0.2497\n",
            "Epoch [8/10], Batch [1875], Loss: 0.3086\n",
            "Epoch [8/10], Batch [1880], Loss: 0.4455\n",
            "Epoch [8/10], Batch [1885], Loss: 0.4004\n",
            "Epoch [8/10], Batch [1890], Loss: 0.3716\n",
            "Epoch [8/10], Batch [1895], Loss: 0.3617\n",
            "Epoch [8/10], Batch [1900], Loss: 0.3950\n",
            "Epoch [8/10], Batch [1905], Loss: 0.2213\n",
            "Epoch [8/10], Batch [1910], Loss: 0.3496\n",
            "Epoch [8/10], Batch [1915], Loss: 0.2770\n",
            "Epoch [8/10], Batch [1920], Loss: 0.3544\n",
            "Epoch [8/10], Batch [1925], Loss: 0.3191\n",
            "Epoch [8/10], Batch [1930], Loss: 0.2692\n",
            "Epoch [8/10], Batch [1935], Loss: 0.2853\n",
            "Epoch [8/10], Batch [1940], Loss: 0.3590\n",
            "Epoch [8/10], Batch [1945], Loss: 0.2873\n",
            "Epoch [8/10], Batch [1950], Loss: 0.3875\n",
            "Epoch [8/10], Batch [1955], Loss: 0.2636\n",
            "Epoch [8/10], Batch [1960], Loss: 0.4325\n",
            "Epoch [8/10], Batch [1965], Loss: 0.4353\n",
            "Epoch [8/10], Batch [1970], Loss: 0.5342\n",
            "Epoch [8/10], Batch [1975], Loss: 0.5782\n",
            "Epoch [8/10], Batch [1980], Loss: 0.2947\n",
            "Epoch [8/10], Batch [1985], Loss: 0.3246\n",
            "Epoch [8/10], Batch [1990], Loss: 0.3036\n",
            "Epoch [8/10], Batch [1995], Loss: 0.2136\n",
            "Epoch [8/10], Batch [2000], Loss: 0.2339\n",
            "Epoch [8/10], Batch [2005], Loss: 0.2816\n",
            "Epoch [8/10], Batch [2010], Loss: 0.4108\n",
            "Epoch [8/10], Batch [2015], Loss: 0.4914\n",
            "Epoch [8/10], Batch [2020], Loss: 0.3391\n",
            "Epoch [8/10], Batch [2025], Loss: 0.2933\n",
            "Epoch [8/10], Batch [2030], Loss: 0.3288\n",
            "Epoch [8/10], Batch [2035], Loss: 0.2453\n",
            "Epoch [8/10], Batch [2040], Loss: 0.2732\n",
            "Epoch [8/10], Batch [2045], Loss: 0.2369\n",
            "Epoch [8/10], Batch [2050], Loss: 0.3633\n",
            "Epoch [8/10], Batch [2055], Loss: 0.3085\n",
            "Epoch [8/10], Batch [2060], Loss: 0.3888\n",
            "Epoch [8/10], Average Loss: 0.3432\n",
            "Epoch [9/10], Batch [0], Loss: 0.3142\n",
            "Epoch [9/10], Batch [5], Loss: 0.3003\n",
            "Epoch [9/10], Batch [10], Loss: 0.3135\n",
            "Epoch [9/10], Batch [15], Loss: 0.3938\n",
            "Epoch [9/10], Batch [20], Loss: 0.3257\n",
            "Epoch [9/10], Batch [25], Loss: 0.4269\n",
            "Epoch [9/10], Batch [30], Loss: 0.4095\n",
            "Epoch [9/10], Batch [35], Loss: 0.4780\n",
            "Epoch [9/10], Batch [40], Loss: 0.2759\n",
            "Epoch [9/10], Batch [45], Loss: 0.2873\n",
            "Epoch [9/10], Batch [50], Loss: 0.3780\n",
            "Epoch [9/10], Batch [55], Loss: 0.3988\n",
            "Epoch [9/10], Batch [60], Loss: 0.3646\n",
            "Epoch [9/10], Batch [65], Loss: 0.2595\n",
            "Epoch [9/10], Batch [70], Loss: 0.4316\n",
            "Epoch [9/10], Batch [75], Loss: 0.3941\n",
            "Epoch [9/10], Batch [80], Loss: 0.3171\n",
            "Epoch [9/10], Batch [85], Loss: 0.4044\n",
            "Epoch [9/10], Batch [90], Loss: 0.4066\n",
            "Epoch [9/10], Batch [95], Loss: 0.5823\n",
            "Epoch [9/10], Batch [100], Loss: 0.2656\n",
            "Epoch [9/10], Batch [105], Loss: 0.3100\n",
            "Epoch [9/10], Batch [110], Loss: 0.3439\n",
            "Epoch [9/10], Batch [115], Loss: 0.4023\n",
            "Epoch [9/10], Batch [120], Loss: 0.2918\n",
            "Epoch [9/10], Batch [125], Loss: 0.3892\n",
            "Epoch [9/10], Batch [130], Loss: 0.3174\n",
            "Epoch [9/10], Batch [135], Loss: 0.4644\n",
            "Epoch [9/10], Batch [140], Loss: 0.3285\n",
            "Epoch [9/10], Batch [145], Loss: 0.3129\n",
            "Epoch [9/10], Batch [150], Loss: 0.3135\n",
            "Epoch [9/10], Batch [155], Loss: 0.3290\n",
            "Epoch [9/10], Batch [160], Loss: 0.3039\n",
            "Epoch [9/10], Batch [165], Loss: 0.2461\n",
            "Epoch [9/10], Batch [170], Loss: 0.1856\n",
            "Epoch [9/10], Batch [175], Loss: 0.3372\n",
            "Epoch [9/10], Batch [180], Loss: 0.3029\n",
            "Epoch [9/10], Batch [185], Loss: 0.3949\n",
            "Epoch [9/10], Batch [190], Loss: 0.2385\n",
            "Epoch [9/10], Batch [195], Loss: 0.4768\n",
            "Epoch [9/10], Batch [200], Loss: 0.4620\n",
            "Epoch [9/10], Batch [205], Loss: 0.3702\n",
            "Epoch [9/10], Batch [210], Loss: 0.2263\n",
            "Epoch [9/10], Batch [215], Loss: 0.4007\n",
            "Epoch [9/10], Batch [220], Loss: 0.4311\n",
            "Epoch [9/10], Batch [225], Loss: 0.5857\n",
            "Epoch [9/10], Batch [230], Loss: 0.3416\n",
            "Epoch [9/10], Batch [235], Loss: 0.3532\n",
            "Epoch [9/10], Batch [240], Loss: 0.3169\n",
            "Epoch [9/10], Batch [245], Loss: 0.2275\n",
            "Epoch [9/10], Batch [250], Loss: 0.2887\n",
            "Epoch [9/10], Batch [255], Loss: 0.3053\n",
            "Epoch [9/10], Batch [260], Loss: 0.2732\n",
            "Epoch [9/10], Batch [265], Loss: 0.2256\n",
            "Epoch [9/10], Batch [270], Loss: 0.3183\n",
            "Epoch [9/10], Batch [275], Loss: 0.3686\n",
            "Epoch [9/10], Batch [280], Loss: 0.3021\n",
            "Epoch [9/10], Batch [285], Loss: 0.4411\n",
            "Epoch [9/10], Batch [290], Loss: 0.2564\n",
            "Epoch [9/10], Batch [295], Loss: 0.2522\n",
            "Epoch [9/10], Batch [300], Loss: 0.2277\n",
            "Epoch [9/10], Batch [305], Loss: 0.4917\n",
            "Epoch [9/10], Batch [310], Loss: 0.1803\n",
            "Epoch [9/10], Batch [315], Loss: 0.2254\n",
            "Epoch [9/10], Batch [320], Loss: 0.2555\n",
            "Epoch [9/10], Batch [325], Loss: 0.1493\n",
            "Epoch [9/10], Batch [330], Loss: 0.3017\n",
            "Epoch [9/10], Batch [335], Loss: 0.2597\n",
            "Epoch [9/10], Batch [340], Loss: 0.4190\n",
            "Epoch [9/10], Batch [345], Loss: 0.3039\n",
            "Epoch [9/10], Batch [350], Loss: 0.3236\n",
            "Epoch [9/10], Batch [355], Loss: 0.3495\n",
            "Epoch [9/10], Batch [360], Loss: 0.2577\n",
            "Epoch [9/10], Batch [365], Loss: 0.2221\n",
            "Epoch [9/10], Batch [370], Loss: 0.3821\n",
            "Epoch [9/10], Batch [375], Loss: 0.2734\n",
            "Epoch [9/10], Batch [380], Loss: 0.2374\n",
            "Epoch [9/10], Batch [385], Loss: 0.3228\n",
            "Epoch [9/10], Batch [390], Loss: 0.4323\n",
            "Epoch [9/10], Batch [395], Loss: 0.2662\n",
            "Epoch [9/10], Batch [400], Loss: 0.2464\n",
            "Epoch [9/10], Batch [405], Loss: 0.1852\n",
            "Epoch [9/10], Batch [410], Loss: 0.2375\n",
            "Epoch [9/10], Batch [415], Loss: 0.3769\n",
            "Epoch [9/10], Batch [420], Loss: 0.2691\n",
            "Epoch [9/10], Batch [425], Loss: 0.3904\n",
            "Epoch [9/10], Batch [430], Loss: 0.1935\n",
            "Epoch [9/10], Batch [435], Loss: 0.3892\n",
            "Epoch [9/10], Batch [440], Loss: 0.5360\n",
            "Epoch [9/10], Batch [445], Loss: 0.5792\n",
            "Epoch [9/10], Batch [450], Loss: 0.3541\n",
            "Epoch [9/10], Batch [455], Loss: 0.3264\n",
            "Epoch [9/10], Batch [460], Loss: 0.2378\n",
            "Epoch [9/10], Batch [465], Loss: 0.2752\n",
            "Epoch [9/10], Batch [470], Loss: 0.3167\n",
            "Epoch [9/10], Batch [475], Loss: 0.4065\n",
            "Epoch [9/10], Batch [480], Loss: 0.2327\n",
            "Epoch [9/10], Batch [485], Loss: 0.4429\n",
            "Epoch [9/10], Batch [490], Loss: 0.2468\n",
            "Epoch [9/10], Batch [495], Loss: 0.2302\n",
            "Epoch [9/10], Batch [500], Loss: 0.4574\n",
            "Epoch [9/10], Batch [505], Loss: 0.2416\n",
            "Epoch [9/10], Batch [510], Loss: 0.2026\n",
            "Epoch [9/10], Batch [515], Loss: 0.2745\n",
            "Epoch [9/10], Batch [520], Loss: 0.3781\n",
            "Epoch [9/10], Batch [525], Loss: 0.3088\n",
            "Epoch [9/10], Batch [530], Loss: 0.2234\n",
            "Epoch [9/10], Batch [535], Loss: 0.2512\n",
            "Epoch [9/10], Batch [540], Loss: 0.3117\n",
            "Epoch [9/10], Batch [545], Loss: 0.3752\n",
            "Epoch [9/10], Batch [550], Loss: 0.2336\n",
            "Epoch [9/10], Batch [555], Loss: 0.2860\n",
            "Epoch [9/10], Batch [560], Loss: 0.2958\n",
            "Epoch [9/10], Batch [565], Loss: 0.2976\n",
            "Epoch [9/10], Batch [570], Loss: 0.2622\n",
            "Epoch [9/10], Batch [575], Loss: 0.3485\n",
            "Epoch [9/10], Batch [580], Loss: 0.4059\n",
            "Epoch [9/10], Batch [585], Loss: 0.3441\n",
            "Epoch [9/10], Batch [590], Loss: 0.3726\n",
            "Epoch [9/10], Batch [595], Loss: 0.2432\n",
            "Epoch [9/10], Batch [600], Loss: 0.3329\n",
            "Epoch [9/10], Batch [605], Loss: 0.4146\n",
            "Epoch [9/10], Batch [610], Loss: 0.4301\n",
            "Epoch [9/10], Batch [615], Loss: 0.3397\n",
            "Epoch [9/10], Batch [620], Loss: 0.3045\n",
            "Epoch [9/10], Batch [625], Loss: 0.3604\n",
            "Epoch [9/10], Batch [630], Loss: 0.3922\n",
            "Epoch [9/10], Batch [635], Loss: 0.4936\n",
            "Epoch [9/10], Batch [640], Loss: 0.2457\n",
            "Epoch [9/10], Batch [645], Loss: 0.4241\n",
            "Epoch [9/10], Batch [650], Loss: 0.6191\n",
            "Epoch [9/10], Batch [655], Loss: 0.3693\n",
            "Epoch [9/10], Batch [660], Loss: 0.4274\n",
            "Epoch [9/10], Batch [665], Loss: 0.4478\n",
            "Epoch [9/10], Batch [670], Loss: 0.2563\n",
            "Epoch [9/10], Batch [675], Loss: 0.4496\n",
            "Epoch [9/10], Batch [680], Loss: 0.4301\n",
            "Epoch [9/10], Batch [685], Loss: 0.1889\n",
            "Epoch [9/10], Batch [690], Loss: 0.3204\n",
            "Epoch [9/10], Batch [695], Loss: 0.2993\n",
            "Epoch [9/10], Batch [700], Loss: 0.3226\n",
            "Epoch [9/10], Batch [705], Loss: 0.2334\n",
            "Epoch [9/10], Batch [710], Loss: 0.3465\n",
            "Epoch [9/10], Batch [715], Loss: 0.3319\n",
            "Epoch [9/10], Batch [720], Loss: 0.3908\n",
            "Epoch [9/10], Batch [725], Loss: 0.4499\n",
            "Epoch [9/10], Batch [730], Loss: 0.3487\n",
            "Epoch [9/10], Batch [735], Loss: 0.3730\n",
            "Epoch [9/10], Batch [740], Loss: 0.3242\n",
            "Epoch [9/10], Batch [745], Loss: 0.3448\n",
            "Epoch [9/10], Batch [750], Loss: 0.4623\n",
            "Epoch [9/10], Batch [755], Loss: 0.2625\n",
            "Epoch [9/10], Batch [760], Loss: 0.1707\n",
            "Epoch [9/10], Batch [765], Loss: 0.2876\n",
            "Epoch [9/10], Batch [770], Loss: 0.2522\n",
            "Epoch [9/10], Batch [775], Loss: 0.2469\n",
            "Epoch [9/10], Batch [780], Loss: 0.2898\n",
            "Epoch [9/10], Batch [785], Loss: 0.3278\n",
            "Epoch [9/10], Batch [790], Loss: 0.2471\n",
            "Epoch [9/10], Batch [795], Loss: 0.3095\n",
            "Epoch [9/10], Batch [800], Loss: 0.1375\n",
            "Epoch [9/10], Batch [805], Loss: 0.4714\n",
            "Epoch [9/10], Batch [810], Loss: 0.4079\n",
            "Epoch [9/10], Batch [815], Loss: 0.2410\n",
            "Epoch [9/10], Batch [820], Loss: 0.2174\n",
            "Epoch [9/10], Batch [825], Loss: 0.3361\n",
            "Epoch [9/10], Batch [830], Loss: 0.3520\n",
            "Epoch [9/10], Batch [835], Loss: 0.3153\n",
            "Epoch [9/10], Batch [840], Loss: 0.2575\n",
            "Epoch [9/10], Batch [845], Loss: 0.2408\n",
            "Epoch [9/10], Batch [850], Loss: 0.2938\n",
            "Epoch [9/10], Batch [855], Loss: 0.4557\n",
            "Epoch [9/10], Batch [860], Loss: 0.4349\n",
            "Epoch [9/10], Batch [865], Loss: 0.3171\n",
            "Epoch [9/10], Batch [870], Loss: 0.3236\n",
            "Epoch [9/10], Batch [875], Loss: 0.3333\n",
            "Epoch [9/10], Batch [880], Loss: 0.2554\n",
            "Epoch [9/10], Batch [885], Loss: 0.3292\n",
            "Epoch [9/10], Batch [890], Loss: 0.3762\n",
            "Epoch [9/10], Batch [895], Loss: 0.2939\n",
            "Epoch [9/10], Batch [900], Loss: 0.3617\n",
            "Epoch [9/10], Batch [905], Loss: 0.3013\n",
            "Epoch [9/10], Batch [910], Loss: 0.3279\n",
            "Epoch [9/10], Batch [915], Loss: 0.4355\n",
            "Epoch [9/10], Batch [920], Loss: 0.3529\n",
            "Epoch [9/10], Batch [925], Loss: 0.3506\n",
            "Epoch [9/10], Batch [930], Loss: 0.2114\n",
            "Epoch [9/10], Batch [935], Loss: 0.4664\n",
            "Epoch [9/10], Batch [940], Loss: 0.3091\n",
            "Epoch [9/10], Batch [945], Loss: 0.1855\n",
            "Epoch [9/10], Batch [950], Loss: 0.2862\n",
            "Epoch [9/10], Batch [955], Loss: 0.2263\n",
            "Epoch [9/10], Batch [960], Loss: 0.2722\n",
            "Epoch [9/10], Batch [965], Loss: 0.4027\n",
            "Epoch [9/10], Batch [970], Loss: 0.1656\n",
            "Epoch [9/10], Batch [975], Loss: 0.2682\n",
            "Epoch [9/10], Batch [980], Loss: 0.2949\n",
            "Epoch [9/10], Batch [985], Loss: 0.4615\n",
            "Epoch [9/10], Batch [990], Loss: 0.3525\n",
            "Epoch [9/10], Batch [995], Loss: 0.2724\n",
            "Epoch [9/10], Batch [1000], Loss: 0.2655\n",
            "Epoch [9/10], Batch [1005], Loss: 0.3478\n",
            "Epoch [9/10], Batch [1010], Loss: 0.3313\n",
            "Epoch [9/10], Batch [1015], Loss: 0.3300\n",
            "Epoch [9/10], Batch [1020], Loss: 0.2977\n",
            "Epoch [9/10], Batch [1025], Loss: 0.4284\n",
            "Epoch [9/10], Batch [1030], Loss: 0.2369\n",
            "Epoch [9/10], Batch [1035], Loss: 0.3118\n",
            "Epoch [9/10], Batch [1040], Loss: 0.3545\n",
            "Epoch [9/10], Batch [1045], Loss: 0.2719\n",
            "Epoch [9/10], Batch [1050], Loss: 0.2707\n",
            "Epoch [9/10], Batch [1055], Loss: 0.4266\n",
            "Epoch [9/10], Batch [1060], Loss: 0.3391\n",
            "Epoch [9/10], Batch [1065], Loss: 0.2083\n",
            "Epoch [9/10], Batch [1070], Loss: 0.2414\n",
            "Epoch [9/10], Batch [1075], Loss: 0.2985\n",
            "Epoch [9/10], Batch [1080], Loss: 0.3286\n",
            "Epoch [9/10], Batch [1085], Loss: 0.2890\n",
            "Epoch [9/10], Batch [1090], Loss: 0.3655\n",
            "Epoch [9/10], Batch [1095], Loss: 0.4895\n",
            "Epoch [9/10], Batch [1100], Loss: 0.3893\n",
            "Epoch [9/10], Batch [1105], Loss: 0.3677\n",
            "Epoch [9/10], Batch [1110], Loss: 0.2781\n",
            "Epoch [9/10], Batch [1115], Loss: 0.2756\n",
            "Epoch [9/10], Batch [1120], Loss: 0.2802\n",
            "Epoch [9/10], Batch [1125], Loss: 0.4404\n",
            "Epoch [9/10], Batch [1130], Loss: 0.2242\n",
            "Epoch [9/10], Batch [1135], Loss: 0.4457\n",
            "Epoch [9/10], Batch [1140], Loss: 0.2489\n",
            "Epoch [9/10], Batch [1145], Loss: 0.3409\n",
            "Epoch [9/10], Batch [1150], Loss: 0.4738\n",
            "Epoch [9/10], Batch [1155], Loss: 0.2041\n",
            "Epoch [9/10], Batch [1160], Loss: 0.2648\n",
            "Epoch [9/10], Batch [1165], Loss: 0.2946\n",
            "Epoch [9/10], Batch [1170], Loss: 0.1941\n",
            "Epoch [9/10], Batch [1175], Loss: 0.2062\n",
            "Epoch [9/10], Batch [1180], Loss: 0.3261\n",
            "Epoch [9/10], Batch [1185], Loss: 0.3538\n",
            "Epoch [9/10], Batch [1190], Loss: 0.4991\n",
            "Epoch [9/10], Batch [1195], Loss: 0.3670\n",
            "Epoch [9/10], Batch [1200], Loss: 0.2369\n",
            "Epoch [9/10], Batch [1205], Loss: 0.3899\n",
            "Epoch [9/10], Batch [1210], Loss: 0.3301\n",
            "Epoch [9/10], Batch [1215], Loss: 0.3476\n",
            "Epoch [9/10], Batch [1220], Loss: 0.3602\n",
            "Epoch [9/10], Batch [1225], Loss: 0.3889\n",
            "Epoch [9/10], Batch [1230], Loss: 0.4213\n",
            "Epoch [9/10], Batch [1235], Loss: 0.3012\n",
            "Epoch [9/10], Batch [1240], Loss: 0.3733\n",
            "Epoch [9/10], Batch [1245], Loss: 0.3143\n",
            "Epoch [9/10], Batch [1250], Loss: 0.3190\n",
            "Epoch [9/10], Batch [1255], Loss: 0.4123\n",
            "Epoch [9/10], Batch [1260], Loss: 0.2882\n",
            "Epoch [9/10], Batch [1265], Loss: 0.3003\n",
            "Epoch [9/10], Batch [1270], Loss: 0.2534\n",
            "Epoch [9/10], Batch [1275], Loss: 0.2792\n",
            "Epoch [9/10], Batch [1280], Loss: 0.2896\n",
            "Epoch [9/10], Batch [1285], Loss: 0.1818\n",
            "Epoch [9/10], Batch [1290], Loss: 0.2170\n",
            "Epoch [9/10], Batch [1295], Loss: 0.3273\n",
            "Epoch [9/10], Batch [1300], Loss: 0.2833\n",
            "Epoch [9/10], Batch [1305], Loss: 0.1991\n",
            "Epoch [9/10], Batch [1310], Loss: 0.2043\n",
            "Epoch [9/10], Batch [1315], Loss: 0.4359\n",
            "Epoch [9/10], Batch [1320], Loss: 0.2863\n",
            "Epoch [9/10], Batch [1325], Loss: 0.3317\n",
            "Epoch [9/10], Batch [1330], Loss: 0.3993\n",
            "Epoch [9/10], Batch [1335], Loss: 0.2989\n",
            "Epoch [9/10], Batch [1340], Loss: 0.2868\n",
            "Epoch [9/10], Batch [1345], Loss: 0.3123\n",
            "Epoch [9/10], Batch [1350], Loss: 0.4155\n",
            "Epoch [9/10], Batch [1355], Loss: 0.5580\n",
            "Epoch [9/10], Batch [1360], Loss: 0.3035\n",
            "Epoch [9/10], Batch [1365], Loss: 0.2905\n",
            "Epoch [9/10], Batch [1370], Loss: 0.3573\n",
            "Epoch [9/10], Batch [1375], Loss: 0.3142\n",
            "Epoch [9/10], Batch [1380], Loss: 0.3923\n",
            "Epoch [9/10], Batch [1385], Loss: 0.4087\n",
            "Epoch [9/10], Batch [1390], Loss: 0.6109\n",
            "Epoch [9/10], Batch [1395], Loss: 0.2524\n",
            "Epoch [9/10], Batch [1400], Loss: 0.3715\n",
            "Epoch [9/10], Batch [1405], Loss: 0.2156\n",
            "Epoch [9/10], Batch [1410], Loss: 0.2859\n",
            "Epoch [9/10], Batch [1415], Loss: 0.2416\n",
            "Epoch [9/10], Batch [1420], Loss: 0.2451\n",
            "Epoch [9/10], Batch [1425], Loss: 0.2907\n",
            "Epoch [9/10], Batch [1430], Loss: 0.5055\n",
            "Epoch [9/10], Batch [1435], Loss: 0.4818\n",
            "Epoch [9/10], Batch [1440], Loss: 0.2838\n",
            "Epoch [9/10], Batch [1445], Loss: 0.3493\n",
            "Epoch [9/10], Batch [1450], Loss: 0.3459\n",
            "Epoch [9/10], Batch [1455], Loss: 0.2588\n",
            "Epoch [9/10], Batch [1460], Loss: 0.1759\n",
            "Epoch [9/10], Batch [1465], Loss: 0.3375\n",
            "Epoch [9/10], Batch [1470], Loss: 0.3277\n",
            "Epoch [9/10], Batch [1475], Loss: 0.1765\n",
            "Epoch [9/10], Batch [1480], Loss: 0.3362\n",
            "Epoch [9/10], Batch [1485], Loss: 0.3574\n",
            "Epoch [9/10], Batch [1490], Loss: 0.4191\n",
            "Epoch [9/10], Batch [1495], Loss: 0.3426\n",
            "Epoch [9/10], Batch [1500], Loss: 0.1715\n",
            "Epoch [9/10], Batch [1505], Loss: 0.3567\n",
            "Epoch [9/10], Batch [1510], Loss: 0.2668\n",
            "Epoch [9/10], Batch [1515], Loss: 0.2435\n",
            "Epoch [9/10], Batch [1520], Loss: 0.2982\n",
            "Epoch [9/10], Batch [1525], Loss: 0.3306\n",
            "Epoch [9/10], Batch [1530], Loss: 0.3419\n",
            "Epoch [9/10], Batch [1535], Loss: 0.3525\n",
            "Epoch [9/10], Batch [1540], Loss: 0.3584\n",
            "Epoch [9/10], Batch [1545], Loss: 0.2954\n",
            "Epoch [9/10], Batch [1550], Loss: 0.2721\n",
            "Epoch [9/10], Batch [1555], Loss: 0.2793\n",
            "Epoch [9/10], Batch [1560], Loss: 0.2892\n",
            "Epoch [9/10], Batch [1565], Loss: 0.3860\n",
            "Epoch [9/10], Batch [1570], Loss: 0.4022\n",
            "Epoch [9/10], Batch [1575], Loss: 0.3824\n",
            "Epoch [9/10], Batch [1580], Loss: 0.2847\n",
            "Epoch [9/10], Batch [1585], Loss: 0.2858\n",
            "Epoch [9/10], Batch [1590], Loss: 0.3924\n",
            "Epoch [9/10], Batch [1595], Loss: 0.3183\n",
            "Epoch [9/10], Batch [1600], Loss: 0.3361\n",
            "Epoch [9/10], Batch [1605], Loss: 0.4726\n",
            "Epoch [9/10], Batch [1610], Loss: 0.2698\n",
            "Epoch [9/10], Batch [1615], Loss: 0.3668\n",
            "Epoch [9/10], Batch [1620], Loss: 0.3130\n",
            "Epoch [9/10], Batch [1625], Loss: 0.2708\n",
            "Epoch [9/10], Batch [1630], Loss: 0.2288\n",
            "Epoch [9/10], Batch [1635], Loss: 0.2193\n",
            "Epoch [9/10], Batch [1640], Loss: 0.3284\n",
            "Epoch [9/10], Batch [1645], Loss: 0.3324\n",
            "Epoch [9/10], Batch [1650], Loss: 0.2614\n",
            "Epoch [9/10], Batch [1655], Loss: 0.3798\n",
            "Epoch [9/10], Batch [1660], Loss: 0.2117\n",
            "Epoch [9/10], Batch [1665], Loss: 0.2409\n",
            "Epoch [9/10], Batch [1670], Loss: 0.2803\n",
            "Epoch [9/10], Batch [1675], Loss: 0.4690\n",
            "Epoch [9/10], Batch [1680], Loss: 0.4439\n",
            "Epoch [9/10], Batch [1685], Loss: 0.3750\n",
            "Epoch [9/10], Batch [1690], Loss: 0.3750\n",
            "Epoch [9/10], Batch [1695], Loss: 0.2266\n",
            "Epoch [9/10], Batch [1700], Loss: 0.3196\n",
            "Epoch [9/10], Batch [1705], Loss: 0.2666\n",
            "Epoch [9/10], Batch [1710], Loss: 0.3972\n",
            "Epoch [9/10], Batch [1715], Loss: 0.3502\n",
            "Epoch [9/10], Batch [1720], Loss: 0.2867\n",
            "Epoch [9/10], Batch [1725], Loss: 0.4544\n",
            "Epoch [9/10], Batch [1730], Loss: 0.2841\n",
            "Epoch [9/10], Batch [1735], Loss: 0.2469\n",
            "Epoch [9/10], Batch [1740], Loss: 0.2413\n",
            "Epoch [9/10], Batch [1745], Loss: 0.2843\n",
            "Epoch [9/10], Batch [1750], Loss: 0.4865\n",
            "Epoch [9/10], Batch [1755], Loss: 0.2771\n",
            "Epoch [9/10], Batch [1760], Loss: 0.1639\n",
            "Epoch [9/10], Batch [1765], Loss: 0.3213\n",
            "Epoch [9/10], Batch [1770], Loss: 0.2907\n",
            "Epoch [9/10], Batch [1775], Loss: 0.3143\n",
            "Epoch [9/10], Batch [1780], Loss: 0.3850\n",
            "Epoch [9/10], Batch [1785], Loss: 0.3456\n",
            "Epoch [9/10], Batch [1790], Loss: 0.3411\n",
            "Epoch [9/10], Batch [1795], Loss: 0.3248\n",
            "Epoch [9/10], Batch [1800], Loss: 0.2854\n",
            "Epoch [9/10], Batch [1805], Loss: 0.3898\n",
            "Epoch [9/10], Batch [1810], Loss: 0.2278\n",
            "Epoch [9/10], Batch [1815], Loss: 0.3104\n",
            "Epoch [9/10], Batch [1820], Loss: 0.3198\n",
            "Epoch [9/10], Batch [1825], Loss: 0.3145\n",
            "Epoch [9/10], Batch [1830], Loss: 0.2850\n",
            "Epoch [9/10], Batch [1835], Loss: 0.2508\n",
            "Epoch [9/10], Batch [1840], Loss: 0.2593\n",
            "Epoch [9/10], Batch [1845], Loss: 0.2264\n",
            "Epoch [9/10], Batch [1850], Loss: 0.2037\n",
            "Epoch [9/10], Batch [1855], Loss: 0.1838\n",
            "Epoch [9/10], Batch [1860], Loss: 0.2217\n",
            "Epoch [9/10], Batch [1865], Loss: 0.3585\n",
            "Epoch [9/10], Batch [1870], Loss: 0.2504\n",
            "Epoch [9/10], Batch [1875], Loss: 0.2868\n",
            "Epoch [9/10], Batch [1880], Loss: 0.4160\n",
            "Epoch [9/10], Batch [1885], Loss: 0.3657\n",
            "Epoch [9/10], Batch [1890], Loss: 0.3718\n",
            "Epoch [9/10], Batch [1895], Loss: 0.3234\n",
            "Epoch [9/10], Batch [1900], Loss: 0.3809\n",
            "Epoch [9/10], Batch [1905], Loss: 0.2201\n",
            "Epoch [9/10], Batch [1910], Loss: 0.3575\n",
            "Epoch [9/10], Batch [1915], Loss: 0.2672\n",
            "Epoch [9/10], Batch [1920], Loss: 0.3517\n",
            "Epoch [9/10], Batch [1925], Loss: 0.3074\n",
            "Epoch [9/10], Batch [1930], Loss: 0.2542\n",
            "Epoch [9/10], Batch [1935], Loss: 0.2910\n",
            "Epoch [9/10], Batch [1940], Loss: 0.3586\n",
            "Epoch [9/10], Batch [1945], Loss: 0.2583\n",
            "Epoch [9/10], Batch [1950], Loss: 0.3749\n",
            "Epoch [9/10], Batch [1955], Loss: 0.2528\n",
            "Epoch [9/10], Batch [1960], Loss: 0.4214\n",
            "Epoch [9/10], Batch [1965], Loss: 0.4178\n",
            "Epoch [9/10], Batch [1970], Loss: 0.4951\n",
            "Epoch [9/10], Batch [1975], Loss: 0.5426\n",
            "Epoch [9/10], Batch [1980], Loss: 0.2711\n",
            "Epoch [9/10], Batch [1985], Loss: 0.3093\n",
            "Epoch [9/10], Batch [1990], Loss: 0.2876\n",
            "Epoch [9/10], Batch [1995], Loss: 0.1946\n",
            "Epoch [9/10], Batch [2000], Loss: 0.2050\n",
            "Epoch [9/10], Batch [2005], Loss: 0.2751\n",
            "Epoch [9/10], Batch [2010], Loss: 0.3827\n",
            "Epoch [9/10], Batch [2015], Loss: 0.4684\n",
            "Epoch [9/10], Batch [2020], Loss: 0.3148\n",
            "Epoch [9/10], Batch [2025], Loss: 0.2493\n",
            "Epoch [9/10], Batch [2030], Loss: 0.3340\n",
            "Epoch [9/10], Batch [2035], Loss: 0.2329\n",
            "Epoch [9/10], Batch [2040], Loss: 0.2545\n",
            "Epoch [9/10], Batch [2045], Loss: 0.2264\n",
            "Epoch [9/10], Batch [2050], Loss: 0.3382\n",
            "Epoch [9/10], Batch [2055], Loss: 0.2919\n",
            "Epoch [9/10], Batch [2060], Loss: 0.3727\n",
            "Epoch [9/10], Average Loss: 0.3289\n",
            "Epoch [10/10], Batch [0], Loss: 0.2848\n",
            "Epoch [10/10], Batch [5], Loss: 0.2856\n",
            "Epoch [10/10], Batch [10], Loss: 0.2980\n",
            "Epoch [10/10], Batch [15], Loss: 0.3715\n",
            "Epoch [10/10], Batch [20], Loss: 0.3040\n",
            "Epoch [10/10], Batch [25], Loss: 0.3903\n",
            "Epoch [10/10], Batch [30], Loss: 0.3995\n",
            "Epoch [10/10], Batch [35], Loss: 0.4720\n",
            "Epoch [10/10], Batch [40], Loss: 0.2702\n",
            "Epoch [10/10], Batch [45], Loss: 0.2594\n",
            "Epoch [10/10], Batch [50], Loss: 0.3380\n",
            "Epoch [10/10], Batch [55], Loss: 0.3591\n",
            "Epoch [10/10], Batch [60], Loss: 0.3265\n",
            "Epoch [10/10], Batch [65], Loss: 0.2425\n",
            "Epoch [10/10], Batch [70], Loss: 0.4219\n",
            "Epoch [10/10], Batch [75], Loss: 0.3507\n",
            "Epoch [10/10], Batch [80], Loss: 0.3003\n",
            "Epoch [10/10], Batch [85], Loss: 0.3994\n",
            "Epoch [10/10], Batch [90], Loss: 0.3846\n",
            "Epoch [10/10], Batch [95], Loss: 0.5694\n",
            "Epoch [10/10], Batch [100], Loss: 0.2388\n",
            "Epoch [10/10], Batch [105], Loss: 0.3135\n",
            "Epoch [10/10], Batch [110], Loss: 0.3244\n",
            "Epoch [10/10], Batch [115], Loss: 0.3959\n",
            "Epoch [10/10], Batch [120], Loss: 0.2774\n",
            "Epoch [10/10], Batch [125], Loss: 0.3673\n",
            "Epoch [10/10], Batch [130], Loss: 0.3086\n",
            "Epoch [10/10], Batch [135], Loss: 0.4457\n",
            "Epoch [10/10], Batch [140], Loss: 0.3051\n",
            "Epoch [10/10], Batch [145], Loss: 0.2940\n",
            "Epoch [10/10], Batch [150], Loss: 0.2888\n",
            "Epoch [10/10], Batch [155], Loss: 0.3162\n",
            "Epoch [10/10], Batch [160], Loss: 0.2853\n",
            "Epoch [10/10], Batch [165], Loss: 0.2380\n",
            "Epoch [10/10], Batch [170], Loss: 0.1860\n",
            "Epoch [10/10], Batch [175], Loss: 0.3236\n",
            "Epoch [10/10], Batch [180], Loss: 0.2711\n",
            "Epoch [10/10], Batch [185], Loss: 0.3743\n",
            "Epoch [10/10], Batch [190], Loss: 0.2267\n",
            "Epoch [10/10], Batch [195], Loss: 0.4652\n",
            "Epoch [10/10], Batch [200], Loss: 0.4882\n",
            "Epoch [10/10], Batch [205], Loss: 0.3396\n",
            "Epoch [10/10], Batch [210], Loss: 0.2079\n",
            "Epoch [10/10], Batch [215], Loss: 0.3824\n",
            "Epoch [10/10], Batch [220], Loss: 0.3941\n",
            "Epoch [10/10], Batch [225], Loss: 0.5556\n",
            "Epoch [10/10], Batch [230], Loss: 0.3249\n",
            "Epoch [10/10], Batch [235], Loss: 0.3566\n",
            "Epoch [10/10], Batch [240], Loss: 0.3013\n",
            "Epoch [10/10], Batch [245], Loss: 0.2350\n",
            "Epoch [10/10], Batch [250], Loss: 0.2719\n",
            "Epoch [10/10], Batch [255], Loss: 0.3001\n",
            "Epoch [10/10], Batch [260], Loss: 0.2709\n",
            "Epoch [10/10], Batch [265], Loss: 0.2434\n",
            "Epoch [10/10], Batch [270], Loss: 0.2853\n",
            "Epoch [10/10], Batch [275], Loss: 0.3132\n",
            "Epoch [10/10], Batch [280], Loss: 0.2980\n",
            "Epoch [10/10], Batch [285], Loss: 0.4454\n",
            "Epoch [10/10], Batch [290], Loss: 0.2546\n",
            "Epoch [10/10], Batch [295], Loss: 0.2287\n",
            "Epoch [10/10], Batch [300], Loss: 0.2044\n",
            "Epoch [10/10], Batch [305], Loss: 0.4750\n",
            "Epoch [10/10], Batch [310], Loss: 0.1788\n",
            "Epoch [10/10], Batch [315], Loss: 0.2249\n",
            "Epoch [10/10], Batch [320], Loss: 0.2212\n",
            "Epoch [10/10], Batch [325], Loss: 0.1561\n",
            "Epoch [10/10], Batch [330], Loss: 0.3067\n",
            "Epoch [10/10], Batch [335], Loss: 0.2351\n",
            "Epoch [10/10], Batch [340], Loss: 0.4075\n",
            "Epoch [10/10], Batch [345], Loss: 0.2869\n",
            "Epoch [10/10], Batch [350], Loss: 0.3366\n",
            "Epoch [10/10], Batch [355], Loss: 0.3580\n",
            "Epoch [10/10], Batch [360], Loss: 0.2425\n",
            "Epoch [10/10], Batch [365], Loss: 0.2284\n",
            "Epoch [10/10], Batch [370], Loss: 0.3692\n",
            "Epoch [10/10], Batch [375], Loss: 0.2408\n",
            "Epoch [10/10], Batch [380], Loss: 0.2205\n",
            "Epoch [10/10], Batch [385], Loss: 0.3033\n",
            "Epoch [10/10], Batch [390], Loss: 0.4260\n",
            "Epoch [10/10], Batch [395], Loss: 0.2587\n",
            "Epoch [10/10], Batch [400], Loss: 0.2237\n",
            "Epoch [10/10], Batch [405], Loss: 0.1746\n",
            "Epoch [10/10], Batch [410], Loss: 0.2097\n",
            "Epoch [10/10], Batch [415], Loss: 0.3521\n",
            "Epoch [10/10], Batch [420], Loss: 0.2575\n",
            "Epoch [10/10], Batch [425], Loss: 0.3546\n",
            "Epoch [10/10], Batch [430], Loss: 0.1804\n",
            "Epoch [10/10], Batch [435], Loss: 0.3615\n",
            "Epoch [10/10], Batch [440], Loss: 0.5279\n",
            "Epoch [10/10], Batch [445], Loss: 0.5327\n",
            "Epoch [10/10], Batch [450], Loss: 0.3269\n",
            "Epoch [10/10], Batch [455], Loss: 0.2969\n",
            "Epoch [10/10], Batch [460], Loss: 0.2188\n",
            "Epoch [10/10], Batch [465], Loss: 0.2541\n",
            "Epoch [10/10], Batch [470], Loss: 0.2980\n",
            "Epoch [10/10], Batch [475], Loss: 0.4046\n",
            "Epoch [10/10], Batch [480], Loss: 0.2335\n",
            "Epoch [10/10], Batch [485], Loss: 0.4218\n",
            "Epoch [10/10], Batch [490], Loss: 0.2315\n",
            "Epoch [10/10], Batch [495], Loss: 0.2300\n",
            "Epoch [10/10], Batch [500], Loss: 0.4387\n",
            "Epoch [10/10], Batch [505], Loss: 0.2304\n",
            "Epoch [10/10], Batch [510], Loss: 0.2077\n",
            "Epoch [10/10], Batch [515], Loss: 0.2712\n",
            "Epoch [10/10], Batch [520], Loss: 0.3770\n",
            "Epoch [10/10], Batch [525], Loss: 0.3098\n",
            "Epoch [10/10], Batch [530], Loss: 0.2153\n",
            "Epoch [10/10], Batch [535], Loss: 0.2473\n",
            "Epoch [10/10], Batch [540], Loss: 0.3028\n",
            "Epoch [10/10], Batch [545], Loss: 0.3470\n",
            "Epoch [10/10], Batch [550], Loss: 0.2316\n",
            "Epoch [10/10], Batch [555], Loss: 0.2793\n",
            "Epoch [10/10], Batch [560], Loss: 0.3045\n",
            "Epoch [10/10], Batch [565], Loss: 0.2902\n",
            "Epoch [10/10], Batch [570], Loss: 0.2675\n",
            "Epoch [10/10], Batch [575], Loss: 0.2888\n",
            "Epoch [10/10], Batch [580], Loss: 0.4154\n",
            "Epoch [10/10], Batch [585], Loss: 0.3256\n",
            "Epoch [10/10], Batch [590], Loss: 0.3543\n",
            "Epoch [10/10], Batch [595], Loss: 0.2489\n",
            "Epoch [10/10], Batch [600], Loss: 0.3348\n",
            "Epoch [10/10], Batch [605], Loss: 0.3968\n",
            "Epoch [10/10], Batch [610], Loss: 0.4265\n",
            "Epoch [10/10], Batch [615], Loss: 0.3288\n",
            "Epoch [10/10], Batch [620], Loss: 0.2745\n",
            "Epoch [10/10], Batch [625], Loss: 0.3304\n",
            "Epoch [10/10], Batch [630], Loss: 0.3691\n",
            "Epoch [10/10], Batch [635], Loss: 0.4977\n",
            "Epoch [10/10], Batch [640], Loss: 0.2148\n",
            "Epoch [10/10], Batch [645], Loss: 0.4026\n",
            "Epoch [10/10], Batch [650], Loss: 0.5930\n",
            "Epoch [10/10], Batch [655], Loss: 0.3476\n",
            "Epoch [10/10], Batch [660], Loss: 0.4185\n",
            "Epoch [10/10], Batch [665], Loss: 0.4559\n",
            "Epoch [10/10], Batch [670], Loss: 0.2366\n",
            "Epoch [10/10], Batch [675], Loss: 0.4451\n",
            "Epoch [10/10], Batch [680], Loss: 0.4193\n",
            "Epoch [10/10], Batch [685], Loss: 0.1875\n",
            "Epoch [10/10], Batch [690], Loss: 0.3306\n",
            "Epoch [10/10], Batch [695], Loss: 0.3099\n",
            "Epoch [10/10], Batch [700], Loss: 0.3285\n",
            "Epoch [10/10], Batch [705], Loss: 0.2182\n",
            "Epoch [10/10], Batch [710], Loss: 0.3325\n",
            "Epoch [10/10], Batch [715], Loss: 0.3324\n",
            "Epoch [10/10], Batch [720], Loss: 0.3600\n",
            "Epoch [10/10], Batch [725], Loss: 0.4172\n",
            "Epoch [10/10], Batch [730], Loss: 0.3311\n",
            "Epoch [10/10], Batch [735], Loss: 0.3639\n",
            "Epoch [10/10], Batch [740], Loss: 0.3206\n",
            "Epoch [10/10], Batch [745], Loss: 0.3305\n",
            "Epoch [10/10], Batch [750], Loss: 0.4297\n",
            "Epoch [10/10], Batch [755], Loss: 0.2388\n",
            "Epoch [10/10], Batch [760], Loss: 0.1564\n",
            "Epoch [10/10], Batch [765], Loss: 0.2778\n",
            "Epoch [10/10], Batch [770], Loss: 0.2120\n",
            "Epoch [10/10], Batch [775], Loss: 0.2241\n",
            "Epoch [10/10], Batch [780], Loss: 0.2802\n",
            "Epoch [10/10], Batch [785], Loss: 0.3159\n",
            "Epoch [10/10], Batch [790], Loss: 0.2634\n",
            "Epoch [10/10], Batch [795], Loss: 0.3039\n",
            "Epoch [10/10], Batch [800], Loss: 0.1344\n",
            "Epoch [10/10], Batch [805], Loss: 0.4505\n",
            "Epoch [10/10], Batch [810], Loss: 0.4305\n",
            "Epoch [10/10], Batch [815], Loss: 0.2275\n",
            "Epoch [10/10], Batch [820], Loss: 0.2237\n",
            "Epoch [10/10], Batch [825], Loss: 0.3030\n",
            "Epoch [10/10], Batch [830], Loss: 0.3431\n",
            "Epoch [10/10], Batch [835], Loss: 0.2859\n",
            "Epoch [10/10], Batch [840], Loss: 0.2432\n",
            "Epoch [10/10], Batch [845], Loss: 0.2498\n",
            "Epoch [10/10], Batch [850], Loss: 0.2750\n",
            "Epoch [10/10], Batch [855], Loss: 0.4176\n",
            "Epoch [10/10], Batch [860], Loss: 0.4331\n",
            "Epoch [10/10], Batch [865], Loss: 0.2893\n",
            "Epoch [10/10], Batch [870], Loss: 0.2965\n",
            "Epoch [10/10], Batch [875], Loss: 0.3316\n",
            "Epoch [10/10], Batch [880], Loss: 0.2403\n",
            "Epoch [10/10], Batch [885], Loss: 0.3201\n",
            "Epoch [10/10], Batch [890], Loss: 0.3657\n",
            "Epoch [10/10], Batch [895], Loss: 0.2727\n",
            "Epoch [10/10], Batch [900], Loss: 0.3276\n",
            "Epoch [10/10], Batch [905], Loss: 0.3111\n",
            "Epoch [10/10], Batch [910], Loss: 0.3319\n",
            "Epoch [10/10], Batch [915], Loss: 0.4328\n",
            "Epoch [10/10], Batch [920], Loss: 0.3462\n",
            "Epoch [10/10], Batch [925], Loss: 0.3542\n",
            "Epoch [10/10], Batch [930], Loss: 0.2044\n",
            "Epoch [10/10], Batch [935], Loss: 0.4284\n",
            "Epoch [10/10], Batch [940], Loss: 0.3084\n",
            "Epoch [10/10], Batch [945], Loss: 0.1802\n",
            "Epoch [10/10], Batch [950], Loss: 0.2899\n",
            "Epoch [10/10], Batch [955], Loss: 0.1968\n",
            "Epoch [10/10], Batch [960], Loss: 0.2704\n",
            "Epoch [10/10], Batch [965], Loss: 0.3709\n",
            "Epoch [10/10], Batch [970], Loss: 0.1655\n",
            "Epoch [10/10], Batch [975], Loss: 0.2561\n",
            "Epoch [10/10], Batch [980], Loss: 0.3007\n",
            "Epoch [10/10], Batch [985], Loss: 0.4302\n",
            "Epoch [10/10], Batch [990], Loss: 0.3495\n",
            "Epoch [10/10], Batch [995], Loss: 0.2609\n",
            "Epoch [10/10], Batch [1000], Loss: 0.2596\n",
            "Epoch [10/10], Batch [1005], Loss: 0.3260\n",
            "Epoch [10/10], Batch [1010], Loss: 0.3103\n",
            "Epoch [10/10], Batch [1015], Loss: 0.3350\n",
            "Epoch [10/10], Batch [1020], Loss: 0.2866\n",
            "Epoch [10/10], Batch [1025], Loss: 0.4024\n",
            "Epoch [10/10], Batch [1030], Loss: 0.2411\n",
            "Epoch [10/10], Batch [1035], Loss: 0.2859\n",
            "Epoch [10/10], Batch [1040], Loss: 0.3349\n",
            "Epoch [10/10], Batch [1045], Loss: 0.2643\n",
            "Epoch [10/10], Batch [1050], Loss: 0.2260\n",
            "Epoch [10/10], Batch [1055], Loss: 0.3973\n",
            "Epoch [10/10], Batch [1060], Loss: 0.3262\n",
            "Epoch [10/10], Batch [1065], Loss: 0.1922\n",
            "Epoch [10/10], Batch [1070], Loss: 0.2328\n",
            "Epoch [10/10], Batch [1075], Loss: 0.2686\n",
            "Epoch [10/10], Batch [1080], Loss: 0.3205\n",
            "Epoch [10/10], Batch [1085], Loss: 0.2722\n",
            "Epoch [10/10], Batch [1090], Loss: 0.3590\n",
            "Epoch [10/10], Batch [1095], Loss: 0.4601\n",
            "Epoch [10/10], Batch [1100], Loss: 0.3721\n",
            "Epoch [10/10], Batch [1105], Loss: 0.3734\n",
            "Epoch [10/10], Batch [1110], Loss: 0.2636\n",
            "Epoch [10/10], Batch [1115], Loss: 0.2522\n",
            "Epoch [10/10], Batch [1120], Loss: 0.2659\n",
            "Epoch [10/10], Batch [1125], Loss: 0.3872\n",
            "Epoch [10/10], Batch [1130], Loss: 0.2238\n",
            "Epoch [10/10], Batch [1135], Loss: 0.4360\n",
            "Epoch [10/10], Batch [1140], Loss: 0.2267\n",
            "Epoch [10/10], Batch [1145], Loss: 0.3169\n",
            "Epoch [10/10], Batch [1150], Loss: 0.4715\n",
            "Epoch [10/10], Batch [1155], Loss: 0.1945\n",
            "Epoch [10/10], Batch [1160], Loss: 0.2590\n",
            "Epoch [10/10], Batch [1165], Loss: 0.2803\n",
            "Epoch [10/10], Batch [1170], Loss: 0.1822\n",
            "Epoch [10/10], Batch [1175], Loss: 0.1901\n",
            "Epoch [10/10], Batch [1180], Loss: 0.3458\n",
            "Epoch [10/10], Batch [1185], Loss: 0.3375\n",
            "Epoch [10/10], Batch [1190], Loss: 0.4779\n",
            "Epoch [10/10], Batch [1195], Loss: 0.3322\n",
            "Epoch [10/10], Batch [1200], Loss: 0.2100\n",
            "Epoch [10/10], Batch [1205], Loss: 0.3878\n",
            "Epoch [10/10], Batch [1210], Loss: 0.2955\n",
            "Epoch [10/10], Batch [1215], Loss: 0.3136\n",
            "Epoch [10/10], Batch [1220], Loss: 0.3525\n",
            "Epoch [10/10], Batch [1225], Loss: 0.3417\n",
            "Epoch [10/10], Batch [1230], Loss: 0.4046\n",
            "Epoch [10/10], Batch [1235], Loss: 0.2894\n",
            "Epoch [10/10], Batch [1240], Loss: 0.3666\n",
            "Epoch [10/10], Batch [1245], Loss: 0.3059\n",
            "Epoch [10/10], Batch [1250], Loss: 0.2897\n",
            "Epoch [10/10], Batch [1255], Loss: 0.3662\n",
            "Epoch [10/10], Batch [1260], Loss: 0.2706\n",
            "Epoch [10/10], Batch [1265], Loss: 0.2691\n",
            "Epoch [10/10], Batch [1270], Loss: 0.2530\n",
            "Epoch [10/10], Batch [1275], Loss: 0.2728\n",
            "Epoch [10/10], Batch [1280], Loss: 0.2893\n",
            "Epoch [10/10], Batch [1285], Loss: 0.1793\n",
            "Epoch [10/10], Batch [1290], Loss: 0.1954\n",
            "Epoch [10/10], Batch [1295], Loss: 0.3081\n",
            "Epoch [10/10], Batch [1300], Loss: 0.2781\n",
            "Epoch [10/10], Batch [1305], Loss: 0.1892\n",
            "Epoch [10/10], Batch [1310], Loss: 0.2028\n",
            "Epoch [10/10], Batch [1315], Loss: 0.4210\n",
            "Epoch [10/10], Batch [1320], Loss: 0.2676\n",
            "Epoch [10/10], Batch [1325], Loss: 0.3092\n",
            "Epoch [10/10], Batch [1330], Loss: 0.3971\n",
            "Epoch [10/10], Batch [1335], Loss: 0.3047\n",
            "Epoch [10/10], Batch [1340], Loss: 0.2542\n",
            "Epoch [10/10], Batch [1345], Loss: 0.2966\n",
            "Epoch [10/10], Batch [1350], Loss: 0.4007\n",
            "Epoch [10/10], Batch [1355], Loss: 0.5396\n",
            "Epoch [10/10], Batch [1360], Loss: 0.2959\n",
            "Epoch [10/10], Batch [1365], Loss: 0.2793\n",
            "Epoch [10/10], Batch [1370], Loss: 0.3547\n",
            "Epoch [10/10], Batch [1375], Loss: 0.2857\n",
            "Epoch [10/10], Batch [1380], Loss: 0.4013\n",
            "Epoch [10/10], Batch [1385], Loss: 0.3859\n",
            "Epoch [10/10], Batch [1390], Loss: 0.6153\n",
            "Epoch [10/10], Batch [1395], Loss: 0.2207\n",
            "Epoch [10/10], Batch [1400], Loss: 0.3828\n",
            "Epoch [10/10], Batch [1405], Loss: 0.2033\n",
            "Epoch [10/10], Batch [1410], Loss: 0.2693\n",
            "Epoch [10/10], Batch [1415], Loss: 0.1976\n",
            "Epoch [10/10], Batch [1420], Loss: 0.2322\n",
            "Epoch [10/10], Batch [1425], Loss: 0.3009\n",
            "Epoch [10/10], Batch [1430], Loss: 0.4847\n",
            "Epoch [10/10], Batch [1435], Loss: 0.4878\n",
            "Epoch [10/10], Batch [1440], Loss: 0.2428\n",
            "Epoch [10/10], Batch [1445], Loss: 0.3349\n",
            "Epoch [10/10], Batch [1450], Loss: 0.3492\n",
            "Epoch [10/10], Batch [1455], Loss: 0.2559\n",
            "Epoch [10/10], Batch [1460], Loss: 0.1678\n",
            "Epoch [10/10], Batch [1465], Loss: 0.3314\n",
            "Epoch [10/10], Batch [1470], Loss: 0.2968\n",
            "Epoch [10/10], Batch [1475], Loss: 0.1746\n",
            "Epoch [10/10], Batch [1480], Loss: 0.3035\n",
            "Epoch [10/10], Batch [1485], Loss: 0.3432\n",
            "Epoch [10/10], Batch [1490], Loss: 0.4077\n",
            "Epoch [10/10], Batch [1495], Loss: 0.3437\n",
            "Epoch [10/10], Batch [1500], Loss: 0.1490\n",
            "Epoch [10/10], Batch [1505], Loss: 0.3378\n",
            "Epoch [10/10], Batch [1510], Loss: 0.2474\n",
            "Epoch [10/10], Batch [1515], Loss: 0.2298\n",
            "Epoch [10/10], Batch [1520], Loss: 0.2977\n",
            "Epoch [10/10], Batch [1525], Loss: 0.3102\n",
            "Epoch [10/10], Batch [1530], Loss: 0.3197\n",
            "Epoch [10/10], Batch [1535], Loss: 0.3270\n",
            "Epoch [10/10], Batch [1540], Loss: 0.3572\n",
            "Epoch [10/10], Batch [1545], Loss: 0.2792\n",
            "Epoch [10/10], Batch [1550], Loss: 0.2668\n",
            "Epoch [10/10], Batch [1555], Loss: 0.2666\n",
            "Epoch [10/10], Batch [1560], Loss: 0.2897\n",
            "Epoch [10/10], Batch [1565], Loss: 0.3447\n",
            "Epoch [10/10], Batch [1570], Loss: 0.3977\n",
            "Epoch [10/10], Batch [1575], Loss: 0.3831\n",
            "Epoch [10/10], Batch [1580], Loss: 0.2738\n",
            "Epoch [10/10], Batch [1585], Loss: 0.3016\n",
            "Epoch [10/10], Batch [1590], Loss: 0.3700\n",
            "Epoch [10/10], Batch [1595], Loss: 0.2849\n",
            "Epoch [10/10], Batch [1600], Loss: 0.2967\n",
            "Epoch [10/10], Batch [1605], Loss: 0.4589\n",
            "Epoch [10/10], Batch [1610], Loss: 0.2452\n",
            "Epoch [10/10], Batch [1615], Loss: 0.3369\n",
            "Epoch [10/10], Batch [1620], Loss: 0.2858\n",
            "Epoch [10/10], Batch [1625], Loss: 0.2618\n",
            "Epoch [10/10], Batch [1630], Loss: 0.2197\n",
            "Epoch [10/10], Batch [1635], Loss: 0.1903\n",
            "Epoch [10/10], Batch [1640], Loss: 0.3266\n",
            "Epoch [10/10], Batch [1645], Loss: 0.3199\n",
            "Epoch [10/10], Batch [1650], Loss: 0.2602\n",
            "Epoch [10/10], Batch [1655], Loss: 0.3593\n",
            "Epoch [10/10], Batch [1660], Loss: 0.1908\n",
            "Epoch [10/10], Batch [1665], Loss: 0.2385\n",
            "Epoch [10/10], Batch [1670], Loss: 0.2681\n",
            "Epoch [10/10], Batch [1675], Loss: 0.4464\n",
            "Epoch [10/10], Batch [1680], Loss: 0.4243\n",
            "Epoch [10/10], Batch [1685], Loss: 0.3683\n",
            "Epoch [10/10], Batch [1690], Loss: 0.3594\n",
            "Epoch [10/10], Batch [1695], Loss: 0.2105\n",
            "Epoch [10/10], Batch [1700], Loss: 0.3160\n",
            "Epoch [10/10], Batch [1705], Loss: 0.2316\n",
            "Epoch [10/10], Batch [1710], Loss: 0.3693\n",
            "Epoch [10/10], Batch [1715], Loss: 0.3573\n",
            "Epoch [10/10], Batch [1720], Loss: 0.3000\n",
            "Epoch [10/10], Batch [1725], Loss: 0.3720\n",
            "Epoch [10/10], Batch [1730], Loss: 0.2665\n",
            "Epoch [10/10], Batch [1735], Loss: 0.2395\n",
            "Epoch [10/10], Batch [1740], Loss: 0.2466\n",
            "Epoch [10/10], Batch [1745], Loss: 0.2689\n",
            "Epoch [10/10], Batch [1750], Loss: 0.4582\n",
            "Epoch [10/10], Batch [1755], Loss: 0.2664\n",
            "Epoch [10/10], Batch [1760], Loss: 0.1464\n",
            "Epoch [10/10], Batch [1765], Loss: 0.3015\n",
            "Epoch [10/10], Batch [1770], Loss: 0.2596\n",
            "Epoch [10/10], Batch [1775], Loss: 0.3093\n",
            "Epoch [10/10], Batch [1780], Loss: 0.3619\n",
            "Epoch [10/10], Batch [1785], Loss: 0.3171\n",
            "Epoch [10/10], Batch [1790], Loss: 0.3002\n",
            "Epoch [10/10], Batch [1795], Loss: 0.2985\n",
            "Epoch [10/10], Batch [1800], Loss: 0.2931\n",
            "Epoch [10/10], Batch [1805], Loss: 0.3723\n",
            "Epoch [10/10], Batch [1810], Loss: 0.2289\n",
            "Epoch [10/10], Batch [1815], Loss: 0.2764\n",
            "Epoch [10/10], Batch [1820], Loss: 0.2861\n",
            "Epoch [10/10], Batch [1825], Loss: 0.2895\n",
            "Epoch [10/10], Batch [1830], Loss: 0.2795\n",
            "Epoch [10/10], Batch [1835], Loss: 0.2467\n",
            "Epoch [10/10], Batch [1840], Loss: 0.2354\n",
            "Epoch [10/10], Batch [1845], Loss: 0.2354\n",
            "Epoch [10/10], Batch [1850], Loss: 0.2036\n",
            "Epoch [10/10], Batch [1855], Loss: 0.1654\n",
            "Epoch [10/10], Batch [1860], Loss: 0.2312\n",
            "Epoch [10/10], Batch [1865], Loss: 0.3502\n",
            "Epoch [10/10], Batch [1870], Loss: 0.2529\n",
            "Epoch [10/10], Batch [1875], Loss: 0.2774\n",
            "Epoch [10/10], Batch [1880], Loss: 0.3688\n",
            "Epoch [10/10], Batch [1885], Loss: 0.3498\n",
            "Epoch [10/10], Batch [1890], Loss: 0.3351\n",
            "Epoch [10/10], Batch [1895], Loss: 0.3077\n",
            "Epoch [10/10], Batch [1900], Loss: 0.3530\n",
            "Epoch [10/10], Batch [1905], Loss: 0.1983\n",
            "Epoch [10/10], Batch [1910], Loss: 0.3373\n",
            "Epoch [10/10], Batch [1915], Loss: 0.2532\n",
            "Epoch [10/10], Batch [1920], Loss: 0.3161\n",
            "Epoch [10/10], Batch [1925], Loss: 0.2857\n",
            "Epoch [10/10], Batch [1930], Loss: 0.2418\n",
            "Epoch [10/10], Batch [1935], Loss: 0.2413\n",
            "Epoch [10/10], Batch [1940], Loss: 0.3397\n",
            "Epoch [10/10], Batch [1945], Loss: 0.2496\n",
            "Epoch [10/10], Batch [1950], Loss: 0.3511\n",
            "Epoch [10/10], Batch [1955], Loss: 0.2303\n",
            "Epoch [10/10], Batch [1960], Loss: 0.3961\n",
            "Epoch [10/10], Batch [1965], Loss: 0.3813\n",
            "Epoch [10/10], Batch [1970], Loss: 0.4913\n",
            "Epoch [10/10], Batch [1975], Loss: 0.4733\n",
            "Epoch [10/10], Batch [1980], Loss: 0.2628\n",
            "Epoch [10/10], Batch [1985], Loss: 0.3066\n",
            "Epoch [10/10], Batch [1990], Loss: 0.2837\n",
            "Epoch [10/10], Batch [1995], Loss: 0.1845\n",
            "Epoch [10/10], Batch [2000], Loss: 0.1950\n",
            "Epoch [10/10], Batch [2005], Loss: 0.2484\n",
            "Epoch [10/10], Batch [2010], Loss: 0.3646\n",
            "Epoch [10/10], Batch [2015], Loss: 0.4488\n",
            "Epoch [10/10], Batch [2020], Loss: 0.3435\n",
            "Epoch [10/10], Batch [2025], Loss: 0.2377\n",
            "Epoch [10/10], Batch [2030], Loss: 0.3222\n",
            "Epoch [10/10], Batch [2035], Loss: 0.2153\n",
            "Epoch [10/10], Batch [2040], Loss: 0.2388\n",
            "Epoch [10/10], Batch [2045], Loss: 0.2091\n",
            "Epoch [10/10], Batch [2050], Loss: 0.3169\n",
            "Epoch [10/10], Batch [2055], Loss: 0.2631\n",
            "Epoch [10/10], Batch [2060], Loss: 0.3661\n",
            "Epoch [10/10], Average Loss: 0.3145\n",
            "Test RMSE: 1.9765\n",
            "Test R-squared: 0.5969\n",
            "Test Correlation: 0.7745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabnet"
      ],
      "metadata": {
        "id": "5hZHQ1rMMW9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3B8REP8MWSX",
        "outputId": "5194847d-0661-4c2b-a97c-ca215dd028b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.6.0)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming small_training is your DataFrame\n",
        "target_column = 'Yield_Mg_ha'\n",
        "\n",
        "# 1. Separate features (X) and target (y)\n",
        "X = small_training.drop(columns=[target_column])\n",
        "y = small_training[target_column]\n",
        "\n",
        "# 2. Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Further split train set into train and validation (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# 5. TabNet model initialization and training\n",
        "clf = TabNetRegressor()\n",
        "\n",
        "clf.fit(\n",
        "    X_train=X_train.values,\n",
        "    y_train=y_train.values.reshape(-1, 1),  # Reshape to (n_samples, 1)\n",
        "    eval_set=[(X_valid.values, y_valid.values.reshape(-1, 1))],\n",
        "    eval_metric=['rmse'],  # Use RMSE for evaluation\n",
        "    cat_idxs=[X_train.columns.get_loc(col) for col in categorical_features] if categorical_features else [], # Define categorical feature indices\n",
        "    cat_dims=[len(X_train[col].unique()) for col in categorical_features] if categorical_features else [] # Define categorical feature dimensionalities\n",
        ")\n",
        "\n",
        "# 6. Prediction\n",
        "preds = clf.predict(X_test.values)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Bx1-wnazkRc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "\n",
        "clf = TabNetClassifier()  #TabNetRegressor()\n",
        "clf.fit(\n",
        "  X_train, Y_train,\n",
        "  eval_set=[(X_valid, y_valid)]\n",
        ")\n",
        "preds = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "ja4fqRS4gNgh",
        "outputId": "c9cbcf4e-c788-4a30-dd4b-00ac5f77ed8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a749e5f43e01>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabNetClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#TabNetRegressor()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m clf.fit(\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    }
  ]
}